{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc84eb37-3b34-4506-8ebc-c70f28166077",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<center><img src=\"images/2024_reInvent_Logo_wDate_Black_V3.png\" alt=\"drawing\" width=\"400\" style=\"background-color:white; padding:1em;\" /></center> <br/>\n",
    "\n",
    "# <a name=\"0\">re:Invent 2024 | Lab 1: Build your RAG powered chatbot  </a>\n",
    "## <a name=\"0\">Build a chatbot with Knowledge Bases and Guardrails to detect and remediate hallucinations </a>\n",
    "\n",
    "## Lab Overview\n",
    "In this lab, you will:\n",
    "1. Take a deeper look at which LLM parameters influence or control for model hallucinations\n",
    "2. Set up Retrieval Augmented Generation and understand how it can control for hallucinations\n",
    "3. Apply contextual grounding in Amazon Bedrock Guardrails to intervene when a model hallucinates\n",
    "4. Use RAGAS evaluation and understand which metrics help us measure hallucinations\n",
    "\n",
    "## Dataset\n",
    "For this workshop, we will use the [Bedrock User Guide](https://docs.aws.amazon.com/pdfs/bedrock/latest/userguide/bedrock-ug.pdf) available as a PDF file.\n",
    "## Use-Case Overview\n",
    "In this lab, we want to develop a chatbot which can answer questions about Amazon Bedrock as factually as possible. We will set up Retrieval Augmented Generation using [Amazon Bedrock Knowledge Bases](https://aws.amazon.com/bedrock/knowledge-bases/) and apply [Amazon Guardrails](https://aws.amazon.com/bedrock/guardrails/) to intervene when hallucinations are detected.\n",
    "\n",
    "\n",
    "#### Lab Sections\n",
    "\n",
    "This lab notebook has the following sections:\n",
    "    \n",
    "Please work top to bottom of this notebook and don't skip sections as this could lead to error messages due to missing code.\n",
    "\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f260f34e-c753-4456-8643-639a3b4cdcfa",
   "metadata": {},
   "source": [
    "# Star Github repository for future reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb09da1-5d9e-4496-b0c8-ed5ae80911ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "\n",
    "<a class=\"github-button\" href=\"https://github.com/aws-samples/responsible_ai_aim325_reduce_hallucinations_for_genai_apps\" data-color-scheme=\"no-preference: light; light: light; dark: dark;\" data-icon=\"octicon-star\" data-size=\"large\" data-show-count=\"true\" aria-label=\"Star Reduce Hallucinations workshop on GitHub\">Star</a>\n",
    "<script async defer src=\"https://buttons.github.io/buttons.js\"></script>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc006a3-48d5-40a5-9eb8-ea9bcd3d85e9",
   "metadata": {},
   "source": [
    "# Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65860ec8-bbea-4e33-b491-25e57c270470",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %pip install --upgrade --quiet pip sagemaker boto3 ragas==0.1.7 pydantic==2.6.1 langchain-core==0.1.40 langchain langchain-aws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba631f57-61f6-422f-8a42-472cb6046eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip3 install -r requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85fe2e2d-9fd5-4586-8fd0-6bc7b7922e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# restart kernel\n",
    "# from IPython.core.display import HTML\n",
    "# HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f420e67-aff9-4065-8525-6fa87fca093e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import json\n",
    "import boto3\n",
    "from time import gmtime, strftime, sleep\n",
    "import pprint\n",
    "import random\n",
    "import zipfile\n",
    "\n",
    "# from retrying import retry\n",
    "from rag_setup.create_kb_utils import *\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sagemaker\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "(sagemaker.__version__, boto3.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6edb7c-dfa1-4460-a365-e3f287951ddb",
   "metadata": {},
   "source": [
    "## Set constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b2c77d-ebdb-4d0a-acdd-04558b7797a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get some variables you need to interact with SageMaker service\n",
    "boto_session = boto3.Session()\n",
    "region = boto_session.region_name\n",
    "bucket_name = sagemaker.Session().default_bucket()\n",
    "bucket_prefix = \"reduce-hallucinations-in-genai-apps\"\n",
    "sm_session = sagemaker.Session()\n",
    "sm_client = boto_session.client(\"sagemaker\")\n",
    "sm_role = sagemaker.get_execution_role()\n",
    "\n",
    "initialized = True\n",
    "\n",
    "print(sm_role)\n",
    "print(bucket_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c34d99ca-e263-44e3-9e35-685a7a8f9859",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embedding_model_id = \"amazon.titan-embed-text-v2:0\"\n",
    "llm_model_id = \"anthropic.claude-3-sonnet-20240229-v1:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2840d5d1-975e-49cb-9805-a7616c369f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store some variables to keep the value between the notebooks\n",
    "%store bucket_name\n",
    "%store bucket_prefix\n",
    "%store sm_role\n",
    "%store region\n",
    "%store initialized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d3afab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test if bedrock model access has been enabled\n",
    "input_prompt = \"Who was the first person to land on the sun?\"\n",
    "test_llm_call(input_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a44d408-1086-48fc-a329-f757fed9b02a",
   "metadata": {},
   "source": [
    "# 1. Chat with Anthropic Claude 3 Sonnet through Bedrock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "340537e1-ef81-4aef-a767-f0df5e1fef45",
   "metadata": {},
   "outputs": [],
   "source": [
    "bedrock_runtime = boto3.client(service_name=\"bedrock-runtime\")\n",
    "\n",
    "\n",
    "def generate_message_claude(\n",
    "    query,\n",
    "    system_prompt=\"\",\n",
    "    max_tokens=1000,\n",
    "    model_id=\"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
    "    temperature=0.9,\n",
    "    top_p=0.99,\n",
    "    top_k=100,\n",
    "):\n",
    "    # Prompt with user turn only.\n",
    "    user_message = {\"role\": \"user\", \"content\": query}\n",
    "    messages = [user_message]\n",
    "    body = json.dumps(\n",
    "        {\n",
    "            \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "            \"max_tokens\": max_tokens,\n",
    "            \"system\": system_prompt,\n",
    "            \"messages\": messages,\n",
    "            \"temperature\": temperature,\n",
    "            \"top_p\": top_p,\n",
    "            \"top_k\": top_k,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    response = bedrock_runtime.invoke_model(body=body, modelId=model_id)\n",
    "    response_body = json.loads(response.get(\"body\").read())\n",
    "    return response_body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd47d4b-8455-4fcf-ae91-874ab2f7c0e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "query = \"How do Amazon Bedrock Guardrails work?\"\n",
    "\n",
    "response = generate_message_claude(query)\n",
    "print(\"User turn only.\")\n",
    "print(json.dumps(response, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621f0fa6-9637-4c65-bad6-0e1eacc692e9",
   "metadata": {},
   "source": [
    "## 1.1 Apply System Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e203f5e9-2d93-4a79-b8ad-754d656f3008",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Is it possible to purchase provisioned throughput for Anthropic Claude models on Amazon Bedrock?\"\n",
    "system_prompt = \"You are a helpful AI assistant. You try to answer the user queries to the best of your knowledge. If you are unsure of the answer, do not make up any information.\"\n",
    "\n",
    "response = generate_message_claude(query, system_prompt)\n",
    "print(\"User turn only.\")\n",
    "print(json.dumps(response, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5057db14-729d-4d64-8355-fd6fe2ee2398",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How do Amazon Bedrock Guardrails work?\"\n",
    "system_prompt = \"You are a helpful AI assistant. You try to answer the user queries to the best of your knowledge. If you are unsure of the answer, do not make up any information.\"\n",
    "\n",
    "response = generate_message_claude(query, system_prompt)\n",
    "print(\"User turn only.\")\n",
    "print(json.dumps(response, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77f9196-7e32-4ef0-b516-b8801a688aa3",
   "metadata": {},
   "source": [
    "## 1.2 Understanding LLM generation parameters\n",
    "### 1. Temperature: The amount of randomness injected into the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f610c2e0-e810-47f4-89f9-9980319e32dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is Amazon Bedrock?\"\n",
    "system_prompt = \"You are a helpful AI assistant. You try to answer the user queries to the best of your knowledge. If you are unsure of the answer, do not make up any information.\"\n",
    "\n",
    "response = generate_message_claude(query, system_prompt, temperature=1)\n",
    "print(\"User turn only.\")\n",
    "print(json.dumps(response, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ff7dba-2f3f-4bd2-87ce-0d84c3c03a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is Amazon Bedrock?\"\n",
    "system_prompt = \"You are a helpful AI assistant. You try to answer the user queries to the best of your knowledge. If you are unsure of the answer, do not make up any information.\"\n",
    "\n",
    "response = generate_message_claude(query, system_prompt, temperature=0)\n",
    "print(\"User turn only.\")\n",
    "print(json.dumps(response, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47723758-0f8c-4190-94ef-3bdd4d386908",
   "metadata": {},
   "source": [
    "#### 2. top_p – Use nucleus sampling.\n",
    "\n",
    "In nucleus sampling, Anthropic Claude computes the cumulative distribution over all the options for each subsequent token in decreasing probability order and cuts it off once it reaches a particular probability specified by top_p. You should alter either temperature or top_p, but not both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e300c40f-1b7b-4db1-bc5f-5f483507262e",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is Amazon Bedrock?\"\n",
    "system_prompt = \"You are a helpful AI assistant. You try to answer the user queries to the best of your knowledge. If you are unsure of the answer, do not make up any information.\"\n",
    "\n",
    "response = generate_message_claude(query, system_prompt, temperature=1, top_p=1)\n",
    "print(\"User turn only.\")\n",
    "print(json.dumps(response, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7946bd-976d-4a8d-81ef-f7ff7e164f94",
   "metadata": {},
   "source": [
    "#### 3. top_k: Only sample from the top K options for each subsequent token.\n",
    "\n",
    "Use top_k to remove long tail low probability responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf753a4-8929-46ef-8aea-7ef040abb096",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is Amazon Bedrock?\"\n",
    "system_prompt = \"You are a helpful AI assistant. You try to answer the user queries to the best of your knowledge. If you are unsure of the answer, do not make up any information.\"\n",
    "\n",
    "response = generate_message_claude(\n",
    "    query, system_prompt, temperature=0, top_p=1, top_k=100\n",
    ")\n",
    "print(\"User turn only.\")\n",
    "print(json.dumps(response, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9caf35-dccd-4c16-82eb-ea3e1327bc80",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation\n",
    "We are using the Retrieval Augmented Generation (RAG) technique with Amazon Bedrock. A RAG implementation consists of two parts:\n",
    "\n",
    "    1. A data pipeline that ingests that from documents (typically stored in Amazon S3) into a Knowledge Base i.e. a vector database such as Amazon OpenSearch Service Serverless (AOSS) so that it is available for lookup when a question is received.\n",
    "\n",
    "The data pipeline represents an undifferentiated heavy lifting and can be implemented using Amazon Bedrock Knowledge Bases. We can now connect an S3 bucket to a vector database such as AOSS and have a Bedrock Knowledge Bases read the objects (html, pdf, text etc.), chunk them, and then convert these chunks into embeddings using Amazon Titan Embeddings model and then store these embeddings in AOSS. All of this without having to build, deploy, and manage the data pipeline.\n",
    "\n",
    "<center><img src=\"images/fully_managed_ingestion.png\" alt=\"This image shows how Aazon Bedrock Knowledge Bases ingests objects in a S3 bucket into the Knowledge Base for use in a RAG set up. The objects are chunks, embedded and then stored in a vector index.\" height=\"700\" width=\"700\" style=\"background-color:white; padding:1em;\" /></center> <br/>\n",
    "    \n",
    "\n",
    "    2. An application that receives a question from the user, looks up the knowledge base for relevant pieces of information (context) and then creates a prompt that includes the question and the context and provides it to an LLM for generating a response.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Once the data is available in the Bedrock knowledge base, then user questions can be answered using the following system design:\n",
    "\n",
    "<center><img src=\"images/retrieveAndGenerate.png\" alt=\"This image shows the retrieval augmented generation (RAG) system design setup with knowledge bases, S3, and AOSS. Knowledge corpus is ingested into a vector database using Amazon Bedrock Knowledge Base Agent and then RAG approach is used to work question answering. The question is converted into embeddings followed by semantic similarity search to get similar documents. With the user prompt being augmented with the RAG search response, the LLM is invoked to get the final raw response for the user.\" height=\"700\" width=\"700\" style=\"background-color:white; padding:1em;\" /></center> <br/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4588c8c3",
   "metadata": {},
   "source": [
    "# Data\n",
    "Let's use the publicly available [Bedrock user guide](https://docs.aws.amazon.com/pdfs/bedrock/latest/userguide/bedrock-ug.pdf) to inform the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b522396d-33e9-4251-9c72-09909436e25b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!wget -P data/ -N https://docs.aws.amazon.com/pdfs/bedrock/latest/userguide/bedrock-ug.pdf --no-check-certificate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb05ed4-b63c-4e3c-a702-28021c506ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload data to S3\n",
    "dataset_file_local_path = \"data/bedrock-ug.pdf\"\n",
    "input_s3_url = sagemaker.Session().upload_data(\n",
    "    path=dataset_file_local_path, bucket=bucket_name\n",
    ")\n",
    "print(f\"Upload the dataset to {input_s3_url}\")\n",
    "\n",
    "%store input_s3_url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41dbbc4e-039f-49d8-8de0-7ce89ca30349",
   "metadata": {},
   "source": [
    "# Steps\n",
    "\n",
    "1. Create Amazon Bedrock Knowledge Base execution role with necessary policies for accessing data from S3 and writing embeddings into OSS.\n",
    "2. Create an empty OpenSearch serverless index.\n",
    "3. Create Amazon Bedrock knowledge base\n",
    "4. Create a data source within knowledge base which will connect to Amazon S3\n",
    "5. Start an ingestion job using KB APIs which will read data from s3, chunk it, convert chunks into embeddings using Amazon Titan Embeddings model and then store these embeddings in AOSS. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ae93890e-c684-4286-9234-91a3e058cd8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!export PYTHONPATH='./lab1/'\n",
    "# import sys\n",
    "# sys.path.insert(0,'./lab1/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0140ab19-a682-4855-b3ee-10589637211f",
   "metadata": {},
   "outputs": [],
   "source": [
    "kb_db_file_uri = \"data\"\n",
    "\n",
    "# if a kb already exists we can use the same, else the infra setup code will create one by itself using the bedrock user guide.\n",
    "use_existing_kb = False\n",
    "existing_kb_id = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6c31b846-fa1f-43e0-ad9f-814bdb7dc3b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from rag_setup.create_kb_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e53913c-2aeb-42b4-be3e-c2eea8136eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# For new KB it takes around ~6 minutes for this setup to complete on a t2.medium instance.\n",
    "infra_response = setup_knowledge_base(\n",
    "    bucket_name, kb_db_file_uri, use_existing_kb, existing_kb_id\n",
    ")\n",
    "infra_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec5179f-d77c-4d52-918c-f4fb37b70f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "kb_id = infra_response[\"knowledge_base_db_id\"]\n",
    "random_id = infra_response[\"prefix_infra\"]\n",
    "# keep the kb_id for invocation later in the invoke request\n",
    "%store kb_id\n",
    "%store bucket_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3cf9eb-6d1e-4ae4-81db-a23379eae74d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kb_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bf3a828f-e46d-4a14-8011-122a399b8af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# allow time for KB to be ready\n",
    "time.sleep(180)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c35041-9487-46e6-9a36-fd8d408712c4",
   "metadata": {},
   "source": [
    "# Chat with the model using the knowledge base by providing the generated KB_ID\n",
    "### Using RetrieveAndGenerate API\n",
    "Behind the scenes, RetrieveAndGenerate API converts queries into embeddings, searches the knowledge base, and then augments the foundation model prompt with the search results as context information and returns the FM-generated response to the question. For multi-turn conversations, Knowledge Bases manage short-term memory of the conversation to provide more contextual results.The output of the RetrieveAndGenerate API includes the generated response, source attribution as well as the retrieved text chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2ca09025-187d-4aa4-8a0f-b6524e734ea6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pp = pprint.PrettyPrinter(indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0480df70-ab90-4c5d-bf07-8acabf7a1a4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kb_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "06645e80-eebb-4883-a086-70ccfdf604c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "bedrock_agent_runtime_client = boto3.client(\"bedrock-agent-runtime\", region_name=region)\n",
    "\n",
    "\n",
    "def ask_bedrock_llm_with_knowledge_base(\n",
    "    query,\n",
    "    kb_id=kb_id,\n",
    "    model_arn=llm_model_id,\n",
    ") -> str:\n",
    "    response = bedrock_agent_runtime_client.retrieve_and_generate(\n",
    "        input={\"text\": query},\n",
    "        retrieveAndGenerateConfiguration={\n",
    "            \"type\": \"KNOWLEDGE_BASE\",\n",
    "            \"knowledgeBaseConfiguration\": {\n",
    "                \"knowledgeBaseId\": kb_id,\n",
    "                \"modelArn\": model_arn,\n",
    "            },\n",
    "        },\n",
    "    )\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a151db3d-b340-46b2-bd57-491714d2068c",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is Amazon Bedrock?\"\n",
    "\n",
    "response = ask_bedrock_llm_with_knowledge_base(query, kb_id)\n",
    "generated_text = response[\"output\"][\"text\"]\n",
    "citations = response[\"citations\"]\n",
    "contexts = []\n",
    "for citation in citations:\n",
    "    retrievedReferences = citation[\"retrievedReferences\"]\n",
    "    for reference in retrievedReferences:\n",
    "        contexts.append(reference[\"content\"][\"text\"])\n",
    "print(f\"---------- Generated using Anthropic Claude 3 Sonnet:\")\n",
    "pp.pprint(generated_text)\n",
    "print(f\"---------- The citations for the response:\")\n",
    "pp.pprint(contexts)\n",
    "print(kb_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0892703-6fa4-4bac-a3b1-7cd30523b888",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Is it possible to purchase provisioned throughput for Anthropic Claude Sonnet on Amazon Bedrock?\"\n",
    "\n",
    "response = ask_bedrock_llm_with_knowledge_base(query, kb_id)\n",
    "generated_text = response[\"output\"][\"text\"]\n",
    "citations = response[\"citations\"]\n",
    "contexts = []\n",
    "for citation in citations:\n",
    "    retrievedReferences = citation[\"retrievedReferences\"]\n",
    "    for reference in retrievedReferences:\n",
    "        contexts.append(reference[\"content\"][\"text\"])\n",
    "print(f\"---------- Generated using Anthropic Claude 3 Sonnet:\")\n",
    "pp.pprint(generated_text)\n",
    "print(f\"---------- The citations for the response:\")\n",
    "pp.pprint(contexts)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424c0f09-2ecf-40c5-9dd2-1c42a7af6231",
   "metadata": {},
   "source": [
    "# Contextual Grounding with Amazon Bedrock Guardrails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810e1acf-ddbe-4e93-bf96-32e7282a7db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create guardrail\n",
    "bedrock_client = boto3.client(\"bedrock\")\n",
    "guardrail_name = f\"bedrock-rag-grounding-guardrail-{random_id}\"\n",
    "print(guardrail_name)\n",
    "guardrail_response = bedrock_client.create_guardrail(\n",
    "    name=guardrail_name,\n",
    "    description=\"Guardrail for ensuring relevance and grounding of model responses in RAG powered chatbot\",\n",
    "    contextualGroundingPolicyConfig={\n",
    "        \"filtersConfig\": [\n",
    "            {\"type\": \"GROUNDING\", \"threshold\": 0.5},\n",
    "            {\"type\": \"RELEVANCE\", \"threshold\": 0.5},\n",
    "        ]\n",
    "    },\n",
    "    blockedInputMessaging=\"Can you please rephrase your question?\",\n",
    "    blockedOutputsMessaging=\"Sorry, I am not able to find the correct answer to your query - Can you try reframing your query to be more specific\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d615898-b2a3-4b8c-8830-f6710fc86ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "guardrailId = guardrail_response[\"guardrailId\"]\n",
    "guardrail_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a503292a-b84c-46d5-8fe2-f9d199198f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "guardrail_version = bedrock_client.create_guardrail_version(\n",
    "    guardrailIdentifier=guardrail_response[\"guardrailId\"],\n",
    "    description=\"Working version of RAG app guardrail with higher thresholds for contextual grounding\",\n",
    ")\n",
    "print(guardrail_version)\n",
    "guardrailVersion = guardrail_response[\"version\"]\n",
    "print(guardrailId)\n",
    "%store guardrailId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0dfbd6ac-6411-47dd-b645-33df81614878",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve and Generate using Guardrail\n",
    "\n",
    "bedrock_agent_runtime_client = boto3.client(\"bedrock-agent-runtime\", region_name=region)\n",
    "\n",
    "\n",
    "def retrieve_and_generate_with_guardrail(\n",
    "    query, kb_id, model_arn=llm_model_id, session_id=None\n",
    "):\n",
    "\n",
    "    prompt_template = \"You are a helpful AI assistant to help users understand documented risks in various projects. \\\n",
    "    Answer the user query based on the context retrieved. If you dont know the answer, dont make up anything. \\\n",
    "    Only answer based on what you know from the provided context. You can ask the user for clarifying questions if anything is unclear\\\n",
    "    But generate an answer only when you are confident about it and based on the provided context.\\\n",
    "    User Query: $query$\\\n",
    "    Context: $search_results$\"\n",
    "\n",
    "    response = bedrock_agent_runtime_client.retrieve_and_generate(\n",
    "        input={\"text\": query},\n",
    "        retrieveAndGenerateConfiguration={\n",
    "            \"type\": \"KNOWLEDGE_BASE\",\n",
    "            \"knowledgeBaseConfiguration\": {\n",
    "                \"generationConfiguration\": {\n",
    "                    \"guardrailConfiguration\": {\n",
    "                        \"guardrailId\": guardrailId,\n",
    "                        \"guardrailVersion\": guardrailVersion,\n",
    "                    },\n",
    "                    \"inferenceConfig\": {\n",
    "                        \"textInferenceConfig\": {\"temperature\": 0.7, \"topP\": 0.25}\n",
    "                    },\n",
    "                    \"promptTemplate\": {\"textPromptTemplate\": prompt_template},\n",
    "                },\n",
    "                \"knowledgeBaseId\": kb_id,\n",
    "                \"modelArn\": model_arn,\n",
    "                \"retrievalConfiguration\": {\n",
    "                    \"vectorSearchConfiguration\": {\"overrideSearchType\": \"SEMANTIC\"}\n",
    "                },\n",
    "            },\n",
    "        },\n",
    "    )\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f710a5-74c9-4480-8308-e9b362730428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Knowledge BAse ID\n",
    "\n",
    "query = \"What is Amazon Bedrock?\"\n",
    "# query = \"Is it possible to purchase provisioned throughput for Anthropic Claude Sonnet on Amazon Bedrock?\"\n",
    "\n",
    "model_response = retrieve_and_generate_with_guardrail(query, kb_id)\n",
    "\n",
    "print(model_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56677a75-464c-453b-b2bd-7813f2a65d1a",
   "metadata": {},
   "source": [
    "# Evaluating RAG with RAGAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4e33826a-0836-4ad1-9553-dcb7df3f87c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pprint\n",
    "from botocore.client import Config\n",
    "from langchain.llms.bedrock import Bedrock\n",
    "from langchain_community.chat_models.bedrock import BedrockChat\n",
    "from langchain.embeddings import BedrockEmbeddings\n",
    "from langchain.retrievers.bedrock import AmazonKnowledgeBasesRetriever\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=2)\n",
    "\n",
    "bedrock_config = Config(\n",
    "    connect_timeout=120, read_timeout=120, retries={\"max_attempts\": 0}\n",
    ")\n",
    "bedrock_client = boto3.client(\"bedrock-runtime\")\n",
    "bedrock_agent_client = boto3.client(\"bedrock-agent-runtime\", config=bedrock_config)\n",
    "\n",
    "llm_for_text_generation = BedrockChat(model_id=llm_model_id, client=bedrock_client)\n",
    "\n",
    "llm_for_evaluation = BedrockChat(model_id=llm_model_id, client=bedrock_client)\n",
    "\n",
    "bedrock_embeddings = BedrockEmbeddings(\n",
    "    model_id=embedding_model_id, client=bedrock_client\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3511743-c8c2-4e3c-949a-79bf9bec79cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "test = pd.read_csv(\"data/bedrock-user-guide-test.csv\")\n",
    "test = test.dropna()\n",
    "test.style.set_properties(**{\"text-align\": \"left\", \"border\": \"1px solid black\"})\n",
    "test.to_string(justify=\"left\", index=False)\n",
    "with pd.option_context(\"display.max_colwidth\", None):\n",
    "    pretty_print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d0a5f15a-8053-4edc-97e5-c6b96641c219",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "questions = test[\"Question/prompt\"].tolist()\n",
    "ground_truths = [[gt] for gt in test[\"Correct answer\"].tolist()]\n",
    "\n",
    "answers = []\n",
    "contexts = []\n",
    "\n",
    "for query in questions:\n",
    "    response = ask_bedrock_llm_with_knowledge_base(query, kb_id)\n",
    "    generatedResult = response[\"output\"][\"text\"]\n",
    "    answers.append(generatedResult)\n",
    "    contexts.append(\n",
    "        [\n",
    "            doc[\"content\"][\"text\"]\n",
    "            for doc in response[\"citations\"][0][\"retrievedReferences\"]\n",
    "        ]\n",
    "    )\n",
    "\n",
    "# To dict\n",
    "data = {\n",
    "    \"question\": questions,\n",
    "    \"answer\": answers,\n",
    "    \"contexts\": contexts,\n",
    "    \"ground_truths\": ground_truths,\n",
    "}\n",
    "\n",
    "# Convert dict to dataset\n",
    "dataset = Dataset.from_dict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16eb532-3c65-4adf-b6f2-cab77d11cc81",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_recall,\n",
    "    context_precision,\n",
    "    context_entity_recall,\n",
    "    answer_similarity,\n",
    "    answer_correctness,\n",
    ")\n",
    "\n",
    "from ragas.metrics.critique import correctness\n",
    "\n",
    "# specify the metrics here, kept one for now, we can add more.\n",
    "metrics = [answer_relevancy]\n",
    "\n",
    "result = evaluate(\n",
    "    dataset=dataset,\n",
    "    metrics=metrics,\n",
    "    llm=llm_for_evaluation,\n",
    "    embeddings=bedrock_embeddings,\n",
    ")\n",
    "\n",
    "ragas_df = result.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76503a5-36de-4365-a390-a37dba57c439",
   "metadata": {},
   "outputs": [],
   "source": [
    "ragas_df.style.set_properties(**{\"text-align\": \"left\", \"border\": \"1px solid black\"})\n",
    "ragas_df.to_string(justify=\"left\", index=False)\n",
    "with pd.option_context(\"display.max_colwidth\", None):\n",
    "    pretty_print(ragas_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95cb27ab",
   "metadata": {},
   "source": [
    "### <a >Challenge Exercise :: Try it Yourself! </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c62acdc",
   "metadata": {},
   "source": [
    "\n",
    "<div style=\"border: 4px solid coral; text-align: left; margin: auto;\">\n",
    "    <br>\n",
    "    <p style=\"text-align: center; margin: auto;\"><b>Try the following exercises on this lab and note the observations.</b></p>\n",
    "<p style=\" text-align: left; margin: auto;\">\n",
    "<ol>\n",
    "    <li>Test the RAG based LLM with more questions about Amazon Bedrock. </li>\n",
    "<li>Look the the citations or retrieved references and see if the answer generated by the RAG chatbot aligns with these retrieved contexts. What response do you get when the retrieved context comes up empty? </li>\n",
    "<li>Apply system prompts to RAG as well as amazon Bedrock Guardrails and test which is more consistent in blocking responses when the model response is hallucinated </li>\n",
    "<li>Run the tutorial for RAG Checker and compare the difference with RAGAS evaluation framework: https://github.com/amazon-science/RAGChecker/blob/main/tutorial/ragchecker_tutorial_en.md </li>\n",
    "</ol>\n",
    "<br>\n",
    "</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dfc89ac-b0e8-438a-bba2-12bc3a4a3f94",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "We now have an understanding of parameters which influence hallucinations in Large Language Models. We learnt how to set up Retrieval Augmented Generation to provide a context to the model while answering.\n",
    "We used Contextual grounding in Amazon Bedrock Guardrials to intervene when hallucinations are detected.\n",
    "Finally we looked into the metrics of RAGAS and how to use them to measure hallucinations in your RAG powered chatbot.\n",
    "\n",
    "In the next lab, we will:\n",
    "1. Build a custom hallucination detector\n",
    "2. Use Amazon Bedrock Agents to intervene when hallucinations are detected\n",
    "3. Call a human for support when the LLM hallucinates\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
