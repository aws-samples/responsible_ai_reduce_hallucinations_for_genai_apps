{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc84eb37-3b34-4506-8ebc-c70f28166077",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<center><img src=\"images/2024_reInvent_Logo_wDate_Black_V3.png\" alt=\"drawing\" width=\"400\" style=\"background-color:white; padding:1em;\" /></center> <br/>\n",
    "\n",
    "# <a name=\"0\">re:Invent 2024 | Lab 1: Build your RAG powered chatbot  </a>\n",
    "## <a name=\"0\">Build a chatbot with Knowledge Bases and Guardrails to detect and remediate hallucinations </a>\n",
    "\n",
    "## Lab Overview\n",
    "In this lab, you will:\n",
    "1. Take a deeper look at which LLM parameters influence or control for model hallucinations\n",
    "2. Understand how Retrieval Augmented Generation can control for hallucinations\n",
    "3. Apply contextual grounding in Amazon Bedrock Guardrails to intervene when a model hallucinates\n",
    "4. Use RAGAS evaluation and understand which metrics help us measure hallucinations\n",
    "\n",
    "## Dataset\n",
    "For this workshop, we will use the [Bedrock User Guide](https://docs.aws.amazon.com/pdfs/bedrock/latest/userguide/bedrock-ug.pdf) available as a PDF file.\n",
    "## Use-Case Overview\n",
    "In this lab, we want to develop a chatbot which can answer questions about Amazon Bedrock as factually as possible. We will work with Retrieval Augmented Generation using [Amazon Bedrock Knowledge Bases](https://aws.amazon.com/bedrock/knowledge-bases/) and apply [Amazon Guardrails](https://aws.amazon.com/bedrock/guardrails/) to intervene when hallucinations are detected.\n",
    "\n",
    "\n",
    "#### Lab Sections\n",
    "\n",
    "This lab notebook has the following sections:\n",
    "    \n",
    "Please work top to bottom of this notebook and don't skip sections as this could lead to error messages due to missing code.\n",
    "\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f260f34e-c753-4456-8643-639a3b4cdcfa",
   "metadata": {},
   "source": [
    "# Star Github repository for future reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb09da1-5d9e-4496-b0c8-ed5ae80911ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "\n",
    "<a class=\"github-button\" href=\"https://github.com/aws-samples/responsible_ai_aim325_reduce_hallucinations_for_genai_apps\" data-color-scheme=\"no-preference: light; light: light; dark: dark;\" data-icon=\"octicon-star\" data-size=\"large\" data-show-count=\"true\" aria-label=\"Star Reduce Hallucinations workshop on GitHub\">Star</a>\n",
    "<script async defer src=\"https://buttons.github.io/buttons.js\"></script>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc006a3-48d5-40a5-9eb8-ea9bcd3d85e9",
   "metadata": {},
   "source": [
    "# Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba631f57-61f6-422f-8a42-472cb6046eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install -r ../requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a617e1-d0ef-44d3-bb56-c1f11ee326dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# restart kernel\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4545e3ba-8b68-4251-9177-8a506d1dae04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import json\n",
    "import boto3\n",
    "from time import gmtime, strftime, sleep\n",
    "import random\n",
    "import zipfile\n",
    "import uuid\n",
    "from rag_setup.create_kb_utils import *\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "from botocore.config import Config\n",
    "\n",
    "import numpy as np  \n",
    "import pandas as pd \n",
    "import sagemaker\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6edb7c-dfa1-4460-a365-e3f287951ddb",
   "metadata": {},
   "source": [
    "## Set constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b2c77d-ebdb-4d0a-acdd-04558b7797a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get some variables you need to interact with SageMaker service\n",
    "boto_session = boto3.Session()\n",
    "region = boto_session.region_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34d99ca-e263-44e3-9e35-685a7a8f9859",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embedding_model_id=\"amazon.titan-embed-text-v2:0\"\n",
    "llm_model_id=\"anthropic.claude-3-sonnet-20240229-v1:0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a44d408-1086-48fc-a329-f757fed9b02a",
   "metadata": {},
   "source": [
    "# 1. Chat with Anthropic Claude 3 Sonnet through Bedrock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340537e1-ef81-4aef-a767-f0df5e1fef45",
   "metadata": {},
   "outputs": [],
   "source": [
    "RETRY_CONFIG = Config(\n",
    "    retries={\n",
    "        'max_attempts': 5,            # Maximum number of retry attempts\n",
    "        'mode': 'adaptive'            # Adaptive mode adjusts based on request limits\n",
    "    },\n",
    "    read_timeout=1000,\n",
    "    connect_timeout=1000\n",
    ")\n",
    "\n",
    "bedrock_runtime = boto3.client(\n",
    "    service_name='bedrock-runtime',\n",
    "    region_name=region,\n",
    "    config=RETRY_CONFIG)\n",
    "\n",
    "def generate_message_claude(\n",
    "    query, \n",
    "    system_prompt=\"\", \n",
    "    max_tokens=1000,\n",
    "    model_id='anthropic.claude-3-sonnet-20240229-v1:0',\n",
    "    temperature=1,\n",
    "    top_p=0.999,\n",
    "    top_k=250\n",
    "):\n",
    "    # Prompt with user turn only.\n",
    "    user_message = {\"role\": \"user\", \"content\": query}\n",
    "    messages = [user_message]\n",
    "    body = json.dumps(\n",
    "        {\n",
    "            \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "            \"max_tokens\": max_tokens,\n",
    "            \"system\": system_prompt,\n",
    "            \"messages\": messages,\n",
    "            \"temperature\": temperature,\n",
    "            \"top_p\": top_p,\n",
    "            \"top_k\": top_k\n",
    "        }\n",
    "    )\n",
    "\n",
    "    response = bedrock_runtime.invoke_model(body=body, modelId=model_id)\n",
    "    response_body = json.loads(response.get('body').read())\n",
    "    return response_body['content'][0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd47d4b-8455-4fcf-ae91-874ab2f7c0e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "query = 'How does Amazon Bedrock Guardrails work?'\n",
    "\n",
    "response = generate_message_claude(query)\n",
    "pp.pprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca8a8b4-4bd2-453e-9796-17b2a9f25234",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### The LLM might hallucinate on what Amazon Bedrock Guardrails are if it does not have the right context to produce a factual answer. [Amazon Bedrock Guardrails](https://aws.amazon.com/bedrock/guardrails/) helps to implement safeguards customized to your application requirements and your responsible AI policies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5637a9c5-0739-425d-a7bf-21b2083ee271",
   "metadata": {},
   "source": [
    "<div style=\"border: 4px solid coral; text-align: left; margin: auto; padding-left: 20px; padding-right: 20px\">\n",
    "    <h4>If LLM call to Bedrock did not work, enable model access on Amazon Bedrock console</h4>\n",
    "</div>\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621f0fa6-9637-4c65-bad6-0e1eacc692e9",
   "metadata": {},
   "source": [
    "## 1.1 Apply System Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba2bec7-62f9-4e97-8cc2-9eafadccc8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = 'You are a helpful AI assistant. You try to answer the user queries to the best of your knowledge.\\\n",
    "If you are unsure of the answer, do not make up any information.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e203f5e9-2d93-4a79-b8ad-754d656f3008",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'Is it possible to purchase provisioned throughput for Anthropic Claude models on Amazon Bedrock?'\n",
    "\n",
    "response = generate_message_claude(query, system_prompt)\n",
    "pp.pprint(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5057db14-729d-4d64-8355-fd6fe2ee2398",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'How do Amazon Bedrock Guardrails work?'\n",
    "\n",
    "response = generate_message_claude(query, system_prompt)\n",
    "pp.pprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77f9196-7e32-4ef0-b516-b8801a688aa3",
   "metadata": {},
   "source": [
    "## 1.2 Understanding LLM generation parameters\n",
    "### 1. Temperature\n",
    "\n",
    "Affects the shape of the probability distribution for the predicted output and influences the likelihood of the model selecting lower-probability outputs.\n",
    "\n",
    "- Choose a lower value to influence the model to select higher-probability outputs.\n",
    "\n",
    "- Choose a higher value to influence the model to select lower-probability outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f610c2e0-e810-47f4-89f9-9980319e32dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'Create a haiku about a unicorn'\n",
    "\n",
    "response = generate_message_claude(query, temperature=0.9)\n",
    "pp.pprint(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ff7dba-2f3f-4bd2-87ce-0d84c3c03a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'Create a haiku about a unicorn'\n",
    "\n",
    "response = generate_message_claude(query, temperature=0.1)\n",
    "pp.pprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47723758-0f8c-4190-94ef-3bdd4d386908",
   "metadata": {},
   "source": [
    "### 2. top_p – Use nucleus sampling.\n",
    "\n",
    "The percentage of most-likely candidates that the model considers for the next token.\n",
    "\n",
    "- Choose a lower value to decrease the size of the pool and limit the options to more likely outputs.\n",
    "\n",
    "- Choose a higher value to increase the size of the pool and allow the model to consider less likely outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e300c40f-1b7b-4db1-bc5f-5f483507262e",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'Who is mans best friend?'\n",
    "\n",
    "response = generate_message_claude(query, top_p=0.1)\n",
    "pp.pprint(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfa57b3-bf14-4ba0-b86f-0c6bb79b1fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'Who is mans best friend?'\n",
    "\n",
    "response = generate_message_claude(query, top_p=0.9)\n",
    "pp.pprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7946bd-976d-4a8d-81ef-f7ff7e164f94",
   "metadata": {},
   "source": [
    "### 3. top_k: Only sample from the top K options for each subsequent token.\n",
    "\n",
    "The number of most-likely candidates that the model considers for the next token.\n",
    "\n",
    "- Choose a lower value to decrease the size of the pool and limit the options to more likely outputs.\n",
    "\n",
    "- Choose a higher value to increase the size of the pool and allow the model to consider less likely outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf753a4-8929-46ef-8aea-7ef040abb096",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'What is the universe'\n",
    "\n",
    "response = generate_message_claude(query, top_k=3)\n",
    "pp.pprint(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea29f890-b8d8-4b8e-9b40-7e9b3b8941d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'What is the universe'\n",
    "\n",
    "response = generate_message_claude(query, top_k=100)\n",
    "pp.pprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9caf35-dccd-4c16-82eb-ea3e1327bc80",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Retrieval Augmented Generation\n",
    "We are using the Retrieval Augmented Generation (RAG) technique with Amazon Bedrock. A RAG implementation consists of two parts:\n",
    "\n",
    "    1. A data pipeline that ingests that from documents (typically stored in Amazon S3) into a Knowledge Base i.e. a vector database such as Amazon OpenSearch Service Serverless (AOSS) so that it is available for lookup when a question is received.\n",
    "\n",
    "The data pipeline represents an undifferentiated heavy lifting and can be implemented using Amazon Bedrock Knowledge Bases. We can now connect an S3 bucket to a vector database such as AOSS and have a Bedrock Knowledge Bases read the objects (html, pdf, text etc.), chunk them, and then convert these chunks into embeddings using Amazon Titan Embeddings model and then store these embeddings in AOSS. All of this without having to build, deploy, and manage the data pipeline.\n",
    "\n",
    "<center><img src=\"images/fully_managed_ingestion.png\" alt=\"This image shows how Aazon Bedrock Knowledge Bases ingests objects in a S3 bucket into the Knowledge Base for use in a RAG set up. The objects are chunks, embedded and then stored in a vector index.\" height=\"700\" width=\"700\" style=\"background-color:white; padding:1em;\" /></center> <br/>\n",
    "    \n",
    "\n",
    "    2. An application that receives a question from the user, looks up the knowledge base for relevant pieces of information (context) and then creates a prompt that includes the question and the context and provides it to an LLM for generating a response.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Once the data is available in the Bedrock knowledge base, then user questions can be answered using the following system design:\n",
    "\n",
    "<center><img src=\"images/retrieveAndGenerate.png\" alt=\"This image shows the retrieval augmented generation (RAG) system design setup with knowledge bases, S3, and AOSS. Knowledge corpus is ingested into a vector database using Amazon Bedrock Knowledge Base Agent and then RAG approach is used to work question answering. The question is converted into embeddings followed by semantic similarity search to get similar documents. With the user prompt being augmented with the RAG search response, the LLM is invoked to get the final raw response for the user.\" height=\"700\" width=\"700\" style=\"background-color:white; padding:1em;\" /></center> <br/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4588c8c3",
   "metadata": {},
   "source": [
    "# Data\n",
    "Let's use publicly available [Bedrock user guide](https://docs.aws.amazon.com/pdfs/bedrock/latest/userguide/bedrock-ug.pdf) to inform the model. If you are running this workshop in a AWS led evet, this dataset is already uploaded to the S3 path (s3://bedrock-rag-us-west-2-<account_id>/kb-docs/bedrock-ug.pdf) and connected as a data source to the Amazon Bedrock Knowledge Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe868f1-d5b1-490f-8e54-e5a856ca1b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "kb_id = None\n",
    "kb_list = bedrock_agent_client.list_knowledge_bases()['knowledgeBaseSummaries']\n",
    "for kb in kb_list:\n",
    "    if kb['name'] == 'bedrock_user_guide_kb':\n",
    "        kb_id = kb['knowledgeBaseId']\n",
    "\n",
    "if kb_id is None:\n",
    "    print(f\"Please navigate to Amazon Bedrock > Builder Tools > Knowledge Bases.\\\n",
    "    Click on 'bedrock_user_guide_kb' KB. Go to Datasource section and click `Sync` button.\\\n",
    "    Please wait for it to finish, then re-run this cell. \")\n",
    "print(kb_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec5179f-d77c-4d52-918c-f4fb37b70f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep the kb_id for invocation later in the invoke request\n",
    "%store kb_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c35041-9487-46e6-9a36-fd8d408712c4",
   "metadata": {},
   "source": [
    "# Chat with the model using the knowledge base by providing the generated KB_ID\n",
    "### Using RetrieveAndGenerate API\n",
    "Behind the scenes, RetrieveAndGenerate API converts queries into embeddings, searches the knowledge base, and then augments the foundation model prompt with the search results as context information and returns the FM-generated response to the question. For multi-turn conversations, Knowledge Bases manage short-term memory of the conversation to provide more contextual results.The output of the RetrieveAndGenerate API includes the generated response, source attribution as well as the retrieved text chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06645e80-eebb-4883-a086-70ccfdf604c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "bedrock_agent_runtime_client = boto3.client(\"bedrock-agent-runtime\", region_name=region, config=RETRY_CONFIG)\n",
    "\n",
    "def ask_bedrock_llm_with_knowledge_base(query,\n",
    "                                        kb_id=kb_id,\n",
    "                                        model_arn=llm_model_id,\n",
    "                                        temperature=0,\n",
    "                                        top_p=1,\n",
    "                                        ) -> str:\n",
    "    response = bedrock_agent_runtime_client.retrieve_and_generate(\n",
    "        input={\n",
    "            'text': query\n",
    "        },\n",
    "        retrieveAndGenerateConfiguration={\n",
    "            'type': 'KNOWLEDGE_BASE',\n",
    "            'knowledgeBaseConfiguration': {\n",
    "                'knowledgeBaseId': kb_id,\n",
    "                'modelArn': model_arn,\n",
    "                'generationConfiguration': {\n",
    "                    'inferenceConfig': {\n",
    "                        'textInferenceConfig': {\n",
    "                            'maxTokens': 2048,\n",
    "                            'temperature': temperature,\n",
    "                            'topP': top_p,\n",
    "                        }\n",
    "                    },\n",
    "                    'promptTemplate': {\n",
    "                        'textPromptTemplate': 'You are a helpful AI assistant. You try to answer the user queries based on the provided context.\\\n",
    "                        If you are unsure of the answer, do not make up any information. Context to the user query is $search_results$ \\\n",
    "                        $output_format_instructions$'\n",
    "                    }\n",
    "                },\n",
    "            },\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a151db3d-b340-46b2-bd57-491714d2068c",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is Amazon Bedrock?\"\n",
    "\n",
    "response = ask_bedrock_llm_with_knowledge_base(query, kb_id, temperature=0)\n",
    "pretty_display_rag_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de961d7-3ca2-4fde-aea9-0f479b87e2ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pretty_display_rag_citations(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e05dee0-9210-4a73-a0dd-b0f061843fe6",
   "metadata": {},
   "source": [
    "### Change the temperature to choose a different amount of randomness in the model response\n",
    "- Choose a lower value to influence the model to select higher-probability outputs.\n",
    "\n",
    "- Choose a higher value to influence the model to select lower-probability outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c81ebda-0321-493e-a2db-e77d170b3dd4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "query = \"What is Amazon Bedrock?\"\n",
    "\n",
    "response = ask_bedrock_llm_with_knowledge_base(query, kb_id, temperature=0.8)\n",
    "pretty_display_rag_response(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31673ff-7e17-45d8-a1f7-330799665725",
   "metadata": {},
   "source": [
    "### Test another query!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0892703-6fa4-4bac-a3b1-7cd30523b888",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "query = \"Is it possible to purchase provisioned throughput for Anthropic Claude Sonnet on Amazon Bedrock?\"\n",
    "\n",
    "response = ask_bedrock_llm_with_knowledge_base(query, kb_id)\n",
    "pretty_display_rag_response(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424c0f09-2ecf-40c5-9dd2-1c42a7af6231",
   "metadata": {},
   "source": [
    "# Contextual Grounding Check with Amazon Bedrock Guardrails\n",
    "Contextual grounding check evaluates for hallucinations across two paradigms:\n",
    "\n",
    "- Grounding – This checks if the model response is factually accurate based on the source and is grounded in the source. Any new information introduced in the response will be considered un-grounded.\n",
    "\n",
    "- Relevance – This checks if the model response is relevant to the user query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810e1acf-ddbe-4e93-bf96-32e7282a7db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create guardrail\n",
    "\n",
    "random_id_suffix = str(uuid.uuid1())[:6] # get first 6 characters of uuid string to generate guardrail name suffix\n",
    "\n",
    "bedrock_client = boto3.client('bedrock')\n",
    "guardrail_name = f\"bedrock-rag-grounding-guardrail-{random_id_suffix}\"\n",
    "print(guardrail_name)\n",
    "\n",
    "guardrail_response = bedrock_client.create_guardrail(\n",
    "    name=guardrail_name,\n",
    "    description='Guardrail for ensuring relevance and grounding of model responses in RAG powered chatbot',\n",
    "    contextualGroundingPolicyConfig={\n",
    "        'filtersConfig': [\n",
    "            {\n",
    "                'type': 'GROUNDING',\n",
    "                'threshold': 0.5\n",
    "            },\n",
    "            {\n",
    "                'type': 'RELEVANCE',\n",
    "                'threshold': 0.5\n",
    "            },\n",
    "        ]\n",
    "    },\n",
    "    blockedInputMessaging='Can you please rephrase your question?',\n",
    "    blockedOutputsMessaging='Sorry, I am not able to find the correct answer to your query - Can you try reframing your query to be more specific'\n",
    ")\n",
    "guardrailId = guardrail_response['guardrailId']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a503292a-b84c-46d5-8fe2-f9d199198f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "guardrail_version = bedrock_client.create_guardrail_version(\n",
    "    guardrailIdentifier=guardrail_response['guardrailId'],\n",
    "    description='Working version of RAG app guardrail with higher thresholds for contextual grounding'\n",
    ")\n",
    "\n",
    "guardrailVersion = guardrail_response['version']\n",
    "\n",
    "%store guardrailId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfbd6ac-6411-47dd-b645-33df81614878",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve and Generate using Guardrail\n",
    "\n",
    "def retrieve_and_generate_with_guardrail(\n",
    "    query,\n",
    "    kb_id,\n",
    "    model_arn=llm_model_id,\n",
    "    session_id=None\n",
    "):\n",
    "\n",
    "    prompt_template = 'You are a helpful AI assistant to help users understand documented risks in various projects. \\\n",
    "    Answer the user query based on the context retrieved. If you dont know the answer, dont make up anything. \\\n",
    "    Only answer based on what you know from the provided context. You can ask the user for clarifying questions if anything is unclear\\\n",
    "    But generate an answer only when you are confident about it and based on the provided context.\\\n",
    "    User Query: $query$\\\n",
    "    Context: $search_results$\\\n",
    "    $output_format_instructions$'\n",
    "\n",
    "    response = bedrock_agent_runtime_client.retrieve_and_generate(\n",
    "        input={\n",
    "            'text': query\n",
    "        },\n",
    "        retrieveAndGenerateConfiguration={\n",
    "            'type': 'KNOWLEDGE_BASE',\n",
    "            'knowledgeBaseConfiguration': {\n",
    "                'generationConfiguration': {\n",
    "                    'guardrailConfiguration': {\n",
    "                        'guardrailId': guardrailId,\n",
    "                        'guardrailVersion': guardrailVersion\n",
    "                    },\n",
    "                    'inferenceConfig': {\n",
    "                        'textInferenceConfig': {\n",
    "                            'temperature': 0.1,\n",
    "                            'topP': 0.25\n",
    "                        }\n",
    "                    },\n",
    "                    'promptTemplate': {\n",
    "                        'textPromptTemplate': prompt_template\n",
    "                    }\n",
    "                },\n",
    "                'knowledgeBaseId': kb_id,\n",
    "                'modelArn': model_arn,\n",
    "                'retrievalConfiguration': {\n",
    "                    'vectorSearchConfiguration': {\n",
    "                        'overrideSearchType': 'SEMANTIC'\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d421f9-a926-47a9-847b-ac38ed0552ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'What is Generative AI?'\n",
    "\n",
    "model_response = retrieve_and_generate_with_guardrail(query, kb_id)\n",
    "\n",
    "pp.pprint(model_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4da3827-b217-422e-97a2-6c231c6322d9",
   "metadata": {},
   "source": [
    "<div style=\"border: 2px solid coral; text-align: left; margin: auto; padding-left: 20px; padding-right: 20px\">\n",
    "    <h4>The Guardrail intervenes when the generated model response is not grounded in a context</h4>\n",
    "</div>\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56677a75-464c-453b-b2bd-7813f2a65d1a",
   "metadata": {},
   "source": [
    "# Evaluating RAG with RAGAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e33826a-0836-4ad1-9553-dcb7df3f87c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from botocore.client import Config\n",
    "from langchain.llms.bedrock import Bedrock\n",
    "from langchain_community.chat_models.bedrock import BedrockChat\n",
    "from langchain.embeddings import BedrockEmbeddings\n",
    "from langchain.retrievers.bedrock import AmazonKnowledgeBasesRetriever\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_core.globals import set_verbose, set_debug\n",
    "\n",
    "# Disable verbose logging\n",
    "set_verbose(False)\n",
    "\n",
    "# Disable debug logging\n",
    "set_debug(False)\n",
    "\n",
    "bedrock_config = Config(connect_timeout=120, read_timeout=120, retries={'max_attempts': 0})\n",
    "bedrock_client = boto3.client('bedrock-runtime')\n",
    "bedrock_agent_client = boto3.client(\"bedrock-agent-runtime\",\n",
    "                              config=bedrock_config\n",
    "                              )\n",
    "\n",
    "llm_for_text_generation = BedrockChat(model_id=llm_model_id, client=bedrock_client)\n",
    "\n",
    "llm_for_evaluation = BedrockChat(model_id=llm_model_id, client=bedrock_client)\n",
    "\n",
    "bedrock_embeddings = BedrockEmbeddings(model_id=embedding_model_id,client=bedrock_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3511743-c8c2-4e3c-949a-79bf9bec79cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "test = pd.read_csv('data/bedrock-user-guide-test.csv').dropna()\n",
    "test.style.set_properties(**{'text-align': 'left', 'border': '1px solid black'})\n",
    "test.to_string(justify='left', index=False)\n",
    "with pd.option_context(\"display.max_colwidth\", None):\n",
    "    display(pd.DataFrame(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a5f15a-8053-4edc-97e5-c6b96641c219",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "questions = test['Question/prompt'].tolist()\n",
    "ground_truth = [gt for gt in test['Correct answer'].tolist()]\n",
    "\n",
    "answers = []\n",
    "contexts = []\n",
    "\n",
    "for query in questions:\n",
    "    response = ask_bedrock_llm_with_knowledge_base(query, kb_id)\n",
    "    generatedResult = response['output']['text']\n",
    "    answers.append(generatedResult)\n",
    "\n",
    "    context = []\n",
    "    citations = response[\"citations\"]\n",
    "    for citation in citations:\n",
    "        retrievedReferences = citation[\"retrievedReferences\"]\n",
    "        for reference in retrievedReferences:\n",
    "            context.append(reference[\"content\"][\"text\"])\n",
    "    contexts.append(context)\n",
    "\n",
    "# To dict\n",
    "data = {\n",
    "    \"question\": questions,\n",
    "    \"answer\": answers,\n",
    "    \"contexts\": contexts,\n",
    "    \"ground_truth\": ground_truth\n",
    "}\n",
    "\n",
    "# Convert dict to dataset\n",
    "dataset = Dataset.from_dict(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bce932e-94b8-4068-acc4-c0d85287daaa",
   "metadata": {},
   "source": [
    "### Let us deep dive into the two RAGAS metrics that we will use in the next lab\n",
    "\n",
    "#### 1. answer_relevancy: Answer Relevancy metric focuses on assessing how pertinent the generated answer is to the given prompt. A lower score is assigned to answers that are incomplete or contain redundant information and higher scores indicate better relevancy. This metric is computed using the user_input, the retrived_contexts and the response.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16eb532-3c65-4adf-b6f2-cab77d11cc81",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    answer_relevancy\n",
    ")\n",
    "\n",
    "#specify the metrics here, kept one for now, we can add more.\n",
    "metrics_ar = [\n",
    "        answer_relevancy\n",
    "    ]\n",
    "\n",
    "result_ar = evaluate(\n",
    "    dataset = dataset, \n",
    "    metrics=metrics_ar,\n",
    "    llm=llm_for_evaluation,\n",
    "    embeddings=bedrock_embeddings,\n",
    "    raise_exceptions=False\n",
    ")\n",
    "\n",
    "ragas_df_ar= result_ar.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76503a5-36de-4365-a390-a37dba57c439",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ragas_df_ar.style.set_properties(**{'text-align': 'left', 'border': '1px solid black'})\n",
    "ragas_df_ar.to_string(justify='left', index=False)\n",
    "with pd.option_context(\"display.max_colwidth\", None):\n",
    "    display(pd.DataFrame(ragas_df_ar))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2380a8fa-076b-4b3c-bd75-746d0e2d1555",
   "metadata": {},
   "source": [
    "#### 2. answer_correctness: The assessment of Answer Correctness involves gauging the accuracy of the generated answer when compared to the ground truth. This evaluation relies on the ground truth and the answer, with scores ranging from 0 to 1. A higher score indicates a closer alignment between the generated answer and the ground truth, signifying better correctness. Answer correctness encompasses two critical aspects: semantic similarity between the generated answer and the ground truth, as well as factual similarity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7cbce17-792b-47da-b9a9-1053e3ae11be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    answer_correctness\n",
    ")\n",
    "\n",
    "metrics_ac = [\n",
    "        answer_correctness\n",
    "    ]\n",
    "\n",
    "result_ac = evaluate(\n",
    "    dataset = dataset, \n",
    "    metrics=metrics_ac,\n",
    "    llm=llm_for_evaluation,\n",
    "    embeddings=bedrock_embeddings,\n",
    "    raise_exceptions=False\n",
    ")\n",
    "\n",
    "ragas_df_ac = result_ac.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78fd21d-f3e4-496d-8f34-bbfda4c91687",
   "metadata": {},
   "outputs": [],
   "source": [
    "ragas_df_ac.style.set_properties(**{'text-align': 'left', 'border': '1px solid black'})\n",
    "ragas_df_ac.to_string(justify='left', index=False)\n",
    "with pd.option_context(\"display.max_colwidth\", None):\n",
    "    display(pd.DataFrame(ragas_df_ac))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95cb27ab",
   "metadata": {},
   "source": [
    "### <a >Challenge Exercise :: Try it Yourself! </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c62acdc",
   "metadata": {},
   "source": [
    "\n",
    "<div style=\"border: 4px solid coral; text-align: left; margin: auto;\">\n",
    "    <br>\n",
    "    <p style=\"text-align: center; margin: auto;\"><b>Try the following exercises in this lab and note the observations.</b></p>\n",
    "<p style=\" text-align: left; margin: auto;\">\n",
    "<ol>\n",
    "    <li>Test the RAG based LLM with more questions about Amazon Bedrock. </li>\n",
    "<li>Look the the citations or retrieved references and see if the answer generated by the RAG chatbot aligns with these retrieved contexts. What response do you get when the retrieved context comes up empty? </li>\n",
    "<li>Apply system prompts to RAG as well as amazon Bedrock Guardrails and test which is more consistent in blocking responses when the model response is hallucinated </li>\n",
    "<li>Run the tutorial for RAG Checker and compare the difference with RAGAS evaluation framework: https://github.com/amazon-science/RAGChecker/blob/main/tutorial/ragchecker_tutorial_en.md </li>\n",
    "</ol>\n",
    "<br>\n",
    "</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dfc89ac-b0e8-438a-bba2-12bc3a4a3f94",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "We now have an understanding of parameters which influence hallucinations in Large Language Models. We learnt how to set up Retrieval Augmented Generation to provide a context to the model while answering.\n",
    "We used Contextual grounding in Amazon Bedrock Guardrials to intervene when hallucinations are detected.\n",
    "Finally we looked into the metrics of RAGAS and how to use them to measure hallucinations in your RAG powered chatbot.\n",
    "\n",
    "In the next lab, we will:\n",
    "1. Build a custom hallucination detector\n",
    "2. Use Amazon Bedrock Agents to intervene when hallucinations are detected\n",
    "3. Call a human for support when the LLM hallucinates\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
