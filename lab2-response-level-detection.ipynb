{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98851f24-a2f5-42c6-8435-219764f5af33",
   "metadata": {},
   "source": [
    "# <a id=\"top\">Lab 2: Response Level Detection</a>\n",
    "\n",
    "## Detecting LLM Hallucinations Through Response Consistency Analysis\n",
    "\n",
    "For many applications built on closed-source, proprietary models accessed via APIs, we don't have access to the model's internal states (like weights or logits) or ground truth data to compare against. **Response Level Detection** techniques are designed for this scenario, treating the model as an opaque system and relying only on its input-output behavior to assess reliability and detect potential hallucinations.\n",
    "\n",
    "In this notebook, we will implement and compare three cutting-edge response level detection methods that work without requiring reference answers or model internals. These methods analyze the **consistency and coherence of multiple model responses** to the same prompt.\n",
    "\n",
    "We'll implement and compare three cutting-edge detection methods:\n",
    "1. **Semantic Similarity Analysis** - Measuring consistency across multiple model responses using embeddings\n",
    "2. **Non-Contradiction Probability** - Using Natural Language Inference (NLI) to detect logical contradictions\n",
    "3. **Normalized Semantic Negentropy (NSN)** - Measuring semantic uncertainty through response clustering\n",
    "\n",
    "<div style=\"border: 4px solid coral; text-align: left; margin: auto; padding-left: 20px; padding-right: 20px\">\n",
    "    <h4>ðŸ’¡ Key Learning Objectives</h4>\n",
    "    <ul>\n",
    "        <li>Understand Response Level hallucination detection techniques</li>\n",
    "        <li>Learn how to implement semantic similarity analysis with sentence embeddings</li>\n",
    "        <li>Master contradiction detection using Natural Language Inference (NLI) models</li>\n",
    "        <li>Apply semantic entropy methods for uncertainty quantification</li>\n",
    "        <li>Build AI agents with Strands Agents SDK that automatically detect hallucinations</li>\n",
    "    </ul>\n",
    "</div>\n",
    "<br/>\n",
    "\n",
    "## Use-Case Overview\n",
    "\n",
    "Response Level hallucination detection is crucial when:\n",
    "- You don't have ground truth answers to compare against\n",
    "- You need to assess model reliability in production without access to model internals\n",
    "- You want to identify when a model is \"making things up\"\n",
    "- You need to implement confidence scoring for AI responses\n",
    "\n",
    "Our approach leverages the principle that **reliable knowledge should be consistent** across multiple generations with different sampling parameters, while hallucinations tend to vary significantly.\n",
    "\n",
    "\n",
    "## Sections\n",
    "\n",
    "This notebook has the following sections:\n",
    "\n",
    "1. [Environment Setup and Model Configuration](#1.-Environment-Setup-and-Model-Configuration)\n",
    "2. [Method 1: Semantic Similarity Detection](#2.-Method-1:-Semantic-Similarity-Detection)\n",
    "3. [Method 2: Non-Contradiction Probability](#3.-Method-2:-Non-Contradiction-Probability)\n",
    "4. [Method 3: Normalized Semantic Negentropy (NSN)](#4.-Method-3:-Normalized-Semantic-Negentropy-(NSN))\n",
    "5. [Comparative Analysis Across Models](#5.-Comparative-Analysis-Across-Models)\n",
    "6. [Building Production-Ready Agents with Semantic Similarity](#6.-Building-Production-Ready-Agents-with-Semantic-Similarity)\n",
    "7. [Conclusion and Best Practices](#7.-Conclusion-and-Best-Practices)\n",
    "8. [(Optional) Challenge Exercises](#8.-(Optional)-Challenge-Exercises)\n",
    "    \n",
    "Please work from top to bottom and don't skip sections as this could lead to error messages due to missing dependencies.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5463f03b-7f8b-4a95-a7c8-0caa46d6ae4b",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Model Configuration\n",
    "(<a href=\"#top\">Go to top</a>)\n",
    "\n",
    "**If you haven't already** installed the workshop's dependencies (from [pyproject.toml](./pyproject.toml)), you can un-comment (remove `# `) and run the below cell to do so. We've commented it out by default, assuming you already ran it at the start of lab 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8risldzg6od",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -q -e ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07e09b1",
   "metadata": {},
   "source": [
    "<div style=\"border: 4px solid coral; text-align: left; margin: auto; padding-left: 20px; padding-right: 20px; padding-top: 15px;\">\n",
    "    <h4>ðŸ”„ Restart the kernel after installing</h4>\n",
    "    <p>\n",
    "        <strong>IF</strong> you ran the above install command cell, you'll need to restart the\n",
    "        notebook kernel afterwards for the installations to take full effect.\n",
    "    </p>\n",
    "    <p>\n",
    "        Note that you may see some error notices about dependency conflicts in SageMaker Studio\n",
    "        environments, but this is okay as long as the installations are completed.\n",
    "    </p>\n",
    "</div>\n",
    "<br/>\n",
    "\n",
    "With the installation complete, you're ready to import the libraries we'll use in the notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31eff904-45e3-471c-881e-6534466429d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import numpy as np\n",
    "from transformers import pipeline\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import time\n",
    "from typing import List, Dict, Tuple\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(\"All packages imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b040e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve stored endpoint configuration\n",
    "%store -r endpoint_name\n",
    "%store -r inference_component_name\n",
    "\n",
    "try:\n",
    "    endpoint_name, inference_component_name\n",
    "except NameError as e:\n",
    "    raise RuntimeError(\n",
    "        \"SageMaker endpoint not found. Please check you've run lab 0 first!\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b84a1f4-9ecb-4e03-a252-7f6fe7832763",
   "metadata": {},
   "source": [
    "### Configure AWS Bedrock and SageMaker Models\n",
    "\n",
    "We'll test our hallucination detection methods across multiple models available on AWS Bedrock and SageMaker. This allows us to compare how different models perform in terms of consistency and reliability.\n",
    "\n",
    "For SageMaker endpoints, we'll automatically discover available endpoints with inference components that you've deployed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f22cc7b-929a-4389-b0bf-3c0a5d1d35b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Bedrock and SageMaker clients\n",
    "bedrock_client = boto3.client(\"bedrock-runtime\")\n",
    "sagemaker_client = boto3.client(\"sagemaker\")\n",
    "smr_client = boto3.client(\"sagemaker-runtime\")\n",
    "\n",
    "# Available models on Bedrock\n",
    "BEDROCK_MODELS = {\"claude-haiku-4-5\": \"global.anthropic.claude-haiku-4-5-20251001-v1:0\"}\n",
    "\n",
    "print(f\"âœ“ Configured {len(BEDROCK_MODELS)} Bedrock model(s) for testing:\")\n",
    "for name in BEDROCK_MODELS.keys():\n",
    "    print(f\"  â€¢ {name}\")\n",
    "if endpoint_name:\n",
    "    print(f\"- SageMaker Endpoint: {endpoint_name}\")\n",
    "    print(f\"- Inference Component: {inference_component_name}\")\n",
    "    sagemaker_endpoint = f\"{endpoint_name},{inference_component_name}\"\n",
    "else:\n",
    "    print(f\"\\n No SageMaker endpoints with inference components found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada2844c-04fd-4dfc-a87d-0a668afb8f82",
   "metadata": {},
   "source": [
    "### Model Response Generation Function\n",
    "\n",
    "This function handles the different API formats for various Bedrock models, allowing us to generate responses consistently across all model types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33decb7-d13a-49ba-bfe2-c0b4d874db4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_bedrock_response(\n",
    "    prompt: str, model_key: str, temperature: float = 0.7\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Generate a response from a Bedrock model with proper error handling.\n",
    "\n",
    "    Args:\n",
    "        prompt: The input prompt to send to the model\n",
    "        model_key: Key from BEDROCK_MODELS dictionary\n",
    "        temperature: Sampling temperature\n",
    "\n",
    "    Returns:\n",
    "        Generated text response or empty string on error\n",
    "    \"\"\"\n",
    "    model_id = BEDROCK_MODELS[model_key]\n",
    "\n",
    "    try:\n",
    "        # Prepare the conversation message format for Converse API\n",
    "        messages = [{\"role\": \"user\", \"content\": [{\"text\": prompt}]}]\n",
    "\n",
    "        # Set up inference configuration parameters\n",
    "        inference_config = {\"maxTokens\": 200, \"temperature\": temperature}\n",
    "\n",
    "        # Use the Converse API for all model types\n",
    "        response = bedrock_client.converse(\n",
    "            modelId=model_id, messages=messages, inferenceConfig=inference_config\n",
    "        )\n",
    "\n",
    "        # Extract the response text from the Converse API response\n",
    "        return response[\"output\"][\"message\"][\"content\"][0][\"text\"]\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error with {model_key}: {str(e)}\")\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "# Test the function with a simple prompt\n",
    "test_response = generate_bedrock_response(\"What is 2+2?\", \"claude-haiku-4-5\", 0.1)\n",
    "if test_response:\n",
    "    print(f\"Model connection successful! Test response: {test_response[:50]}...\")\n",
    "else:\n",
    "    print(\"Model connection failed. Please check your model IDs and region.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839b7749-56cf-4a29-9cb4-ef552db89e1c",
   "metadata": {},
   "source": [
    "This function handles the invocation to the models deployed to the SageMaker AI real-time endpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b0bf69-367c-437e-beb1-2431c7b67c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sagemaker_response(\n",
    "    prompt: str, endpoint_name: str, inference_component: str, temperature: float = 0.7\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Generate a response from a SageMaker endpoint with proper error handling.\n",
    "\n",
    "    Args:\n",
    "        prompt: The input prompt to send to the model\n",
    "        endpoint_name: Name of the SageMaker endpoint\n",
    "        temperature: Sampling temperature\n",
    "\n",
    "    Returns:\n",
    "        Generated text response or empty string on error\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Prepare the payload for SageMaker endpoint\n",
    "        payload = {\n",
    "            \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "            \"temperature\": temperature,\n",
    "            \"top_p\": 0.9,\n",
    "        }\n",
    "\n",
    "        # Invoke the SageMaker endpoint\n",
    "        response = smr_client.invoke_endpoint(\n",
    "            EndpointName=endpoint_name,\n",
    "            InferenceComponentName=inference_component,\n",
    "            ContentType=\"application/json\",\n",
    "            Body=json.dumps(payload),\n",
    "        )\n",
    "\n",
    "        # Parse the response\n",
    "        result = json.loads(response[\"Body\"].read().decode())\n",
    "\n",
    "        # Extract the response text (adjust based on your model's response format)\n",
    "        if \"choices\" in result and len(result[\"choices\"]) > 0:\n",
    "            return result[\"choices\"][0][\"message\"][\"content\"]\n",
    "        elif \"generated_text\" in result:\n",
    "            return result[\"generated_text\"]\n",
    "        else:\n",
    "            print(f\"Unexpected response format: {result}\")\n",
    "            return \"\"\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error with SageMaker endpoint {endpoint_name}: {str(e)}\")\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "# Test the function with a simple prompt\n",
    "test_response = generate_sagemaker_response(\n",
    "    \"What is 2+2?\", endpoint_name, inference_component_name, 0.1\n",
    ")\n",
    "if test_response:\n",
    "    print(f\"Model connection successful! Test response: {test_response[:50]}...\")\n",
    "else:\n",
    "    print(\"Model connection failed. Please check your model IDs and region.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5217c15b-c2c9-4426-8d65-e9dbe11607f7",
   "metadata": {},
   "source": [
    "Create a unified response generation function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0aae91-ee3d-4ea2-a6a1-c23ae9e2da0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(\n",
    "    prompt: str,\n",
    "    model_identifier: str,\n",
    "    temperature: float = 0.7,\n",
    "    invocation_method: str = \"bedrock\",\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Unified function to generate responses from either Bedrock or SageMaker.\n",
    "\n",
    "    Args:\n",
    "        prompt: The input prompt to send to the model\n",
    "        model_identifier: Either model_key (for Bedrock) or endpoint_name (for SageMaker)\n",
    "        temperature: Sampling temperature\n",
    "        invocation_method: \"bedrock\" or \"sagemaker\"\n",
    "\n",
    "    Returns:\n",
    "        Generated text response or empty string on error\n",
    "    \"\"\"\n",
    "    if invocation_method.lower() == \"bedrock\":\n",
    "        return generate_bedrock_response(prompt, model_identifier, temperature)\n",
    "    elif invocation_method.lower() == \"sagemaker\":\n",
    "        model_identifier, inference_component = model_identifier.split(\",\")\n",
    "        return generate_sagemaker_response(\n",
    "            prompt, model_identifier, inference_component, temperature\n",
    "        )\n",
    "    else:\n",
    "        print(\n",
    "            f\"Invalid invocation method: {invocation_method}. Use 'bedrock' or 'sagemaker'\"\n",
    "        )\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53daa38a-d3a6-4cc3-823c-c3d2fdd9be92",
   "metadata": {},
   "source": [
    "## 2. Method 1: Semantic Similarity Detection\n",
    "(<a href=\"#top\">Go to top</a>)\n",
    "\n",
    "### How It Works\n",
    "\n",
    "Semantic Similarity Detection works on the principle that **reliable information should be expressed consistently**, even when generated multiple times with different parameters.\n",
    "\n",
    "**The Process:**\n",
    "1. **Generate Multiple Responses**: Ask the same question several times with varying temperatures\n",
    "2. **Create Embeddings**: Convert each response to a high-dimensional vector representation\n",
    "3. **Calculate Similarities**: Measure cosine similarity between all response pairs\n",
    "4. **Assess Consistency**: High similarity = reliable, low similarity = potential hallucination\n",
    "\n",
    "**Key Principle**: Reliable information should be expressed consistently, regardless of sampling parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f13f0e-55a2-437f-9a16-7fa67c933825",
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_similarity_detection(\n",
    "    prompt: str,\n",
    "    model_identifier: str = None,\n",
    "    num_samples: int = 5,\n",
    "    invocation_method: str = \"bedrock\",\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Detect hallucinations using semantic similarity across multiple responses.\n",
    "\n",
    "    Args:\n",
    "        prompt: Question to ask the model\n",
    "        model_identifier: Model key (for Bedrock) or endpoint name (for SageMaker)\n",
    "        num_samples: Number of responses to generate and compare\n",
    "        invocation_method: \"bedrock\" or \"sagemaker\" - which service to use\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with similarity metrics and confidence assessment\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"ðŸ” SEMANTIC SIMILARITY DETECTION\")\n",
    "    print(f\"Invocation Method: {invocation_method.upper()}\")\n",
    "    print(f\"Model/Endpoint: {model_identifier}\")\n",
    "    print(f\"Question: {prompt}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Step 1: Generate multiple responses with different temperatures\n",
    "    print(f\"Generating {num_samples} responses using {invocation_method}...\")\n",
    "    responses = []\n",
    "    for i in range(num_samples):\n",
    "        # Use low temperature for first response, higher for others\n",
    "        temp = 0.1 if i == 0 else 0.7\n",
    "        response = generate_response(\n",
    "            prompt,\n",
    "            model_identifier,\n",
    "            temperature=temp,\n",
    "            invocation_method=invocation_method,\n",
    "        )\n",
    "        if response:\n",
    "            responses.append(response)\n",
    "            print(\n",
    "                f\"  Response {i + 1}: {response[:120]}{'...' if len(response) > 120 else ''}\"\n",
    "            )\n",
    "\n",
    "    if len(responses) < 2:\n",
    "        return {\"error\": \"Need at least 2 responses for comparison\"}\n",
    "\n",
    "    # Step 2: Create semantic embeddings\n",
    "    print(f\"\\nCreating semantic embeddings...\")\n",
    "    model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "    embeddings = model.encode(responses)\n",
    "\n",
    "    # Step 3: Calculate pairwise similarities\n",
    "    print(f\"Calculating semantic similarities...\")\n",
    "    similarity_matrix = cosine_similarity(embeddings)\n",
    "\n",
    "    # Get all pairwise similarities (excluding diagonal)\n",
    "    mask = np.ones(similarity_matrix.shape, dtype=bool)\n",
    "    np.fill_diagonal(mask, False)\n",
    "    similarities = similarity_matrix[mask]\n",
    "\n",
    "    # Step 4: Calculate metrics\n",
    "    mean_similarity = np.mean(similarities)\n",
    "    std_similarity = np.std(similarities)\n",
    "    min_similarity = np.min(similarities)\n",
    "    max_similarity = np.max(similarities)\n",
    "\n",
    "    # Step 5: Determine confidence level\n",
    "    if mean_similarity > 0.95 and std_similarity < 0.03:\n",
    "        confidence = \"ðŸŸ¢ HIGH\"\n",
    "        interpretation = \"Responses are highly consistent - reliable knowledge\"\n",
    "    elif mean_similarity > 0.85 and std_similarity < 0.08:\n",
    "        confidence = \"ðŸŸ¡ MEDIUM\"\n",
    "        interpretation = \"Moderate consistency - some uncertainty present\"\n",
    "    else:\n",
    "        confidence = \"ðŸ”´ LOW\"\n",
    "        interpretation = \"Inconsistent responses - possible hallucination\"\n",
    "\n",
    "    # Display results\n",
    "    print(f\"\\nRESULTS:\")\n",
    "    print(f\"Mean Similarity:    {mean_similarity:.3f}\")\n",
    "    print(f\"Std Deviation:      {std_similarity:.3f}\")\n",
    "    print(f\"Min Similarity:     {min_similarity:.3f}\")\n",
    "    print(f\"Max Similarity:     {max_similarity:.3f}\")\n",
    "    print(f\"\\nCONFIDENCE: {confidence}\")\n",
    "    print(f\"Interpretation: {interpretation}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    return {\n",
    "        \"method\": \"Semantic Similarity\",\n",
    "        \"invocation_method\": invocation_method,\n",
    "        \"model_identifier\": model_identifier,\n",
    "        \"num_responses\": len(responses),\n",
    "        \"mean_similarity\": round(mean_similarity, 3),\n",
    "        \"std_similarity\": round(std_similarity, 3),\n",
    "        \"min_similarity\": round(min_similarity, 3),\n",
    "        \"confidence\": confidence,\n",
    "        \"interpretation\": interpretation,\n",
    "        \"responses\": responses,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3a0249-e6b4-46d9-aade-c7b006bb22c3",
   "metadata": {},
   "source": [
    "### Test Semantic Similarity Detection\n",
    "\n",
    "Let's test this method with a question that should have a clear, factual answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bf8e78-b31c-4567-8eb0-1f5d88f8b926",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = semantic_similarity_detection(\n",
    "    \"What album was Dua Lipa's 'Hallucinate' featured on?\",\n",
    "    \"claude-haiku-4-5\",\n",
    "    num_samples=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53317ad8",
   "metadata": {},
   "source": [
    "### Optional: Test with SageMaker Endpoint\n",
    "\n",
    "If you have SageMaker endpoints with inference components available, you can test them here. Otherwise, skip to the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b6ac5b-d203-49f6-8c17-7dcecd042156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with SageMaker endpoint (if available)\n",
    "if sagemaker_endpoint:\n",
    "    print(f\"Testing with SageMaker endpoint: {sagemaker_endpoint}\")\n",
    "    result = semantic_similarity_detection(\n",
    "        \"What album was Dua Lipa's 'Hallucinate' featured on?\",\n",
    "        sagemaker_endpoint,\n",
    "        num_samples=5,\n",
    "        invocation_method=\"sagemaker\",\n",
    "    )\n",
    "else:\n",
    "    print(\"â„¹ï¸  No SageMaker endpoints available - skipping this test.\")\n",
    "    print(\"   You can continue with Bedrock models only.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96296d9c-6e24-4be6-a5fd-920068e71892",
   "metadata": {},
   "source": [
    "## 3. Method 2: Non-Contradiction Probability\n",
    "(<a href=\"#top\">Go to top</a>)\n",
    "\n",
    "### How It Works\n",
    "\n",
    "Non-Contradiction Probability uses **Natural Language Inference (NLI)** to detect logical inconsistencies between multiple model responses.\n",
    "\n",
    "**The Process:**\n",
    "1. **Generate Multiple Responses**: Get several answers to the same question\n",
    "2. **Pairwise Analysis**: Compare every response pair using an NLI model\n",
    "3. **Contradiction Detection**: Identify logical contradictions between responses\n",
    "4. **Probability Calculation**: Calculate the probability that responses don't contradict each other\n",
    "\n",
    "**Key Principle**: Reliable knowledge should not contradict itself, while hallucinations often contain conflicting information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3800e0c4-0570-4d49-b62e-3579ae69f4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_contradiction_detection(\n",
    "    prompt: str,\n",
    "    model_identifier: str = None,\n",
    "    num_samples: int = 5,\n",
    "    invocation_method: str = \"bedrock\",\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Detect hallucinations by analyzing logical contradictions between responses.\n",
    "\n",
    "    Args:\n",
    "        prompt: Question to ask the model\n",
    "        model_identifier: Which model to test\n",
    "        num_samples: Number of responses to generate and analyze\n",
    "        invocation_method: \"bedrock\" or \"sagemaker\" - which service to use\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with contradiction analysis and confidence assessment\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"NON-CONTRADICTION PROBABILITY DETECTION\")\n",
    "    print(f\"Invocation Method: {invocation_method.upper()}\")\n",
    "    print(f\"Model/Endpoint: {model_identifier}\")\n",
    "    print(f\"Question: {prompt}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Step 1: Generate multiple responses\n",
    "    print(f\"Generating {num_samples} responses...\")\n",
    "    responses = []\n",
    "    for i in range(num_samples):\n",
    "        temp = 0.1 if i == 0 else 0.7\n",
    "        response = generate_response(\n",
    "            prompt,\n",
    "            model_identifier,\n",
    "            temperature=temp,\n",
    "            invocation_method=invocation_method,\n",
    "        )\n",
    "        if response:\n",
    "            responses.append(response)\n",
    "            print(\n",
    "                f\"  Response {i + 1}: {response[:120]}{'...' if len(response) > 120 else ''}\"\n",
    "            )\n",
    "\n",
    "    if len(responses) < 2:\n",
    "        return {\"error\": \"Need at least 2 responses for analysis\"}\n",
    "\n",
    "    # Step 2: Load NLI model for contradiction detection\n",
    "    print(f\"\\nLoading Natural Language Inference model...\")\n",
    "    nli_model = pipeline(\n",
    "        \"text-classification\",\n",
    "        model=\"microsoft/deberta-base-mnli\",\n",
    "        return_all_scores=True,\n",
    "    )\n",
    "\n",
    "    # Step 3: Analyze all response pairs for contradictions\n",
    "    print(f\"\\nAnalyzing {len(responses)} responses for logical contradictions...\")\n",
    "    non_contradiction_scores = []\n",
    "    pair_analyses = []\n",
    "\n",
    "    for i in range(len(responses)):\n",
    "        for j in range(i + 1, len(responses)):\n",
    "            try:\n",
    "                # Check both directions (Aâ†’B and Bâ†’A) for mutual contradiction\n",
    "                result1 = nli_model(f\"{responses[i]} [SEP] {responses[j]}\")\n",
    "                result2 = nli_model(f\"{responses[j]} [SEP] {responses[i]}\")\n",
    "\n",
    "                # Extract contradiction probabilities\n",
    "                contra_prob1 = 0.0\n",
    "                contra_prob2 = 0.0\n",
    "\n",
    "                # Parse NLI results\n",
    "                if isinstance(result1, list) and len(result1) > 0:\n",
    "                    inner_list1 = (\n",
    "                        result1[0] if isinstance(result1[0], list) else result1\n",
    "                    )\n",
    "                    for item in inner_list1:\n",
    "                        if (\n",
    "                            isinstance(item, dict)\n",
    "                            and item.get(\"label\") == \"CONTRADICTION\"\n",
    "                        ):\n",
    "                            contra_prob1 = item.get(\"score\", 0.0)\n",
    "                            break\n",
    "\n",
    "                if isinstance(result2, list) and len(result2) > 0:\n",
    "                    inner_list2 = (\n",
    "                        result2[0] if isinstance(result2[0], list) else result2\n",
    "                    )\n",
    "                    for item in inner_list2:\n",
    "                        if (\n",
    "                            isinstance(item, dict)\n",
    "                            and item.get(\"label\") == \"CONTRADICTION\"\n",
    "                        ):\n",
    "                            contra_prob2 = item.get(\"score\", 0.0)\n",
    "                            break\n",
    "\n",
    "                # Calculate average contradiction probability\n",
    "                avg_contradiction = (contra_prob1 + contra_prob2) / 2\n",
    "                non_contradiction_score = 1 - avg_contradiction\n",
    "\n",
    "                non_contradiction_scores.append(non_contradiction_score)\n",
    "                pair_analyses.append(\n",
    "                    {\n",
    "                        \"pair\": f\"{i + 1}-{j + 1}\",\n",
    "                        \"contradiction\": avg_contradiction,\n",
    "                        \"non_contradiction\": non_contradiction_score,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "                print(\n",
    "                    f\"  Pair {i + 1}-{j + 1}: Contradiction={avg_contradiction:.3f}, Non-contradiction={non_contradiction_score:.3f}\"\n",
    "                )\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"  Error analyzing pair {i + 1}-{j + 1}: {str(e)}\")\n",
    "                non_contradiction_scores.append(0.5)  # Neutral score for errors\n",
    "\n",
    "    if not non_contradiction_scores:\n",
    "        return {\"error\": \"Could not analyze any response pairs\"}\n",
    "\n",
    "    # Step 4: Calculate overall metrics\n",
    "    mean_ncp = np.mean(non_contradiction_scores)\n",
    "    std_ncp = np.std(non_contradiction_scores)\n",
    "    min_ncp = np.min(non_contradiction_scores)\n",
    "\n",
    "    # Step 5: Determine confidence level\n",
    "    if mean_ncp > 0.8 and min_ncp > 0.7:\n",
    "        confidence = \"ðŸŸ¢ HIGH\"\n",
    "        interpretation = \"No significant contradictions - facts are consistent\"\n",
    "    elif mean_ncp > 0.65 and min_ncp > 0.5:\n",
    "        confidence = \"ðŸŸ¡ MEDIUM\"\n",
    "        interpretation = \"Minor contradictions detected - mostly consistent\"\n",
    "    else:\n",
    "        confidence = \"ðŸ”´ LOW\"\n",
    "        interpretation = \"Significant contradictions detected - likely hallucination\"\n",
    "\n",
    "    print(f\"\\nRESULTS:\")\n",
    "    print(f\"Mean Non-Contradiction: {mean_ncp:.3f}\")\n",
    "    print(f\"Std Deviation:          {std_ncp:.3f}\")\n",
    "    print(f\"Min Non-Contradiction:  {min_ncp:.3f}\")\n",
    "    print(f\"\\nCONFIDENCE: {confidence}\")\n",
    "    print(f\"Interpretation: {interpretation}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    return {\n",
    "        \"method\": \"Non-Contradiction Probability\",\n",
    "        \"model\": model_identifier,\n",
    "        \"confidence\": confidence,\n",
    "        \"mean_ncp\": mean_ncp,\n",
    "        \"std_ncp\": std_ncp,\n",
    "        \"min_ncp\": min_ncp,\n",
    "        \"interpretation\": interpretation,\n",
    "        \"pair_analyses\": pair_analyses,\n",
    "        \"responses\": responses,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa89ac0d-b1af-4efa-bbb8-59a8f486a622",
   "metadata": {},
   "source": [
    "### Test Non-Contradiction Detection\n",
    "\n",
    "Let's test this method with the same question to see how it compares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd8a571-0c28-4c3a-8d84-8f4a927d5e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_ncp = non_contradiction_detection(\n",
    "    \"What album was Dua Lipa's 'Hallucinate' featured on?\",\n",
    "    \"claude-haiku-4-5\",\n",
    "    num_samples=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11f6be0-09ac-4946-ad24-1c75280f6046",
   "metadata": {},
   "source": [
    "## 4. Method 3: Normalized Semantic Negentropy (NSN)\n",
    "(<a href=\"#top\">Go to top</a>)\n",
    "\n",
    "### How It Works\n",
    "\n",
    "Normalized Semantic Negentropy is the most sophisticated method, measuring **semantic uncertainty** through response clustering.\n",
    "\n",
    "**The Process:**\n",
    "1. **Generate Multiple Responses**: Create diverse responses with different sampling parameters\n",
    "2. **Semantic Clustering**: Group responses that convey the same meaning\n",
    "3. **Entropy Calculation**: Measure how \"scattered\" the responses are across clusters\n",
    "4. **Confidence Scoring**: Convert entropy to a confidence score (low entropy = high confidence)\n",
    "\n",
    "**Key Principle**: Confident knowledge clusters tightly, while uncertain knowledge fragments across many different meanings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab187128-0a44-4210-bbdc-d05971c7801c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_entropy_detection(\n",
    "    prompt: str,\n",
    "    model_identifier: str = None,\n",
    "    num_samples: int = 5,\n",
    "    invocation_method: str = \"bedrock\",\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Detect hallucinations using Normalized Semantic Negentropy (NSN).\n",
    "\n",
    "    This method clusters responses by meaning and calculates semantic entropy\n",
    "    to measure model uncertainty. Lower entropy indicates higher confidence.\n",
    "\n",
    "    Args:\n",
    "        prompt: Question to ask the model\n",
    "        model_identifier: Which model to test\n",
    "        num_samples: Number of responses to generate and cluster\n",
    "        invocation_method: \"bedrock\" or \"sagemaker\" - which service to use\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with entropy metrics and confidence assessment\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\" NORMALIZED SEMANTIC NEGENTROPY DETECTION\")\n",
    "    print(f\"Invocation Method: {invocation_method.upper()}\")\n",
    "    print(f\"Model/Endpoint: {model_identifier}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Step 1: Generate diverse responses\n",
    "    print(f\" Generating {num_samples} diverse responses...\")\n",
    "    responses = []\n",
    "    for i in range(num_samples):\n",
    "        temp = 0.1 if i == 0 else min(1.0, 0.5 + (i * 0.1))  # Cap at 1.0\n",
    "        response = generate_response(\n",
    "            prompt,\n",
    "            model_identifier,\n",
    "            temperature=temp,\n",
    "            invocation_method=invocation_method,\n",
    "        )\n",
    "        if response:\n",
    "            responses.append(response)\n",
    "            print(\n",
    "                f\"  Response {i + 1}: {response[:120]}{'...' if len(response) > 120 else ''}\"\n",
    "            )\n",
    "\n",
    "    if len(responses) < 3:\n",
    "        return {\"error\": \"Need at least 3 responses for clustering analysis\"}\n",
    "\n",
    "    # Step 2: Load NLI model for semantic clustering\n",
    "    print(f\"\\nLoading NLI model for semantic clustering...\")\n",
    "    nli_model = pipeline(\n",
    "        \"text-classification\",\n",
    "        model=\"microsoft/deberta-base-mnli\",\n",
    "        return_all_scores=True,\n",
    "    )\n",
    "\n",
    "    # Step 3: Cluster responses by semantic meaning\n",
    "    print(f\"\\n Clustering responses by semantic meaning...\")\n",
    "    clusters = []\n",
    "\n",
    "    for i, response in enumerate(responses):\n",
    "        assigned_to_cluster = False\n",
    "\n",
    "        # Try to assign to existing cluster\n",
    "        for cluster in clusters:\n",
    "            # Compare with representative response from cluster\n",
    "            representative = responses[cluster[0]]\n",
    "\n",
    "            # Check for bidirectional entailment (same meaning)\n",
    "            result1 = nli_model(f\"{response} [SEP] {representative}\")\n",
    "            result2 = nli_model(f\"{representative} [SEP] {response}\")\n",
    "\n",
    "            # Extract entailment scores\n",
    "            entail1 = 0.0\n",
    "            entail2 = 0.0\n",
    "\n",
    "            if isinstance(result1, list) and len(result1) > 0:\n",
    "                inner_list1 = result1[0] if isinstance(result1[0], list) else result1\n",
    "                for item in inner_list1:\n",
    "                    if isinstance(item, dict) and item.get(\"label\") == \"ENTAILMENT\":\n",
    "                        entail1 = item.get(\"score\", 0.0)\n",
    "                        break\n",
    "\n",
    "            if isinstance(result2, list) and len(result2) > 0:\n",
    "                inner_list2 = result2[0] if isinstance(result2[0], list) else result2\n",
    "                for item in inner_list2:\n",
    "                    if isinstance(item, dict) and item.get(\"label\") == \"ENTAILMENT\":\n",
    "                        entail2 = item.get(\"score\", 0.0)\n",
    "                        break\n",
    "\n",
    "            # Use geometric mean for mutual entailment\n",
    "            mutual_entailment = np.sqrt(entail1 * entail2)\n",
    "\n",
    "            if mutual_entailment > 0.5:  # Threshold for \"same meaning\"\n",
    "                cluster.append(i)\n",
    "                assigned_to_cluster = True\n",
    "                break\n",
    "\n",
    "        # Create new cluster if no match found\n",
    "        if not assigned_to_cluster:\n",
    "            clusters.append([i])\n",
    "\n",
    "    # Step 4: Display cluster information\n",
    "    print(f\"Found {len(clusters)} semantic clusters:\")\n",
    "    for i, cluster in enumerate(clusters):\n",
    "        print(\n",
    "            f\"  Cluster {i + 1}: Responses {[j + 1 for j in cluster]} ({len(cluster)} responses)\"\n",
    "        )\n",
    "\n",
    "    # Step 5: Calculate semantic entropy\n",
    "    cluster_probabilities = [len(cluster) / len(responses) for cluster in clusters]\n",
    "    semantic_entropy = -sum(p * np.log(p) for p in cluster_probabilities if p > 0)\n",
    "\n",
    "    # Normalize entropy (0 = all same meaning, 1 = all different meanings)\n",
    "    max_entropy = np.log(len(responses))\n",
    "    normalized_entropy = semantic_entropy / max_entropy if max_entropy > 0 else 0\n",
    "    nsn_score = 1 - normalized_entropy  # Convert to confidence score\n",
    "\n",
    "    # Step 6: Calculate additional metrics\n",
    "    fragmentation_ratio = len(clusters) / len(responses)\n",
    "    largest_cluster_size = max(len(cluster) for cluster in clusters)\n",
    "    cluster_dominance = largest_cluster_size / len(responses)\n",
    "\n",
    "    # Step 7: Determine confidence level\n",
    "    if nsn_score > 0.8 and fragmentation_ratio < 0.4:\n",
    "        confidence = \"ðŸŸ¢ HIGH\"\n",
    "        interpretation = \"Responses cluster tightly - coherent understanding\"\n",
    "    elif nsn_score > 0.6 and fragmentation_ratio < 0.6:\n",
    "        confidence = \"ðŸŸ¡ MEDIUM\"\n",
    "        interpretation = \"Moderate clustering - some uncertainty present\"\n",
    "    else:\n",
    "        confidence = \"ðŸ”´ LOW\"\n",
    "        interpretation = \"Highly fragmented responses - model is uncertain\"\n",
    "\n",
    "    print(f\"\\n RESULTS:\")\n",
    "    print(f\"Semantic Entropy:       {semantic_entropy:.3f}\")\n",
    "    print(f\"NSN Score:              {nsn_score:.3f}\")\n",
    "    print(f\"Fragmentation Ratio:    {fragmentation_ratio:.3f}\")\n",
    "    print(f\"Cluster Dominance:      {cluster_dominance:.3f}\")\n",
    "    print(f\"\\n CONFIDENCE: {confidence}\")\n",
    "    print(f\"\\n Interpretation: {interpretation}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    return {\n",
    "        \"method\": \"Normalized Semantic Negentropy\",\n",
    "        \"model\": model_identifier,\n",
    "        \"confidence\": confidence,\n",
    "        \"nsn_score\": nsn_score,\n",
    "        \"num_clusters\": len(clusters),\n",
    "        \"semantic_entropy\": semantic_entropy,\n",
    "        \"fragmentation_ratio\": fragmentation_ratio,\n",
    "        \"cluster_dominance\": cluster_dominance,\n",
    "        \"interpretation\": interpretation,\n",
    "        \"clusters\": clusters,\n",
    "        \"responses\": responses,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b400720-accd-41f7-84f3-c895b2eb3123",
   "metadata": {},
   "source": [
    "### Test Semantic Entropy Detection\n",
    "\n",
    "Let's test this advanced method with our Mars question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c85db4-2791-4c5e-9d1f-8b4a2f5c7b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_nsn = semantic_entropy_detection(\n",
    "    \"What album was Dua Lipa's 'Hallucinate' featured on?\",\n",
    "    \"claude-haiku-4-5\",\n",
    "    num_samples=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbebbe7-f35e-4b4a-ae16-28c29de5ae25",
   "metadata": {},
   "source": [
    "## 5. Comparative Analysis Across Models\n",
    "(<a href=\"#top\">Go to top</a>)\n",
    "\n",
    "Now let's compare how different models perform across all three detection methods. This will help us understand which models are more reliable and which detection methods are most effective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8d1a8f-7931-4ad4-af2d-6ab8b3e54e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def comprehensive_model_analysis(prompt: str, models_to_test: Dict = None) -> Dict:\n",
    "    \"\"\"\n",
    "    Run all three detection methods across multiple models for comparison.\n",
    "\n",
    "    Args:\n",
    "        prompt: Question to test\n",
    "        models_to_test: List of model keys to test (None = test all available)\n",
    "\n",
    "    Returns:\n",
    "        Comprehensive results dictionary\n",
    "    \"\"\"\n",
    "\n",
    "    # if models_to_test is None:\n",
    "    #     models_to_test = ['claude-haiku-4-5']\n",
    "\n",
    "    print(f\"COMPREHENSIVE MODEL ANALYSIS\")\n",
    "    print(f\"Question: {prompt}\")\n",
    "    print(f\"Models: {', '.join(models_to_test)}\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    results = {}\n",
    "    methods = [\n",
    "        (\"Semantic Similarity\", semantic_similarity_detection),\n",
    "        (\"Non-Contradiction\", non_contradiction_detection),\n",
    "        (\"Semantic Entropy\", semantic_entropy_detection),\n",
    "    ]\n",
    "\n",
    "    for model_key, model_values in models_to_test.items():\n",
    "        invocation_method = model_key\n",
    "        for model_name in model_values:\n",
    "            model_results = {}\n",
    "\n",
    "            for method_name, method_func in methods:\n",
    "                try:\n",
    "                    print(f\"\\n--- {method_name} ---\")\n",
    "                    result = method_func(\n",
    "                        prompt,\n",
    "                        model_name,\n",
    "                        num_samples=4,\n",
    "                        invocation_method=invocation_method,\n",
    "                    )\n",
    "                    model_results[method_name] = result\n",
    "                    time.sleep(2)  # Rate limiting\n",
    "                except Exception as e:\n",
    "                    print(f\"Error with {method_name}: {str(e)}\")\n",
    "                    model_results[method_name] = {\"error\": str(e)}\n",
    "\n",
    "            results[model_key] = model_results\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def display_comparison_summary(results: Dict):\n",
    "    \"\"\"\n",
    "    Display a neat summary table of all results.\n",
    "    \"\"\"\n",
    "    print(f\"\\n SUMMARY COMPARISON\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"{'Model':<15} {'Semantic Sim':<15} {'Non-Contra':<15} {'Sem Entropy':<15}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    for model, model_results in results.items():\n",
    "        row = f\"{model:<15}\"\n",
    "\n",
    "        methods = [\"Semantic Similarity\", \"Non-Contradiction\", \"Semantic Entropy\"]\n",
    "        for method in methods:\n",
    "            if method in model_results and \"confidence\" in model_results[method]:\n",
    "                confidence = model_results[method][\"confidence\"]\n",
    "                # Extract just the confidence level (HIGH/MEDIUM/LOW)\n",
    "                conf_level = confidence.split()[-1] if confidence else \"ERROR\"\n",
    "                row += f\"{conf_level:<15}\"\n",
    "            else:\n",
    "                row += f\"{'ERROR':<15}\"\n",
    "\n",
    "        print(row)\n",
    "\n",
    "    print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54203674",
   "metadata": {},
   "source": [
    "### Run Comprehensive Analysis\n",
    "\n",
    "Let's test multiple models with different types of questions to see how they perform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f9e55a-0e8c-4e1a-8f7d-9b3c4a5a7c42",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Test 1: Factual question with clear answer\n",
    "print(\"TEST 1: Factual Question\")\n",
    "\n",
    "# Build models to test dictionary\n",
    "models_to_test = {\"bedrock\": list(BEDROCK_MODELS.keys())}\n",
    "\n",
    "# Add SageMaker endpoint if available\n",
    "if sagemaker_endpoint:\n",
    "    models_to_test[\"sagemaker\"] = [sagemaker_endpoint]\n",
    "    print(f\"Testing Bedrock + SageMaker endpoint\")\n",
    "else:\n",
    "    print(f\"Testing {len(BEDROCK_MODELS)} Bedrock model(s) only\")\n",
    "\n",
    "results1 = comprehensive_model_analysis(\n",
    "    \"What album was Dua Lipa's 'Hallucinate' featured on?\",\n",
    "    models_to_test=models_to_test,\n",
    ")\n",
    "display_comparison_summary(results1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88211718-0f51-455a-8190-cf4301716579",
   "metadata": {},
   "source": [
    "## 6. Building Production-Ready Agents with Semantic Similarity\n",
    "(<a href=\"#top\">Go to top</a>)\n",
    "\n",
    "In this section, we'll integrate response level hallucination detection into a production-ready agentic system using **AWS Strands Agents**. This demonstrates how to build agents that automatically detect and mitigate hallucinations in real-time.\n",
    "\n",
    "### What are Strands Agents?\n",
    "\n",
    "[Strands](https://github.com/strands-agents/sdk-python) is an open-source Python framework for building agentic AI applications. It provides:\n",
    "- **Unified Model Interface**: Work with multiple LLM providers (Bedrock, SageMaker, etc.)\n",
    "- **Tool Integration**: Connect agents to external tools and APIs\n",
    "- **Observability**: Built-in tracing and monitoring via OpenTelemetry\n",
    "- **Extensibility**: Easy to add custom behaviors like hallucination detection\n",
    "\n",
    "### Our Approach: Semantic Similarity as a Guardrail\n",
    "\n",
    "We'll build an agent that:\n",
    "1. **Generates multiple candidate responses** in parallel for each user query\n",
    "2. **Analyzes semantic similarity** between the candidates\n",
    "3. **Intervenes if inconsistency is detected** (potential hallucination)\n",
    "4. **Returns the most reliable response** or raises an alert\n",
    "\n",
    "#### Why This Matters:\n",
    "\n",
    "Unlike simple API wrappers, this pattern provides:\n",
    "- **Automated Quality Control**: No need for human review of every response\n",
    "- **Transparent Telemetry**: Full visibility into guardrail decisions via OpenTelemetry\n",
    "- **Flexible Deployment**: Works with any Strands-compatible model (Bedrock, SageMaker, OpenAI, etc.)\n",
    "- **Production-Ready**: Built-in error handling, logging, and monitoring\n",
    "\n",
    "\n",
    "This pattern is ideal for production systems where you need automated quality control without human review of every response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311bbff2",
   "metadata": {},
   "source": [
    "### Step 1: Prepare environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fqeayw6u8qh",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python Built-Ins:\n",
    "import base64\n",
    "import os\n",
    "\n",
    "# External Dependencies:\n",
    "from dotenv import load_dotenv\n",
    "from strands import Agent\n",
    "from strands.models.bedrock import BedrockModel\n",
    "from strands.telemetry import StrandsTelemetry\n",
    "\n",
    "# For our custom parallel model implementation:\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "from hallucination_utils.models.parallel import (\n",
    "    ParallelModelWithConsolidation,\n",
    "    ModelResponse,\n",
    "    TraceAttributes,\n",
    ")\n",
    "\n",
    "print(\"âœ“ Imports successful!\")\n",
    "print(f\"âœ“ Strands Agent module: {Agent.__module__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebobguidyic",
   "metadata": {},
   "source": [
    "### Optional: Configure Langfuse for Observability\n",
    "\n",
    "[Langfuse](https://langfuse.com/) provides production-grade observability for LLM applications. It integrates seamlessly with Strands via OpenTelemetry to track:\n",
    "- Individual LLM calls and their responses\n",
    "- Guardrail interventions and scores\n",
    "- Token usage and latency metrics\n",
    "- End-to-end conversation traces\n",
    "\n",
    "The Langfuse configuration will be handled automatically by the `set_up_notebook_langfuse()` function below.\n",
    "\n",
    "**Note**: You should have already obtained your Langfuse credentials following the **Enable Langfuse tracing** instruction in the Getting Started section. If you haven't set up Langfuse yet, you'll be prompted for your credentials when you run the cell below.\n",
    "\n",
    "If you prefer not to use Langfuse, you can skip this setup - the agent will work fine without it, you just won't see traces in the Langfuse dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zf7wy8wj5cn",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import tracing utilities\n",
    "from hallucination_utils.tracing import set_up_notebook_langfuse\n",
    "\n",
    "# Set up Langfuse (will prompt for credentials if not already configured)\n",
    "set_up_notebook_langfuse()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "u2j4du1l6h",
   "metadata": {},
   "source": [
    "### Step 2: Create Base Model\n",
    "\n",
    "Let's set up our base model - we'll use Claude 3.7 Sonnet via AWS Bedrock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "apd0fvw6ixt",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the base Bedrock model\n",
    "base_model = BedrockModel(model_id=\"global.anthropic.claude-haiku-4-5-20251001-v1:0\")\n",
    "\n",
    "print(\"âœ“ Base model initialized:\", base_model.get_config()[\"model_id\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "s1syy9y8e9",
   "metadata": {},
   "source": [
    "### Step 3: Build Semantic Similarity Model Wrapper\n",
    "\n",
    "Now we'll create a custom model wrapper that:\n",
    "1. **Calls the base model multiple times in parallel** (3x in this example)\n",
    "2. **Computes semantic similarity** between all candidate responses\n",
    "3. **Consolidates to the best response** or raises an alert if inconsistency is detected\n",
    "\n",
    "This wrapper extends `ParallelModelWithConsolidation` from the `hallucination_utils` module, which handles:\n",
    "- Parallel execution of multiple model calls\n",
    "- Stream processing and response collection\n",
    "- Telemetry integration for observability\n",
    "\n",
    "We just need to implement the `consolidate()` method with our semantic similarity logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9rs9zrf8wwd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class SemanticSimilarityModel(ParallelModelWithConsolidation):\n",
    "    \"\"\"\n",
    "    Custom model wrapper that detects hallucinations via semantic similarity analysis.\n",
    "\n",
    "    This model:\n",
    "    - Generates N candidate responses in parallel\n",
    "    - Computes pairwise semantic similarity between all candidates\n",
    "    - Intervenes if mean similarity falls below threshold\n",
    "    - Returns the first response if all candidates are consistent\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, base_model, num_candidates: int = 3, similarity_threshold: float = 0.75\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            base_model: The Strands model to wrap (e.g., BedrockModel)\n",
    "            num_candidates: Number of parallel responses to generate\n",
    "            similarity_threshold: Minimum mean similarity score (0-1) to pass\n",
    "        \"\"\"\n",
    "        # Create list of models (repeat the same model N times)\n",
    "        super().__init__([base_model] * num_candidates)\n",
    "        self.similarity_threshold = similarity_threshold\n",
    "        self.encoder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "    def consolidate(\n",
    "        self, alternatives: list[ModelResponse]\n",
    "    ) -> ModelResponse | tuple[ModelResponse, TraceAttributes]:\n",
    "        \"\"\"\n",
    "        Analyze semantic similarity between candidate responses.\n",
    "\n",
    "        Returns:\n",
    "            Consolidated response with telemetry attributes\n",
    "        \"\"\"\n",
    "        # Extract response texts from alternatives\n",
    "        response_texts = [\n",
    "            alt.message[\"content\"][0][\"text\"]\n",
    "            for alt in alternatives\n",
    "            if \"content\" in alt.message and len(alt.message[\"content\"]) > 0\n",
    "        ]\n",
    "\n",
    "        if len(response_texts) < 2:\n",
    "            # Not enough responses to compare - return first one\n",
    "            return (\n",
    "                ModelResponse.from_alternatives(\n",
    "                    alternatives=alternatives, final_message=alternatives[0].message\n",
    "                ),\n",
    "                {\n",
    "                    \"gen_ai.guardrail.semantic_similarity.intervened\": False,\n",
    "                    \"gen_ai.guardrail.semantic_similarity.status\": \"insufficient_samples\",\n",
    "                },\n",
    "            )\n",
    "\n",
    "        # Step 1: Encode all responses to embeddings\n",
    "        embeddings = self.encoder.encode(response_texts)\n",
    "\n",
    "        # Step 2: Calculate pairwise similarities\n",
    "        similarity_matrix = cosine_similarity(embeddings)\n",
    "\n",
    "        # Step 3: Get upper triangle (excluding diagonal)\n",
    "        mask = np.triu(np.ones_like(similarity_matrix, dtype=bool), k=1)\n",
    "        similarities = similarity_matrix[mask]\n",
    "\n",
    "        # Step 4: Calculate metrics\n",
    "        mean_similarity = float(np.mean(similarities))\n",
    "        min_similarity = float(np.min(similarities))\n",
    "        max_similarity = float(np.max(similarities))\n",
    "        std_similarity = float(np.std(similarities))\n",
    "\n",
    "        # Step 5: Determine if we should intervene\n",
    "        intervened = mean_similarity < self.similarity_threshold\n",
    "\n",
    "        # Step 6: Prepare telemetry attributes\n",
    "        trace_attrs = {\n",
    "            \"gen_ai.guardrail.semantic_similarity.intervened\": intervened,\n",
    "            \"gen_ai.guardrail.semantic_similarity.mean_score\": mean_similarity,\n",
    "            \"gen_ai.guardrail.semantic_similarity.min_score\": min_similarity,\n",
    "            \"gen_ai.guardrail.semantic_similarity.max_score\": max_similarity,\n",
    "            \"gen_ai.guardrail.semantic_similarity.std_score\": std_similarity,\n",
    "            \"gen_ai.guardrail.semantic_similarity.threshold\": self.similarity_threshold,\n",
    "            \"gen_ai.guardrail.semantic_similarity.num_candidates\": len(alternatives),\n",
    "        }\n",
    "\n",
    "        if intervened:\n",
    "            # Hallucination detected - you could raise an error, return a warning, etc.\n",
    "            trace_attrs[\"gen_ai.guardrail.semantic_similarity.status\"] = (\n",
    "                \"INCONSISTENT_RESPONSES\"\n",
    "            )\n",
    "            print(f\"âš ï¸ Semantic Similarity Guardrail: Inconsistent responses detected!\")\n",
    "            print(\n",
    "                f\"   Mean similarity: {mean_similarity:.3f} (threshold: {self.similarity_threshold})\"\n",
    "            )\n",
    "        else:\n",
    "            trace_attrs[\"gen_ai.guardrail.semantic_similarity.status\"] = (\n",
    "                \"CONSISTENT_RESPONSES\"\n",
    "            )\n",
    "            print(f\"âœ“ Semantic Similarity Guardrail: Responses are consistent\")\n",
    "            print(f\"   Mean similarity: {mean_similarity:.3f}\")\n",
    "\n",
    "        # Return first response as the consolidated output\n",
    "        # (In production, you might choose the response with highest avg similarity to others)\n",
    "        return (\n",
    "            ModelResponse.from_alternatives(\n",
    "                alternatives=alternatives, final_message=alternatives[0].message\n",
    "            ),\n",
    "            trace_attrs,\n",
    "        )\n",
    "\n",
    "\n",
    "print(\"âœ“ SemanticSimilarityModel class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pa0oithx3e",
   "metadata": {},
   "source": [
    "### Step 4: Instantiate the Semantic Similarity Model\n",
    "\n",
    "Now let's create an instance of our guardrail-enabled model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kxk4pii0h3h",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create semantic similarity model with 3 parallel candidates\n",
    "semsim_model = SemanticSimilarityModel(\n",
    "    base_model=base_model,\n",
    "    num_candidates=3,\n",
    "    similarity_threshold=0.75,  # Adjust based on your requirements\n",
    ")\n",
    "\n",
    "print(\"âœ“ Semantic Similarity Model created\")\n",
    "print(f\"  - Number of parallel candidates: 3\")\n",
    "print(f\"  - Similarity threshold: 0.75\")\n",
    "print(f\"  - Encoder model: all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "x2dv95kzveq",
   "metadata": {},
   "source": [
    "### Step 5: Create the Strands Agent\n",
    "\n",
    "Finally, let's create a Strands agent that uses our semantic similarity model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dhala7c95o7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create agent with our custom model\n",
    "agent = Agent(model=semsim_model)\n",
    "\n",
    "print(\"âœ“ Agent created and ready to use!\")\n",
    "print(\"\\nThe agent will:\")\n",
    "print(\"  1. Generate 3 candidate responses in parallel for each query\")\n",
    "print(\"  2. Compute semantic similarity between all candidates\")\n",
    "print(\"  3. Alert if responses are inconsistent (potential hallucination)\")\n",
    "print(\"  4. Return the most reliable response\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2hbildicrvl",
   "metadata": {},
   "source": [
    "### Step 6: Test the Agent\n",
    "\n",
    "Let's test our agent with different types of questions to see the semantic similarity guardrail in action.\n",
    "\n",
    "#### Test 1: Factual Question (Should be Consistent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0w7sufodah",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"TEST 1: Factual Question\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "result = agent(\"What is the capital of France?\")\n",
    "print(\"\\nFinal Response:\", result.message[\"content\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nc64xlz0l4m",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_with_semantic_similarity(\n",
    "    prompt: str,\n",
    "    num_candidates: int = 3,\n",
    "    similarity_threshold: float = 0.75,\n",
    "    show_details: bool = True,\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Evaluate a prompt using semantic similarity-based hallucination detection.\n",
    "\n",
    "    Args:\n",
    "        prompt: The question/prompt to evaluate\n",
    "        num_candidates: Number of parallel responses to generate\n",
    "        similarity_threshold: Minimum similarity score to pass (0-1)\n",
    "        show_details: Whether to print detailed analysis\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with response and detection metrics\n",
    "    \"\"\"\n",
    "    # Create model and agent\n",
    "    model = SemanticSimilarityModel(\n",
    "        base_model=base_model,\n",
    "        num_candidates=num_candidates,\n",
    "        similarity_threshold=similarity_threshold,\n",
    "    )\n",
    "    eval_agent = Agent(model=model)\n",
    "\n",
    "    if show_details:\n",
    "        print(f\"\\n{'=' * 70}\")\n",
    "        print(f\"SEMANTIC SIMILARITY EVALUATION\")\n",
    "        print(f\"{'=' * 70}\")\n",
    "        print(f\"Prompt: {prompt}\")\n",
    "        print(f\"Candidates: {num_candidates} | Threshold: {similarity_threshold}\")\n",
    "        print(f\"{'=' * 70}\\n\")\n",
    "\n",
    "    # Run the agent\n",
    "    result = eval_agent(prompt)\n",
    "\n",
    "    # Extract response\n",
    "    response_text = result.message[\"content\"][0][\"text\"]\n",
    "\n",
    "    if show_details:\n",
    "        print(f\"\\n{'=' * 70}\")\n",
    "        print(f\"FINAL RESPONSE:\")\n",
    "        print(f\"{'=' * 70}\")\n",
    "        print(response_text)\n",
    "        print(f\"{'=' * 70}\")\n",
    "\n",
    "    return {\n",
    "        \"prompt\": prompt,\n",
    "        \"response\": response_text,\n",
    "        \"num_candidates\": num_candidates,\n",
    "        \"threshold\": similarity_threshold,\n",
    "        \"result\": result,\n",
    "    }\n",
    "\n",
    "\n",
    "# Example usage\n",
    "evaluation = evaluate_with_semantic_similarity(\n",
    "    \"What are the main causes of climate change?\",\n",
    "    num_candidates=3,\n",
    "    similarity_threshold=0.75,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ee9744-fba5-457d-8b29-4c81748c5f27",
   "metadata": {},
   "source": [
    "You can go to the Langfuse console to investigate the detailed traces of the agent outputs.\n",
    "\n",
    "![langfuse-lab2](./img/langfuse-lab2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qfl6j2vzdmh",
   "metadata": {},
   "source": [
    "### Practical Wrapper Function\n",
    "\n",
    "Here's a practical wrapper you can use to easily evaluate prompts with the semantic similarity agent:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7u1z16wggou",
   "metadata": {},
   "source": [
    "### Production Deployment Considerations\n",
    "\n",
    "When deploying this pattern to production, consider:\n",
    "\n",
    "#### 1. Performance vs. Accuracy Tradeoffs\n",
    "- **Latency**: Running 3 parallel model calls increases latency slightly, but reduces wall-clock time vs. sequential calls\n",
    "- **Cost**: 3x model invocations = 3x cost per query\n",
    "- **Optimization**: Use caching, reduce `num_candidates` for non-critical queries, or use faster/cheaper models\n",
    "\n",
    "#### 2. Threshold Tuning\n",
    "- **similarity_threshold=0.75** is a starting point - tune based on your domain\n",
    "- Lower threshold (e.g., 0.6) = fewer false positives, but might miss hallucinations\n",
    "- Higher threshold (e.g., 0.9) = stricter checking, but might flag legitimate variation\n",
    "- Use A/B testing and human evaluation to find optimal threshold\n",
    "\n",
    "#### 3. Error Handling\n",
    "- Current implementation prints warnings but returns a response anyway\n",
    "- In production, you might:\n",
    "  - Raise an exception to force human review\n",
    "  - Return a confidence score to the user\n",
    "  - Fallback to a RAG system or web search\n",
    "  - Log to monitoring system for offline analysis\n",
    "\n",
    "#### 4. Observability\n",
    "- Enable Langfuse or another observability platform\n",
    "- Track metrics: `mean_similarity`, `intervention_rate`, `user_satisfaction`\n",
    "- Set up alerts when intervention rate exceeds normal baselines\n",
    "- Review flagged responses to improve prompts and thresholds\n",
    "\n",
    "#### 5. Advanced Consolidation Strategies\n",
    "- Current implementation returns the first response\n",
    "- Alternatives:\n",
    "  - Return the response with **highest average similarity** to others\n",
    "  - Use **majority voting** for classification tasks\n",
    "  - **Blend responses** for creative tasks\n",
    "  - Apply **secondary checks** (e.g., NLI contradiction detection)\n",
    "\n",
    "#### 6. Multi-Model Ensembles\n",
    "- Instead of calling the same model 3x, call different models:\n",
    "  ```python\n",
    "  models = [\n",
    "      BedrockModel(\"claude-haiku-4-5\"),\n",
    "      BedrockModel(\"claude-3-5-sonnet\"),\n",
    "      # Could even mix SageMaker models, etc.\n",
    "  ]\n",
    "  ensemble_model = SemanticSimilarityModel(models)\n",
    "  ```\n",
    "- Provides diversity and can catch model-specific hallucinations\n",
    "\n",
    "#### 7. Integration with Other Guardrails\n",
    "- Combine with **Bedrock Guardrails** for content filtering\n",
    "- Add **contextual grounding** checks if using RAG\n",
    "- Layer with **Token Probability Level methods** (perplexity, entropy) for more robust detection\n",
    "\n",
    "<br/>\n",
    "\n",
    "> **Key Considerations**\n",
    ">\n",
    "> These methods have higher latency and computational cost due to multiple model generations and additional processing. However, Section 6 demonstrates how to mitigate this through parallel execution and production optimization strategies. These methods are valuable for high-stakes applications where accuracy is critical, and can be combined with faster methods for different use cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e11a07-f94a-454d-9d2c-c530c4acbe16",
   "metadata": {},
   "source": [
    "## 7. Conclusion and Best Practices\n",
    "(<a href=\"#top\">Go to top</a>)\n",
    "\n",
    "### What We've Learned\n",
    "\n",
    "In this notebook, we've explored three powerful Response Level techniques for detecting LLM hallucinations:\n",
    "\n",
    "1. **Semantic Similarity Detection** - Fast and effective for detecting response consistency\n",
    "2. **Non-Contradiction Probability** - Excellent for identifying logical inconsistencies  \n",
    "3. **Normalized Semantic Negentropy** - Most sophisticated, measuring semantic uncertainty\n",
    "<br>\n",
    "\n",
    "| Method | What It Detects | When to Use (High-Level Context) | Why It Works |\n",
    "|--------|----------------|----------------------------------|--------------|\n",
    "| **Semantic Similarity Detection** | Inconsistent knowledge across multiple generations | â€¢ **Content verification** where consistency matters<br>â€¢ **Knowledge validation** with available compute budget<br>â€¢ **Quick consistency checks** for important claims | True knowledge should be expressed the same way repeatedly - if answers vary wildly, the model isn't really \"sure\" |\n",
    "| **Non-Contradiction Probability** | Logical inconsistencies between responses | â€¢ **Factual content** where contradictions are unacceptable<br>â€¢ **Research applications** requiring logical coherence<br>â€¢ **Decision support** systems with clear right/wrong answers | Real facts don't contradict themselves - if multiple responses conflict, at least one must be wrong |\n",
    "| **Normalized Semantic Negentropy** | Semantic uncertainty through response clustering | â€¢ **High-precision applications** requiring confidence scores<br>â€¢ **Research environments** with computational resources<br>â€¢ **Advanced quality control** for critical outputs | Confident knowledge produces similar responses that cluster together - scattered responses indicate uncertainty |\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- **No single method is perfect** - Different techniques catch different types of hallucinations\n",
    "- **Model quality varies significantly** - Some models are much more consistent than others\n",
    "- **Question type matters** - Factual questions and creative prompts require different approaches\n",
    "- **Confidence thresholds are domain-specific** - Adjust based on your use case criticality\n",
    "\n",
    "### Best Practices for Production\n",
    "\n",
    "1. **Use Multiple Methods**: Combine techniques for more robust detection\n",
    "2. **Set Appropriate Thresholds**: Adjust confidence levels based on domain requirements\n",
    "3. **Consider Computational Cost**: Balance accuracy with response time needs\n",
    "4. **Monitor and Adapt**: Continuously evaluate and update detection parameters\n",
    "5. **Human-in-the-Loop**: Always have escalation paths for uncertain cases\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- **Integrate with existing systems**: Add hallucination detection to your AI pipelines\n",
    "- **Build domain-specific datasets**: Create evaluation sets for your specific use cases\n",
    "- **Experiment with ensemble methods**: Combine multiple models and detection techniques\n",
    "- **Explore fine-tuning**: Train models to be more consistent in your domain\n",
    "\n",
    "### Additional Resources\n",
    "\n",
    "- [AWS Bedrock Documentation](https://docs.aws.amazon.com/bedrock/)\n",
    "- [Sentence Transformers Documentation](https://www.sbert.net/)\n",
    "- [HuggingFace Transformers Guide](https://huggingface.co/docs/transformers)\n",
    "\n",
    "---\n",
    "\n",
    "**Remember**: The goal isn't to eliminate all uncertainty, but to **quantify and manage it** appropriately for your specific use cases.\n",
    "\n",
    "Happy detecting!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810acda7-1a6b-4d55-851c-0e29ebe73e40",
   "metadata": {},
   "source": [
    "## 8. (Optional) Challenge Exercises\n",
    "(<a href=\"#top\">Go to top</a>)\n",
    "\n",
    "<div style=\"border: 4px solid coral; text-align: left; margin: auto; padding: 20px;\">\n",
    "    <h3>ðŸŽ¯ Try These Exercises on your own time!</h3>\n",
    "    <p><b>Complete these challenges to deepen your understanding of hallucination detection:</b></p>\n",
    "    <h4>Beginner Challenges:</h4>\n",
    "    <ol>\n",
    "        <li><b>Different Question Types:</b> Test the methods with various question types:\n",
    "            <ul>\n",
    "                <li>Historical facts: \"When did World War II end?\"</li>\n",
    "                <li>Scientific concepts: \"What is photosynthesis?\"</li>\n",
    "                <li>Fictional scenarios: \"What is Sherlock Holmes' favorite tea?\"</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li><b>Model Comparison:</b> Compare all available models on the same question</li>\n",
    "        <li><b>Parameter Tuning:</b> Experiment with different temperature values and sample sizes</li>\n",
    "    </ol>\n",
    "    <h4>Intermediate Challenges:</h4>\n",
    "    <ol start=\"4\">\n",
    "        <li><b>Custom Thresholds:</b> Modify the confidence thresholds in each method</li>\n",
    "        <li><b>Ambiguous Questions:</b> Test with deliberately ambiguous prompts</li>\n",
    "        <li><b>Method Combination:</b> Create a combined score using all three methods</li>\n",
    "    </ol>\n",
    "    <h4>Advanced Challenges:</h4>\n",
    "    <ol start=\"7\">\n",
    "        <li><b>Domain-Specific Testing:</b> Test on medical, legal, or technical questions</li>\n",
    "        <li><b>Multi-Language Support:</b> Adapt methods for non-English responses</li>\n",
    "        <li><b>Real-Time Application:</b> Build a web interface for live hallucination detection</li>\n",
    "    </ol>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
