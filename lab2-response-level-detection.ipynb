{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98851f24-a2f5-42c6-8435-219764f5af33",
   "metadata": {},
   "source": [
    "# <a id=\"top\">Lab 2: Response Level Detection</a>\n",
    "\n",
    "## Detecting LLM Hallucinations Through Response Consistency Analysis\n",
    "\n",
    "For many applications built on closed-source, proprietary models accessed via APIs, we don't have access to the model's internal states (like weights or logits) or ground truth data to compare against. **Response Level Detection** techniques are designed for this scenario, treating the model as an opaque system and relying only on its input-output behavior to assess reliability and detect potential hallucinations.\n",
    "\n",
    "In this notebook, we will implement and compare three cutting-edge response level detection methods that work without requiring reference answers or model internals. These methods analyze the **consistency and coherence of multiple model responses** to the same prompt.\n",
    "\n",
    "We'll implement and compare three cutting-edge detection methods:\n",
    "1. **Semantic Similarity Analysis** - Measuring consistency across multiple model responses using embeddings\n",
    "2. **Non-Contradiction Probability** - Using Natural Language Inference (NLI) to detect logical contradictions\n",
    "3. **Normalized Semantic Negentropy (NSN)** - Measuring semantic uncertainty through response clustering\n",
    "\n",
    "<div style=\"border: 4px solid coral; text-align: left; margin: auto; padding-left: 20px; padding-right: 20px\">\n",
    "    <h4>üí° Key Learning Objectives</h4>\n",
    "    <ul>\n",
    "        <li>Understand Response Level hallucination detection techniques</li>\n",
    "        <li>Learn how to implement semantic similarity analysis with sentence embeddings</li>\n",
    "        <li>Master contradiction detection using Natural Language Inference (NLI) models</li>\n",
    "        <li>Apply semantic entropy methods for uncertainty quantification</li>\n",
    "        <li>Build AI agents with Strands Agents SDK that automatically detect hallucinations</li>\n",
    "    </ul>\n",
    "</div>\n",
    "<br/>\n",
    "\n",
    "## Use-Case Overview\n",
    "\n",
    "Response Level hallucination detection is crucial when:\n",
    "- You don't have ground truth answers to compare against\n",
    "- You need to assess model reliability in production without access to model internals\n",
    "- You want to identify when a model is \"making things up\"\n",
    "- You need to implement confidence scoring for AI responses\n",
    "\n",
    "Our approach leverages the principle that **reliable knowledge should be consistent** across multiple generations with different sampling parameters, while hallucinations tend to vary significantly.\n",
    "\n",
    "\n",
    "## Sections\n",
    "\n",
    "This notebook has the following sections:\n",
    "\n",
    "1. [Environment Setup and Model Configuration](#1.-Environment-Setup-and-Model-Configuration)\n",
    "2. [Method 1: Semantic Similarity Detection](#2.-Method-1:-Semantic-Similarity-Detection)\n",
    "3. [Method 2: Non-Contradiction Probability](#3.-Method-2:-Non-Contradiction-Probability)\n",
    "4. [Method 3: Normalized Semantic Negentropy (NSN)](#4.-Method-3:-Normalized-Semantic-Negentropy-(NSN))\n",
    "5. [Comparative Analysis Across Models](#5.-Comparative-Analysis-Across-Models)\n",
    "6. [Building Production-Ready Agents with Semantic Similarity](#6.-Building-Production-Ready-Agents-with-Semantic-Similarity)\n",
    "7. [Conclusion and Best Practices](#7.-Conclusion-and-Best-Practices)\n",
    "8. [(Optional) Challenge Exercises](#8.-(Optional)-Challenge-Exercises)\n",
    "    \n",
    "Please work from top to bottom and don't skip sections as this could lead to error messages due to missing dependencies.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5463f03b-7f8b-4a95-a7c8-0caa46d6ae4b",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Model Configuration\n",
    "(<a href=\"#top\">Go to top</a>)\n",
    "\n",
    "**If you haven't already** installed the workshop's dependencies (from [pyproject.toml](./pyproject.toml)), you can un-comment (remove `# `) and run the below cell to do so. We've commented it out by default, assuming you already ran it at the start of lab 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8risldzg6od",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -q -e ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07e09b1",
   "metadata": {},
   "source": [
    "<div style=\"border: 4px solid coral; text-align: left; margin: auto; padding-left: 20px; padding-right: 20px; padding-top: 15px;\">\n",
    "    <h4>üîÑ Restart the kernel after installing</h4>\n",
    "    <p>\n",
    "        <strong>IF</strong> you ran the above install command cell, you'll need to restart the\n",
    "        notebook kernel afterwards for the installations to take full effect.\n",
    "    </p>\n",
    "    <p>\n",
    "        Note that you may see some error notices about dependency conflicts in SageMaker Studio\n",
    "        environments, but this is okay as long as the installations are completed.\n",
    "    </p>\n",
    "</div>\n",
    "<br/>\n",
    "\n",
    "With the installation complete, you're ready to import the libraries we'll use in the notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31eff904-45e3-471c-881e-6534466429d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Python Built-Ins:\n",
    "from dataclasses import dataclass\n",
    "import json\n",
    "from logging import getLogger, basicConfig\n",
    "import time\n",
    "from typing import Callable, Dict, List, Tuple\n",
    "import warnings\n",
    "\n",
    "# External Dependencies:\n",
    "import boto3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import pipeline, Pipeline\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "# Local Utilities:\n",
    "from hallucination_utils.llm_wrappers import BedrockLLM, SageMakerVLLM\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logger = getLogger()\n",
    "\n",
    "print(\"All packages imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a344378a-6cda-4846-8012-b11afc49db1e",
   "metadata": {},
   "source": [
    "### Configure Foundation Models\n",
    "\n",
    "We'll test the hallucination detection models in this lab on *both* the Amazon Bedrock Foundation Model as used in lab 1, and the Open Weights model we deployed on Amazon SageMaker AI in lab 0.\n",
    "\n",
    "Although the underlying APIs for these two services are different (see Bedrock [`converse()`](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/bedrock-runtime/client/converse.html) and SageMaker [`invoke_endpoint()`](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker-runtime/client/invoke_endpoint.html) in Python), all we really need for this lab is to be able to send in a text prompt and get back the text response. Open Source libraries like [LangChain](https://docs.langchain.com/oss/python/langchain/models) and [LiteLLM](https://docs.litellm.ai/docs/providers) offer standardised wrappers for a wide variety of LLM hosting providers, but it's also pretty easy to build your own if you value minimizing dependencies - which is what we've done here. If you're curious, you can see the implementations in [hallucination_utils/llm_wrappers.py](hallucination_utils/llm_wrappers.py)\n",
    "\n",
    "#### Amazon Bedrock\n",
    "\n",
    "First, let's connect to Anthropic Claude on Amazon Bedrock as we used earlier in lab 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15cf3903-9360-4a95-ab9a-6fa0fcabe51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "BEDROCK_MODEL_ID = \"global.anthropic.claude-haiku-4-5-20251001-v1:0\"\n",
    "\n",
    "claude_llm = BedrockLLM(model_id=BEDROCK_MODEL_ID)\n",
    "print(f\"Using Bedrock model '{claude_llm.model_id}'\")\n",
    "\n",
    "test_prompt = \"What is 2 + 2?\"\n",
    "print(f\"\\nQuestion: {test_prompt}\\nAnswer:\")\n",
    "print(claude_llm.invoke(test_prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48434f4-e3aa-432f-82ad-9d190383c539",
   "metadata": {},
   "source": [
    "#### Amazon SageMaker AI\n",
    "\n",
    "Next, we'll connect to the Open Weight GPT-OSS model you deployed to Amazon SageMaker AI in lab 0.\n",
    "\n",
    "For this, we'll need the endpoint name and inference component name. The below cell will try to retrieve those automatically from where they were `%store`d in the lab 0 notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b040e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r endpoint_name\n",
    "%store -r inference_component_name\n",
    "# (Alternatively if this is not working, could just define these variables here directly)\n",
    "\n",
    "try:\n",
    "    endpoint_name, inference_component_name\n",
    "except NameError as e:\n",
    "    raise RuntimeError(\n",
    "        \"SageMaker endpoint not found. Please check you've run lab 0 first!\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9858d8-4975-4ed0-a710-6444e2e9dab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_oss_llm = SageMakerVLLM(\n",
    "    endpoint_name=endpoint_name,\n",
    "    inference_component_name=inference_component_name,\n",
    ")\n",
    "print(\n",
    "    \"Using SageMaker endpoint '%s'\\n  ‚îî‚îÄ inf component '%s'\"\n",
    "    % (gpt_oss_llm.endpoint_name, gpt_oss_llm.inference_component_name)\n",
    ")\n",
    "\n",
    "test_prompt = \"What is 2 + 2?\"\n",
    "print(f\"\\nQuestion: {test_prompt}\\nAnswer:\")\n",
    "print(gpt_oss_llm.invoke(test_prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a44a033-85b7-4904-b5e8-e99678a58f96",
   "metadata": {},
   "source": [
    "### Generating Multiple Responses\n",
    "\n",
    "Since all the methods we'll explore here depend on generating and comparing **multiple** responses to a single input prompt, we'll set up a re-usable utility to handle that (and ignore if a small fraction of the generations fail due to transient errors):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ae89b8-4927-4b75-95b1-7fde4db20bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_responses(\n",
    "    models: list, prompt: str, min_gen_pct: float = 0.8\n",
    ") -> list[str]:\n",
    "    n_models = len(models)\n",
    "    print(f'üí¨ Generating {n_models} responses for prompt:\\n\"{prompt}\"')\n",
    "    results = []\n",
    "    for ix, model in enumerate(models):\n",
    "        try:\n",
    "            res = model.invoke(prompt)\n",
    "            results.append(res)\n",
    "            print(f'ü§ñ Model {ix + 1}/{n_models} responded:\\n\"{res}\"')\n",
    "        except Exception as e:\n",
    "            # In case of intermittent throttling or other issues during the workshop,\n",
    "            # we'll try to carry on if some of the responses fail to generate\n",
    "            logger.info(\"Failed to generate model response: %s\", e)\n",
    "    n_gens = len(results)\n",
    "    n_fails = n_models - n_gens\n",
    "    if n_fails:\n",
    "        gen_pct = n_fails / n_models\n",
    "        msg = (\n",
    "            \"%s of %s models (%.1%) failed to generate a response - trying to continue\"\n",
    "            % (n_fails, n_models, gen_pct)\n",
    "        )\n",
    "        if gen_pct < min_gen_pct:\n",
    "            raise RuntimeError(msg)\n",
    "        else:\n",
    "            logger.warning(msg)\n",
    "    print()\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d050fea7-affd-4b24-ac1c-418e710bc6b6",
   "metadata": {},
   "source": [
    "The actual approach you take to generate your multiple responses is flexible, depending on the analysis you're performing. For example, it could be:\n",
    "- Multiple calls to the exact same model with the exact same inference parameters (Since LLMs are typically non-deterministic)\n",
    "- Calls to the same model with a range of inference parameters (like varying the temperature or maximum length a little around your target value)\n",
    "- Calls to multiple different models\n",
    "\n",
    "In this lab we'll mainly use one model at a time, but vary the temperature to encourage some diversity in the responses. Run the cells below to set up these groups for both Claude on Bedrock and your Open Weight model on SageMaker:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789c1a93-1349-4aaf-88c4-b6548aa85608",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 5\n",
    "claude_models = [\n",
    "    BedrockLLM(\n",
    "        model_id=BEDROCK_MODEL_ID, temperature=0.1 + 0.9 * (ix / (n_samples - 1))\n",
    "    )\n",
    "    for ix in range(n_samples)\n",
    "]\n",
    "\n",
    "answers = generate_responses(\n",
    "    claude_models, \"Choose a preferred name for yourself, and reply with only the name.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6c0e4a-57e5-4eaa-951f-516082f24116",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 5\n",
    "gpt_oss_models = [\n",
    "    SageMakerVLLM(\n",
    "        endpoint_name=endpoint_name,\n",
    "        inference_component_name=inference_component_name,\n",
    "        temperature=0.1 + 0.9 * (ix / (n_samples - 1)),\n",
    "    )\n",
    "    for ix in range(n_samples)\n",
    "]\n",
    "\n",
    "answers = generate_responses(\n",
    "    gpt_oss_models,\n",
    "    \"Choose a preferred name for yourself, and reply with only the name.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5ffee3-2bc7-4296-884e-8fa00323a0e3",
   "metadata": {},
   "source": [
    "Now we're comfortable with generating a batch of possible answers to a given prompt, let's dive in to the methods for analyzing them for lack of confidence and likely hallucinations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53daa38a-d3a6-4cc3-823c-c3d2fdd9be92",
   "metadata": {},
   "source": [
    "## 2. Method 1: Semantic Similarity Detection\n",
    "(<a href=\"#top\">Go to top</a>)\n",
    "\n",
    "### How It Works\n",
    "\n",
    "Semantic Similarity Detection works on the principle that **reliable information should be expressed consistently**, even when generated multiple times with different parameters.\n",
    "\n",
    "**The Process:**\n",
    "\n",
    "Given multiple candidate responses (in this case we'll use the same model, but vary the temperature):\n",
    "1. **Create embeddings**: Convert each response to a high-dimensional vector representation\n",
    "2. **Calculate pairwise similarities**: Measure cosine similarity between all response pairs\n",
    "3. **Assess consistency**: High similarity = reliable, low similarity = potential hallucination\n",
    "\n",
    "**Key Principle**: Reliable information should be expressed consistently, regardless of sampling parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f8b92d-93e9-4fd2-bf8d-07f246af2af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_similarity_detection(\n",
    "    responses: list[str],\n",
    "    embedding_model=SentenceTransformer(\"all-MiniLM-L6-v2\"),\n",
    ") -> dict:\n",
    "    \"\"\"Analyze semantic similarity across multiple responses to detect hallucinations\n",
    "\n",
    "    Args:\n",
    "        responses: The multiple alternative responses generated by the model\n",
    "        embedding_model: A SentenceTransformer-compatible model to calculate text embedding vectors\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with similarity metrics and confidence assessment\n",
    "    \"\"\"\n",
    "    n_responses = len(responses)\n",
    "    if n_responses < 2:\n",
    "        raise ValueError(\n",
    "            f\"Need at least 2 responses for semantic similarity comparison. Got {n_responses}\"\n",
    "        )\n",
    "    print(f\"üîç Analyzing semantic similarity of {n_responses} candidate responses...\")\n",
    "\n",
    "    # Step 1: Create a semantic embedding vector for each response\n",
    "    print(f\"‚îú‚îÄ Creating semantic embeddings...\")\n",
    "    embeddings = embedding_model.encode(responses)\n",
    "\n",
    "    # Step 2: Calculate pairwise similarities\n",
    "    print(f\"‚îú‚îÄ Calculating semantic similarities...\")\n",
    "    similarity_matrix = cosine_similarity(embeddings)\n",
    "\n",
    "    # Get all pairwise similarities (excluding diagonal similarity-with-self)\n",
    "    mask = np.ones(similarity_matrix.shape, dtype=bool)\n",
    "    np.fill_diagonal(mask, False)\n",
    "    similarities = similarity_matrix[mask]\n",
    "\n",
    "    # Step 3: Calculate metrics and determine confidence level\n",
    "    mean_similarity = np.mean(similarities)\n",
    "    std_similarity = np.std(similarities)\n",
    "    min_similarity = np.min(similarities)\n",
    "    max_similarity = np.max(similarities)\n",
    "\n",
    "    if mean_similarity > 0.95 and std_similarity < 0.03:\n",
    "        confidence = \"üü¢ HIGH\"\n",
    "        interpretation = \"Responses are highly consistent - reliable knowledge\"\n",
    "    elif mean_similarity > 0.85 and std_similarity < 0.08:\n",
    "        confidence = \"üü° MEDIUM\"\n",
    "        interpretation = \"Moderate consistency - some uncertainty present\"\n",
    "    else:\n",
    "        confidence = \"üî¥ LOW\"\n",
    "        interpretation = \"Inconsistent responses - possible hallucination\"\n",
    "\n",
    "    # Display results\n",
    "    print(f\"‚îú‚îÄ METRICS:\")\n",
    "    print(f\"‚îÇ   ‚îú‚îÄ Mean Similarity:  {mean_similarity:.3f}\")\n",
    "    print(f\"‚îÇ   ‚îú‚îÄ Std Deviation:    {std_similarity:.3f}\")\n",
    "    print(f\"‚îÇ   ‚îú‚îÄ Min Similarity:   {min_similarity:.3f}\")\n",
    "    print(f\"‚îÇ   ‚îî‚îÄ Max Similarity:   {max_similarity:.3f}\")\n",
    "    print(f\"‚îî‚îÄ CONFIDENCE: {confidence}\")\n",
    "    print(f\"    ‚îî‚îÄ {interpretation}\")\n",
    "\n",
    "    return {\n",
    "        \"method\": \"Semantic Similarity\",\n",
    "        \"num_responses\": len(responses),\n",
    "        \"mean_similarity\": round(mean_similarity, 3),\n",
    "        \"std_similarity\": round(std_similarity, 3),\n",
    "        \"min_similarity\": round(min_similarity, 3),\n",
    "        \"confidence\": confidence,\n",
    "        \"interpretation\": interpretation,\n",
    "        \"responses\": responses,\n",
    "    }\n",
    "\n",
    "\n",
    "# Don't worry about the red warnings that might get generated here:\n",
    "print(\"‚úÖ Set up semantic_similarity_detection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c5a893-c267-4596-a4b4-4176ed50fbd9",
   "metadata": {},
   "source": [
    "### Test Semantic Similarity Detection\n",
    "\n",
    "Let's test this method with a question that should have a clear, factual answer.\n",
    "\n",
    "We'll compare 5 responses generated using Claude on Bedrock, at a range of different temperatures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395170cc-15d7-4f02-be50-e67698af8708",
   "metadata": {},
   "outputs": [],
   "source": [
    "semsim_bedrock_responses = generate_responses(\n",
    "    claude_models, \"What album was Dua Lipa's 'Hallucinate' featured on?\"\n",
    ")\n",
    "\n",
    "semantic_similarity_detection(semsim_bedrock_responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53317ad8",
   "metadata": {},
   "source": [
    "### Optional: Test with SageMaker Endpoint\n",
    "\n",
    "If you have SageMaker endpoints with inference components available, you can test them here. Otherwise, skip to the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b6ac5b-d203-49f6-8c17-7dcecd042156",
   "metadata": {},
   "outputs": [],
   "source": [
    "semsim_sm_responses = generate_responses(\n",
    "    gpt_oss_models,\n",
    "    \"What album was Dua Lipa's 'Hallucinate' featured on?\",\n",
    ")\n",
    "\n",
    "semantic_similarity_detection(semsim_sm_responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96296d9c-6e24-4be6-a5fd-920068e71892",
   "metadata": {},
   "source": [
    "## 3. Method 2: Non-Contradiction Probability\n",
    "(<a href=\"#top\">Go to top</a>)\n",
    "\n",
    "### How It Works\n",
    "\n",
    "Non-Contradiction Probability uses [**Natural Language Inference (NLI)**](https://huggingface.co/tasks/text-classification#natural-language-inference-nli) to detect logical inconsistencies between multiple model responses.\n",
    "\n",
    "**The Process:**\n",
    "\n",
    "Given multiple candidate responses (again in this case we'll use the same model, but vary the temperature):\n",
    "1. **Pairwise contradiction detection**: Compare every response pair using an NLI model to identify logical contradictions\n",
    "2. **Calculate frequency metrics**: The overall frequency of contradictions and the variance between models\n",
    "3. **Assess confidence**: Determine confidence in the answer based on the rate of contradictions observed\n",
    "\n",
    "**Key Principle**: Reliable knowledge should not contradict itself, while hallucinations often contain conflicting information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3800e0c4-0570-4d49-b62e-3579ae69f4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_contradiction_detection(\n",
    "    responses: list[str],\n",
    "    nli_model: Pipeline = pipeline(\n",
    "        \"text-classification\",\n",
    "        model=\"microsoft/deberta-base-mnli\",\n",
    "        return_all_scores=True,\n",
    "    ),\n",
    ") -> dict:\n",
    "    \"\"\"Detect hallucinations by analyzing logical contradictions between responses\n",
    "\n",
    "    Args:\n",
    "        responses: The multiple alternative responses generated by the model\n",
    "        nli_model: HF Transformers Pipeline-compatible model for Natural Language Inference\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with contradiction analysis metrics and confidence assessment\n",
    "    \"\"\"\n",
    "    n_responses = len(responses)\n",
    "    if n_responses < 2:\n",
    "        raise ValueError(f\"Need at least 2 responses for comparison. Got {n_responses}\")\n",
    "    print(\n",
    "        f\"üîç Analyzing non-contradiction probability of {n_responses} candidate responses...\"\n",
    "    )\n",
    "\n",
    "    # Step 1: Analyze all response pairs for contradictions\n",
    "    print(f\"‚îú‚îÄ Analyzing {len(responses)} responses for logical contradictions...\")\n",
    "    non_contradiction_scores = []\n",
    "    pair_analyses = []\n",
    "    for i in range(len(responses)):\n",
    "        for j in range(i + 1, len(responses)):\n",
    "            try:\n",
    "                # Check both directions (A‚ÜíB and B‚ÜíA) for mutual contradiction\n",
    "                result1 = nli_model(f\"{responses[i]} [SEP] {responses[j]}\")\n",
    "                result2 = nli_model(f\"{responses[j]} [SEP] {responses[i]}\")\n",
    "\n",
    "                # Extract contradiction probabilities\n",
    "                contra_prob1 = 0.0\n",
    "                contra_prob2 = 0.0\n",
    "\n",
    "                # Parse NLI results\n",
    "                if isinstance(result1, list) and len(result1) > 0:\n",
    "                    inner_list1 = (\n",
    "                        result1[0] if isinstance(result1[0], list) else result1\n",
    "                    )\n",
    "                    for item in inner_list1:\n",
    "                        if (\n",
    "                            isinstance(item, dict)\n",
    "                            and item.get(\"label\") == \"CONTRADICTION\"\n",
    "                        ):\n",
    "                            contra_prob1 = item.get(\"score\", 0.0)\n",
    "                            break\n",
    "\n",
    "                if isinstance(result2, list) and len(result2) > 0:\n",
    "                    inner_list2 = (\n",
    "                        result2[0] if isinstance(result2[0], list) else result2\n",
    "                    )\n",
    "                    for item in inner_list2:\n",
    "                        if (\n",
    "                            isinstance(item, dict)\n",
    "                            and item.get(\"label\") == \"CONTRADICTION\"\n",
    "                        ):\n",
    "                            contra_prob2 = item.get(\"score\", 0.0)\n",
    "                            break\n",
    "\n",
    "                # Calculate average contradiction probability\n",
    "                avg_contradiction = (contra_prob1 + contra_prob2) / 2\n",
    "                non_contradiction_score = 1 - avg_contradiction\n",
    "\n",
    "                non_contradiction_scores.append(non_contradiction_score)\n",
    "                pair_analyses.append(\n",
    "                    {\n",
    "                        \"pair\": f\"{i + 1}-{j + 1}\",\n",
    "                        \"contradiction\": avg_contradiction,\n",
    "                        \"non_contradiction\": non_contradiction_score,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "                print(\n",
    "                    \"‚îÇ   ‚îú‚îÄ Pair {}-{}: Contradiction={:.3f}, Non-contradiction={:.3f}\".format(\n",
    "                        i + 1, j + 1, avg_contradiction, non_contradiction_score\n",
    "                    )\n",
    "                )\n",
    "\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Error analyzing pair {i + 1}-{j + 1}: {str(e)}\")\n",
    "                non_contradiction_scores.append(0.5)  # Neutral score for errors\n",
    "\n",
    "    if not non_contradiction_scores:\n",
    "        raise RuntimeError(\"Failed to analyze *any* response pairs with NLI\")\n",
    "\n",
    "    # Step 2: Calculate overall metrics\n",
    "    mean_ncp = np.mean(non_contradiction_scores)\n",
    "    std_ncp = np.std(non_contradiction_scores)\n",
    "    min_ncp = np.min(non_contradiction_scores)\n",
    "\n",
    "    # Step 3: Determine confidence level\n",
    "    if mean_ncp > 0.8 and min_ncp > 0.7:\n",
    "        confidence = \"üü¢ HIGH\"\n",
    "        interpretation = \"No significant contradictions - facts are consistent\"\n",
    "    elif mean_ncp > 0.65 and min_ncp > 0.5:\n",
    "        confidence = \"üü° MEDIUM\"\n",
    "        interpretation = \"Minor contradictions detected - mostly consistent\"\n",
    "    else:\n",
    "        confidence = \"üî¥ LOW\"\n",
    "        interpretation = \"Significant contradictions detected - likely hallucination\"\n",
    "\n",
    "    # Display results\n",
    "    print(f\"‚îú‚îÄ METRICS:\")\n",
    "    print(f\"‚îÇ   ‚îú‚îÄ Mean Non-Contradiction:  {mean_ncp:.3f}\")\n",
    "    print(f\"‚îÇ   ‚îú‚îÄ Std Deviation:           {std_ncp:.3f}\")\n",
    "    print(f\"‚îÇ   ‚îî‚îÄ Min Non-Contradiction:   {std_ncp:.3f}\")\n",
    "    print(f\"‚îî‚îÄ CONFIDENCE: {confidence}\")\n",
    "    print(f\"    ‚îî‚îÄ {interpretation}\")\n",
    "\n",
    "    return {\n",
    "        \"method\": \"Non-Contradiction Probability\",\n",
    "        \"num_responses\": n_responses,\n",
    "        \"mean_ncp\": mean_ncp,\n",
    "        \"std_ncp\": std_ncp,\n",
    "        \"min_ncp\": min_ncp,\n",
    "        \"pair_analyses\": pair_analyses,\n",
    "        \"confidence\": confidence,\n",
    "        \"interpretation\": interpretation,\n",
    "        \"responses\": responses,\n",
    "    }\n",
    "\n",
    "\n",
    "# Don't worry about the red warnings that usually get generated here:\n",
    "print(\"‚úÖ Set up non_contradiction_detection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa89ac0d-b1af-4efa-bbb8-59a8f486a622",
   "metadata": {},
   "source": [
    "### Test Non-Contradiction Detection\n",
    "\n",
    "Let's test this method with the same question to see how it compares:\n",
    "\n",
    "> ‚ÑπÔ∏è Note that you *could* re-use the `semsmim_bedrock_responses` from earlier directly, but we've chosen to generate new responses to keep things a bit more interactive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd8a571-0c28-4c3a-8d84-8f4a927d5e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ncp_bedrock_responses = generate_responses(\n",
    "    claude_models, \"What album was Dua Lipa's 'Hallucinate' featured on?\"\n",
    ")\n",
    "\n",
    "non_contradiction_detection(ncp_bedrock_responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf428be7-c61a-401e-ae49-b504016ae1dd",
   "metadata": {},
   "source": [
    "### Optional: Test with SageMaker Endpoint\n",
    "\n",
    "Again if you've deployed the SageMaker endpoint(s) in lab 0, you can test them here. Otherwise, skip to the next section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5da9be-cac2-4f91-97ce-6688ab81a8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "ncp_sm_responses = generate_responses(\n",
    "    gpt_oss_models,\n",
    "    \"What album was Dua Lipa's 'Hallucinate' featured on?\",\n",
    ")\n",
    "\n",
    "non_contradiction_detection(ncp_sm_responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11f6be0-09ac-4946-ad24-1c75280f6046",
   "metadata": {},
   "source": [
    "## 4. Method 3: Normalized Semantic Negentropy (NSN)\n",
    "(<a href=\"#top\">Go to top</a>)\n",
    "\n",
    "### How It Works\n",
    "\n",
    "Normalized Semantic Negentropy is the most sophisticated method, measuring **semantic uncertainty** through response clustering.\n",
    "\n",
    "**The Process:**\n",
    "\n",
    "Given multiple candidate responses (again in this case we'll use the same model, but vary the temperature):\n",
    "\n",
    "1. **Semantic Clustering**: Group responses that convey the same meaning (using Natural Language Inference model, as we did earlier)\n",
    "2. **Entropy Calculation**: Measure how \"scattered\" the responses are across clusters\n",
    "3. **Confidence Scoring**: Convert entropy to a confidence score (low entropy = high confidence)\n",
    "\n",
    "**Key Principle**: Confident knowledge clusters tightly, while uncertain knowledge fragments across many different meanings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab187128-0a44-4210-bbdc-d05971c7801c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_entropy_detection(\n",
    "    responses: list[str],\n",
    "    nli_model: Pipeline = pipeline(\n",
    "        \"text-classification\",\n",
    "        model=\"microsoft/deberta-base-mnli\",\n",
    "        return_all_scores=True,\n",
    "    ),\n",
    ") -> dict:\n",
    "    \"\"\"Detect hallucinations using Normalized Semantic Negentropy (NSN)\n",
    "\n",
    "    This method clusters responses by meaning, then calculates entropy of the distribution between\n",
    "    those clusters to model uncertainty. Lower entropy indicates higher confidence.\n",
    "\n",
    "    Args:\n",
    "        responses: The multiple alternative responses generated by the model\n",
    "        nli_model: HF Transformers Pipeline-compatible model for Natural Language Inference\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with entropy metrics and confidence assessment\n",
    "    \"\"\"\n",
    "    n_responses = len(responses)\n",
    "    if n_responses < 3:\n",
    "        raise ValueError(\n",
    "            f\"Need at least 3 responses for clustering analysis. Got {n_responses}\"\n",
    "        )\n",
    "    print(\n",
    "        f\"üîç Analyzing normalized semantic negentropy of {n_responses} candidate responses...\"\n",
    "    )\n",
    "\n",
    "    # Step 1: Cluster responses by semantic meaning\n",
    "    print(f\"‚îú‚îÄ Clustering responses by semantic meaning...\")\n",
    "    clusters = []\n",
    "\n",
    "    for i, response in enumerate(responses):\n",
    "        assigned_to_cluster = False\n",
    "\n",
    "        # Try to assign to existing cluster\n",
    "        for cluster in clusters:\n",
    "            # Compare with representative response from cluster\n",
    "            representative = responses[cluster[0]]\n",
    "\n",
    "            # Check for bidirectional entailment (same meaning)\n",
    "            result1 = nli_model(f\"{response} [SEP] {representative}\")\n",
    "            result2 = nli_model(f\"{representative} [SEP] {response}\")\n",
    "\n",
    "            # Extract entailment scores\n",
    "            entail1 = 0.0\n",
    "            entail2 = 0.0\n",
    "\n",
    "            if isinstance(result1, list) and len(result1) > 0:\n",
    "                inner_list1 = result1[0] if isinstance(result1[0], list) else result1\n",
    "                for item in inner_list1:\n",
    "                    if isinstance(item, dict) and item.get(\"label\") == \"ENTAILMENT\":\n",
    "                        entail1 = item.get(\"score\", 0.0)\n",
    "                        break\n",
    "\n",
    "            if isinstance(result2, list) and len(result2) > 0:\n",
    "                inner_list2 = result2[0] if isinstance(result2[0], list) else result2\n",
    "                for item in inner_list2:\n",
    "                    if isinstance(item, dict) and item.get(\"label\") == \"ENTAILMENT\":\n",
    "                        entail2 = item.get(\"score\", 0.0)\n",
    "                        break\n",
    "\n",
    "            # Use geometric mean for mutual entailment\n",
    "            mutual_entailment = np.sqrt(entail1 * entail2)\n",
    "\n",
    "            if mutual_entailment > 0.5:  # Threshold for \"same meaning\"\n",
    "                cluster.append(i)\n",
    "                assigned_to_cluster = True\n",
    "                break\n",
    "\n",
    "        # Create new cluster if no match found\n",
    "        if not assigned_to_cluster:\n",
    "            clusters.append([i])\n",
    "\n",
    "    # Step 4: Display cluster information\n",
    "    print(f\"Found {len(clusters)} semantic clusters:\")\n",
    "    for i, cluster in enumerate(clusters):\n",
    "        print(\n",
    "            \"‚îÇ   ‚îú‚îÄ Cluster {}: Responses {} ({} responses)\".format(\n",
    "                i + 1, [j + 1 for j in cluster], len(cluster)\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Step 5: Calculate semantic entropy\n",
    "    cluster_probabilities = [len(cluster) / len(responses) for cluster in clusters]\n",
    "    semantic_entropy = -sum(p * np.log(p) for p in cluster_probabilities if p > 0)\n",
    "\n",
    "    # Normalize entropy (0 = all same meaning, 1 = all different meanings)\n",
    "    max_entropy = np.log(len(responses))\n",
    "    normalized_entropy = semantic_entropy / max_entropy if max_entropy > 0 else 0\n",
    "    nsn_score = 1 - normalized_entropy  # Convert to confidence score\n",
    "\n",
    "    # Step 6: Calculate additional metrics\n",
    "    fragmentation_ratio = len(clusters) / len(responses)\n",
    "    largest_cluster_size = max(len(cluster) for cluster in clusters)\n",
    "    cluster_dominance = largest_cluster_size / len(responses)\n",
    "\n",
    "    # Step 7: Determine confidence level\n",
    "    if nsn_score > 0.8 and fragmentation_ratio < 0.4:\n",
    "        confidence = \"üü¢ HIGH\"\n",
    "        interpretation = \"Responses cluster tightly - coherent understanding\"\n",
    "    elif nsn_score > 0.6 and fragmentation_ratio < 0.6:\n",
    "        confidence = \"üü° MEDIUM\"\n",
    "        interpretation = \"Moderate clustering - some uncertainty present\"\n",
    "    else:\n",
    "        confidence = \"üî¥ LOW\"\n",
    "        interpretation = \"Highly fragmented responses - model is uncertain\"\n",
    "\n",
    "    # Display results\n",
    "    print(f\"‚îú‚îÄ METRICS:\")\n",
    "    print(f\"‚îÇ   ‚îú‚îÄ Semantic Entropy:     {semantic_entropy:.3f}\")\n",
    "    print(f\"‚îÇ   ‚îú‚îÄ NSN Score:            {nsn_score:.3f}\")\n",
    "    print(f\"‚îÇ   ‚îú‚îÄ Fragmentation Ratio:  {fragmentation_ratio:.3f}\")\n",
    "    print(f\"‚îÇ   ‚îî‚îÄ Cluster Dominance:    {cluster_dominance:.3f}\")\n",
    "    print(f\"‚îî‚îÄ CONFIDENCE: {confidence}\")\n",
    "    print(f\"    ‚îî‚îÄ {interpretation}\")\n",
    "\n",
    "    return {\n",
    "        \"method\": \"Normalized Semantic Negentropy\",\n",
    "        \"num_responses\": n_responses,\n",
    "        \"num_clusters\": len(clusters),\n",
    "        \"nsn_score\": nsn_score,\n",
    "        \"semantic_entropy\": semantic_entropy,\n",
    "        \"fragmentation_ratio\": fragmentation_ratio,\n",
    "        \"cluster_dominance\": cluster_dominance,\n",
    "        \"clusters\": clusters,\n",
    "        \"confidence\": confidence,\n",
    "        \"interpretation\": interpretation,\n",
    "        \"responses\": responses,\n",
    "    }\n",
    "\n",
    "\n",
    "# Don't worry about the red warnings that usually get generated here:\n",
    "print(\"‚úÖ Set up semantic_entropy_detection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f66ac00-803e-4650-b859-806bb76e1ef7",
   "metadata": {},
   "source": [
    "### Test Semantic Entropy Detection\n",
    "\n",
    "Let's test this method too with the same question to see how it compares:\n",
    "\n",
    "> ‚ÑπÔ∏è Note that you *could* re-use the `semsmim_bedrock_responses` from earlier directly, but we've chosen to generate new responses to keep things a bit more interactive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86026fe4-3d26-40b1-b4b9-faf9c3f1b801",
   "metadata": {},
   "outputs": [],
   "source": [
    "nsn_bedrock_responses = generate_responses(\n",
    "    claude_models, \"What album was Dua Lipa's 'Hallucinate' featured on?\"\n",
    ")\n",
    "\n",
    "semantic_entropy_detection(nsn_bedrock_responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cbebe4a-d8fd-44f2-940b-940a60799ced",
   "metadata": {},
   "source": [
    "### Optional: Test with SageMaker Endpoint\n",
    "\n",
    "Again if you've deployed the SageMaker endpoint(s) in lab 0, you can test them here. Otherwise, skip to the next section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2decd2e1-ce5c-4b4f-8658-eec9184dd62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nsn_sm_responses = generate_responses(\n",
    "    gpt_oss_models,\n",
    "    \"What album was Dua Lipa's 'Hallucinate' featured on?\",\n",
    ")\n",
    "\n",
    "semantic_entropy_detection(nsn_sm_responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbebbe7-f35e-4b4a-ae16-28c29de5ae25",
   "metadata": {},
   "source": [
    "## 5. Comparative Analysis Across Models\n",
    "(<a href=\"#top\">Go to top</a>)\n",
    "\n",
    "Now we've seen a few individual examples, let's compare the results across different detection methods, models, and example questions.\n",
    "\n",
    "Run the cell below to analyze a small batch of questions with all 3 detection methods, across both our models Claude and GPT-OSS.\n",
    "\n",
    "‚è∞ This may take a couple of minutes to complete, and will output the full detailed logs for each combination. Carry on to the following cell to see the overall summary table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ba5366-c303-4134-8f2a-a370766e0ac0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "test_questions = [\n",
    "    # Should be pretty straightforward:\n",
    "    \"What's the square root of 16? Reply only with a numeric answer.\",\n",
    "    # Mixed performance with these models:\n",
    "    \"What album was Dua Lipa's 'Hallucinate' featured on?\",\n",
    "    # Future knowledge these models are unlikely to know:\n",
    "    \"What date does AWS re:Invent 2025 start?\",\n",
    "    # *Should* be highly random in response - so low confidence:\n",
    "    \"Think of a random animal. What is it?\",\n",
    "]\n",
    "model_groups = {\"claude\": claude_models, \"gpt_oss\": gpt_oss_models}\n",
    "methods = {\n",
    "    \"SSD\": semantic_similarity_detection,\n",
    "    \"NCP\": non_contradiction_detection,\n",
    "    \"NSN\": semantic_entropy_detection,\n",
    "}\n",
    "\n",
    "\n",
    "def compare_models_and_metrics(\n",
    "    questions: list[str],\n",
    "    model_groups: Dict[str, list],\n",
    "    methods: Dict[str, Callable],\n",
    ") -> dict:\n",
    "    \"\"\"Calculate confidence across a range of questions, models, and metrics at once\"\"\"\n",
    "    results_flat = []\n",
    "    for q in questions:\n",
    "        for model_name, models in model_groups.items():\n",
    "            try:\n",
    "                responses = generate_responses(models, q)\n",
    "            except Exception as emodel:\n",
    "                logger.error(\"Failed get responses from %s: %s\", model_name, emodel)\n",
    "                for method_name in methods.keys():\n",
    "                    results_flat.append(\n",
    "                        {\n",
    "                            \"question\": q,\n",
    "                            \"model\": model_name,\n",
    "                            \"method\": method_name,\n",
    "                            \"confidence\": \"‚ö†Ô∏è MODEL ERR\",\n",
    "                        }\n",
    "                    )\n",
    "                continue\n",
    "            for method_name, method in methods.items():\n",
    "                try:\n",
    "                    result = method(responses)\n",
    "                    confidence = result[\"confidence\"]\n",
    "                except Exception as escore:\n",
    "                    logger.error(\"Failed to calculate %s: %s\", method_name, escore)\n",
    "                    confidence = \"‚ö†Ô∏è SCORE ERR\"\n",
    "                results_flat.append(\n",
    "                    {\n",
    "                        \"question\": q,\n",
    "                        \"model\": model_name,\n",
    "                        \"method\": method_name,\n",
    "                        \"confidence\": confidence,\n",
    "                    }\n",
    "                )\n",
    "    results_df = pd.DataFrame(results_flat)\n",
    "    return results_df\n",
    "\n",
    "\n",
    "comparison_df = compare_models_and_metrics(test_questions, model_groups, methods)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f7642a-83a0-40c1-a937-50cf1191c2ab",
   "metadata": {},
   "source": [
    "Once the analyses are complete, we can view the summary results as a table as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20b7b79-3be5-4196-932f-832270a49808",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_df.pivot(index=\"question\", columns=[\"model\", \"method\"]).style.set_properties(\n",
    "    **{\"text-align\": \"left\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05604014-4f78-44ed-8808-875d8d155584",
   "metadata": {},
   "source": [
    "> **Note:** If you see '‚ö†Ô∏è SCORE ERR' or '‚ö†Ô∏è MODEL ERR' entries in your table, you might've run in to throttling errors in our workshop environment. Consider reducing the number of questions, and/or waiting a little before trying again.\n",
    "\n",
    "What do you see in your results?\n",
    "- Did both models and all the detection methods consistently score \"HIGH\" confidence for the \"easy\" question(s) and \"LOW\" confidence for the purposefully random ones?\n",
    "- In areas where the different detection *methods* gave a different score, which one would you agree with based on the detailed logs for that question+model above?\n",
    "- When the different *models* are scored differently, did this correlate well to one model giving more uncertain or varied answers?\n",
    "\n",
    "A screenshot of an example output is shown below - although yours might differ a little due to randomness in the generations:\n",
    "\n",
    "![](img/lab2-sample-comparison-table.png \"Screenshot of a pivot table. The questions asked run down the left. The model (claude vs gpt_oss) and detection methods (SSD, NCP, NSN) are nested labels along the top. Each point lists a confidence score (HIGH, MEDIUM, or LOW). The 'Think of a random animal' question scored LOW confidence across all models and detection methods, while the 'square root of 16' question was consistently HIGH. Other questions showed more variable results.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88211718-0f51-455a-8190-cf4301716579",
   "metadata": {},
   "source": [
    "## 6. Building Production-Ready Agents with Semantic Similarity\n",
    "(<a href=\"#top\">Go to top</a>)\n",
    "\n",
    "So far we've applied these techniques on basic LLM calls. But how can they fit in more complex AI applications like agents? In this section, we'll integrate real-time, response-level hallucination detection into an example agent using the Strands Agents SDK.\n",
    "\n",
    "### What are Strands Agents?\n",
    "\n",
    "[Strands](https://github.com/strands-agents/sdk-python) is an open-source Python framework for building agentic AI applications. It provides:\n",
    "- **Unified Model Interface**: Work with multiple LLM providers (including Bedrock, SageMaker, and others outside AWS)\n",
    "- **Tool Integration**: Connect agents to external tools and APIs\n",
    "- **Observability**: Built-in tracing and monitoring via OpenTelemetry\n",
    "- **Extensibility**: Easy to add custom behaviors like hallucination detection\n",
    "\n",
    "### Our Approach: Semantic Similarity as a Model-Level Guardrail\n",
    "\n",
    "We'll build an agent that:\n",
    "1. **Generates multiple candidate responses** every time it calls its core LLM\n",
    "2. **Analyzes semantic similarity** between the candidate responses\n",
    "3. **Logs the analysis results** to standard tracing & observability tools\n",
    "4. **Intervenes if inconsistency is detected** (potential hallucination)\n",
    "5. **Returns the most reliable response** or raises an alert\n",
    "\n",
    "#### Why This Matters:\n",
    "\n",
    "Agents; which typically go through multiple rounds of LLM inference and external tool calls before returning a final answer to the user; are a more complex environment for implementing guardrails than a single LLM call.\n",
    "\n",
    "Some tool calls, like internal APIs, may be **irreversible, or at least complicated to undo**: So implementing this guardrail at the level of each LLM call (as we've done here) is more generalizable than trying to invoke the *whole agent* several times in parallel and analysing the final responses. However, that approach could also be viable for agents that **only** have information-fetching, read-only tools."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311bbff2",
   "metadata": {},
   "source": [
    "### Step 1: Prepare environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fqeayw6u8qh",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python Built-Ins:\n",
    "import base64\n",
    "import os\n",
    "\n",
    "# External Dependencies:\n",
    "from dotenv import load_dotenv\n",
    "from strands import Agent\n",
    "from strands.models.bedrock import BedrockModel\n",
    "from strands.telemetry import StrandsTelemetry\n",
    "\n",
    "# For our custom parallel model implementation:\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "from hallucination_utils.strands_models.parallel import (\n",
    "    ParallelModelWithConsolidation,\n",
    "    ModelResponse,\n",
    "    TraceAttributes,\n",
    ")\n",
    "\n",
    "print(\"‚úì Imports successful!\")\n",
    "print(f\"‚úì Strands Agent module: {Agent.__module__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebobguidyic",
   "metadata": {},
   "source": [
    "### Optional: Configure Langfuse for Observability\n",
    "\n",
    "[Langfuse](https://langfuse.com/) provides production-grade observability for LLM applications. It integrates seamlessly with Strands via OpenTelemetry to track:\n",
    "- Individual LLM calls and their responses\n",
    "- Guardrail interventions and scores\n",
    "- Token usage and latency metrics\n",
    "- End-to-end conversation traces\n",
    "\n",
    "The Langfuse configuration will be handled automatically by the `set_up_notebook_langfuse()` function below.\n",
    "\n",
    "**Note**: You should have already obtained your Langfuse credentials following the **Enable Langfuse tracing** instruction in the Getting Started section. If you haven't set up Langfuse yet, you'll be prompted for your credentials when you run the cell below.\n",
    "\n",
    "If you prefer not to use Langfuse, you can skip this setup. The agent will work fine without it - you just won't see traces in the Langfuse dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zf7wy8wj5cn",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import tracing utilities\n",
    "from hallucination_utils.tracing import set_up_notebook_langfuse\n",
    "\n",
    "# Set up Langfuse (will prompt for credentials if not already configured)\n",
    "set_up_notebook_langfuse()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "u2j4du1l6h",
   "metadata": {},
   "source": [
    "### Step 2: Create Base Model\n",
    "\n",
    "Strands provides integrations to a [wide range](https://strandsagents.com/latest/documentation/docs/api-reference/models/) of Foundation Model providers, and we'll implement the guardrail *on top of* this abstraction so it works for any model Strands supports.\n",
    "\n",
    "In this case, we'll be wrapping over the same Claude model on Amazon Bedrock:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "apd0fvw6ixt",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the base Bedrock model\n",
    "base_model = BedrockModel(model_id=\"global.anthropic.claude-haiku-4-5-20251001-v1:0\")\n",
    "\n",
    "print(\"‚úì Base model initialized:\", base_model.get_config()[\"model_id\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "s1syy9y8e9",
   "metadata": {},
   "source": [
    "### Step 3: Build Semantic Similarity Model Wrapper\n",
    "\n",
    "Now we'll set up a custom model wrapper that:\n",
    "1. **Calls the base model multiple times in parallel** (3x in this example)\n",
    "2. **Computes semantic similarity** between all candidate responses\n",
    "3. **Consolidates to the best response** or raises an alert if inconsistency is detected\n",
    "\n",
    "Most of the complexity is handled by the reusable `ParallelModelWithConsolidation` utility we've provided in [hallucination_utils/strands_models/parallel.py](hallucination_utils/strands_models/parallel.py), which handles:\n",
    "- Parallel execution of multiple model calls\n",
    "- Stream processing and response collection (since Strands is stream-based)\n",
    "- Telemetry integration for observability\n",
    "\n",
    "Here in the notebook just need to implement the `consolidate()` method with our semantic similarity logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9rs9zrf8wwd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class SemanticSimilarityModel(ParallelModelWithConsolidation):\n",
    "    \"\"\"\n",
    "    Custom model wrapper that detects hallucinations via semantic similarity analysis.\n",
    "\n",
    "    This model:\n",
    "    - Generates N candidate responses in parallel\n",
    "    - Computes pairwise semantic similarity between all candidates\n",
    "    - Intervenes if mean similarity falls below threshold\n",
    "    - Returns the first response if all candidates are consistent\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, base_model, num_candidates: int = 3, similarity_threshold: float = 0.75\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            base_model: The Strands model to wrap (e.g., BedrockModel)\n",
    "            num_candidates: Number of parallel responses to generate\n",
    "            similarity_threshold: Minimum mean similarity score (0-1) to pass\n",
    "        \"\"\"\n",
    "        # Create list of models (repeat the same model N times)\n",
    "        super().__init__([base_model] * num_candidates)\n",
    "        self.similarity_threshold = similarity_threshold\n",
    "        self.encoder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "    def consolidate(\n",
    "        self, alternatives: list[ModelResponse]\n",
    "    ) -> ModelResponse | tuple[ModelResponse, TraceAttributes]:\n",
    "        \"\"\"\n",
    "        Analyze semantic similarity between candidate responses.\n",
    "\n",
    "        Returns:\n",
    "            Consolidated response with telemetry attributes\n",
    "        \"\"\"\n",
    "        # Extract response texts from alternatives\n",
    "        response_texts = [\n",
    "            alt.message[\"content\"][0][\"text\"]\n",
    "            for alt in alternatives\n",
    "            if \"content\" in alt.message and len(alt.message[\"content\"]) > 0\n",
    "        ]\n",
    "\n",
    "        if len(response_texts) < 2:\n",
    "            # Not enough responses to compare - return first one\n",
    "            return (\n",
    "                ModelResponse.from_alternatives(\n",
    "                    alternatives=alternatives, final_message=alternatives[0].message\n",
    "                ),\n",
    "                {\n",
    "                    \"gen_ai.guardrail.semantic_similarity.intervened\": False,\n",
    "                    \"gen_ai.guardrail.semantic_similarity.status\": \"insufficient_samples\",\n",
    "                },\n",
    "            )\n",
    "\n",
    "        # Step 1: Encode all responses to embeddings\n",
    "        embeddings = self.encoder.encode(response_texts)\n",
    "\n",
    "        # Step 2: Calculate pairwise similarities\n",
    "        similarity_matrix = cosine_similarity(embeddings)\n",
    "\n",
    "        # Step 3: Get upper triangle (excluding diagonal)\n",
    "        mask = np.triu(np.ones_like(similarity_matrix, dtype=bool), k=1)\n",
    "        similarities = similarity_matrix[mask]\n",
    "\n",
    "        # Step 4: Calculate metrics\n",
    "        mean_similarity = float(np.mean(similarities))\n",
    "        min_similarity = float(np.min(similarities))\n",
    "        max_similarity = float(np.max(similarities))\n",
    "        std_similarity = float(np.std(similarities))\n",
    "\n",
    "        # Step 5: Determine if we should intervene\n",
    "        intervened = mean_similarity < self.similarity_threshold\n",
    "\n",
    "        # Step 6: Prepare telemetry attributes\n",
    "        trace_attrs = {\n",
    "            \"gen_ai.guardrail.semantic_similarity.intervened\": intervened,\n",
    "            \"gen_ai.guardrail.semantic_similarity.mean_score\": mean_similarity,\n",
    "            \"gen_ai.guardrail.semantic_similarity.min_score\": min_similarity,\n",
    "            \"gen_ai.guardrail.semantic_similarity.max_score\": max_similarity,\n",
    "            \"gen_ai.guardrail.semantic_similarity.std_score\": std_similarity,\n",
    "            \"gen_ai.guardrail.semantic_similarity.threshold\": self.similarity_threshold,\n",
    "            \"gen_ai.guardrail.semantic_similarity.num_candidates\": len(alternatives),\n",
    "        }\n",
    "\n",
    "        if intervened:\n",
    "            # Hallucination detected - you could raise an error, return a warning, etc.\n",
    "            trace_attrs[\"gen_ai.guardrail.semantic_similarity.status\"] = (\n",
    "                \"INCONSISTENT_RESPONSES\"\n",
    "            )\n",
    "\n",
    "            print(\n",
    "                f\"\\n‚ö†Ô∏è Semantic Similarity Guardrail: Inconsistent responses detected!\"\n",
    "            )\n",
    "            print(\n",
    "                \"‚îî‚îÄ‚îÄ Mean similarity: {:.3f} (threshold: {})\\n\".format(\n",
    "                    mean_similarity, self.similarity_threshold\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            trace_attrs[\"gen_ai.guardrail.semantic_similarity.status\"] = (\n",
    "                \"CONSISTENT_RESPONSES\"\n",
    "            )\n",
    "            print(f\"\\n‚úì Semantic Similarity Guardrail: Responses are consistent\")\n",
    "            print(f\"‚îî‚îÄ‚îÄ Mean similarity: {mean_similarity:.3f}\\n\")\n",
    "\n",
    "        # Return first response as the consolidated output\n",
    "        # (In production, you might choose the response with highest avg similarity to others)\n",
    "        return (\n",
    "            ModelResponse.from_alternatives(\n",
    "                alternatives=alternatives, final_message=alternatives[0].message\n",
    "            ),\n",
    "            trace_attrs,\n",
    "        )\n",
    "\n",
    "\n",
    "print(\"‚úì SemanticSimilarityModel class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pa0oithx3e",
   "metadata": {},
   "source": [
    "### Step 4: Instantiate the Semantic Similarity Model\n",
    "\n",
    "Now let's create an instance of our guardrail-enabled model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kxk4pii0h3h",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create semantic similarity model with 3 parallel candidates\n",
    "semsim_model = SemanticSimilarityModel(\n",
    "    base_model=base_model,\n",
    "    num_candidates=3,\n",
    "    similarity_threshold=0.75,  # Adjust based on your requirements\n",
    ")\n",
    "\n",
    "print(\"‚úì Semantic Similarity Model created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "x2dv95kzveq",
   "metadata": {},
   "source": [
    "### Step 5: Create and test the Strands Agent\n",
    "\n",
    "Finally, let's create a Strands Agent that uses our semantic similarity model: So every underlying LLM call is repeated multiple times and checked for consistent outputs.\n",
    "\n",
    "Note that Strands Agents **remember** interactions (in-memory) by default, so you should re-create the agent when wanting to reset your conversation session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dhala7c95o7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create agent with our custom model\n",
    "agent = Agent(model=semsim_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67bcbcbb-b6c8-467e-b523-fa7df4815be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask a simple factual question to start with:\n",
    "input_message = \"What is the capital of France?\"\n",
    "print(f\"üòä === User Message: ===\\n{input_message}\\n========================\")\n",
    "result = agent(input_message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2hbildicrvl",
   "metadata": {},
   "source": [
    "For testing scenarios with a fresh session each time, we can wrap the agent create and invoke together in a function as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0w7sufodah",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_and_invoke_semsim_agent(prompt: str):\n",
    "    agent = Agent(model=semsim_model)\n",
    "    print(f\"üòä === User Message: ===\\n{input_message}\\n========================\")\n",
    "    result = agent(prompt)\n",
    "    return result\n",
    "\n",
    "\n",
    "result = create_and_invoke_semsim_agent(\n",
    "    \"What are the main causes of climate change?\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ee9744-fba5-457d-8b29-4c81748c5f27",
   "metadata": {},
   "source": [
    "If you enabled Langfuse tracing above, you can also explore the generated traces in the Langfuse UI to see more details.\n",
    "\n",
    "As shown in the screenshot below, the Langfuse trace should include an extra guardrail 'span' for each place the semantic similarity guardrail was invoked during response generation. The guardrail span should include details like the multiple draft model responses, and the output metrics from the check:\n",
    "\n",
    "![](./img/langfuse-lab2.png \"Screenshot of Langfuse UI showing a trace with a 'semantic_similarity' span indicating where the custom guardrail was called.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7u1z16wggou",
   "metadata": {},
   "source": [
    "### Production Deployment Considerations\n",
    "\n",
    "When deploying this pattern to production, consider:\n",
    "\n",
    "#### 1. Performance vs. Accuracy Tradeoffs\n",
    "- **Latency**: Running 3 parallel model calls increases latency slightly, but reduces wall-clock time vs. sequential calls\n",
    "- **Cost**: 3x model invocations = 3x cost per query\n",
    "- **Optimization**: Use caching, reduce `num_candidates` for non-critical queries, or use faster/cheaper models\n",
    "\n",
    "#### 2. Threshold Tuning\n",
    "- **similarity_threshold=0.75** is a starting point - tune based on your domain\n",
    "- Lower threshold (e.g., 0.6) = fewer false positives, but might miss hallucinations\n",
    "- Higher threshold (e.g., 0.9) = stricter checking, but might flag legitimate variation\n",
    "- Use A/B testing and human evaluation to find optimal threshold\n",
    "\n",
    "#### 3. Error Handling\n",
    "- Current implementation prints warnings but returns a response anyway\n",
    "- In production, you might:\n",
    "  - Raise an exception to force human review\n",
    "  - Return a confidence score to the user\n",
    "  - Fallback to a RAG system or web search\n",
    "  - Log to monitoring system for offline analysis\n",
    "\n",
    "#### 4. Observability\n",
    "- Enable Langfuse or another observability platform\n",
    "- Track metrics: `mean_similarity`, `intervention_rate`, `user_satisfaction`\n",
    "- Set up alerts when intervention rate exceeds normal baselines\n",
    "- Review flagged responses to improve prompts and thresholds\n",
    "\n",
    "#### 5. Advanced Consolidation Strategies\n",
    "- Current implementation returns the first response\n",
    "- Alternatives:\n",
    "  - Return the response with **highest average similarity** to others\n",
    "  - Use **majority voting** for classification tasks\n",
    "  - **Blend responses** for creative tasks\n",
    "  - Apply **secondary checks** (e.g., NLI contradiction detection)\n",
    "\n",
    "#### 6. Multi-Model Ensembles\n",
    "- Instead of calling the same model 3x, call different models:\n",
    "  ```python\n",
    "  models = [\n",
    "      BedrockModel(\"claude-haiku-4-5\"),\n",
    "      BedrockModel(\"claude-3-5-sonnet\"),\n",
    "      # Could even mix SageMaker models, etc.\n",
    "  ]\n",
    "  ensemble_model = SemanticSimilarityModel(models)\n",
    "  ```\n",
    "- Provides diversity and can catch model-specific hallucinations\n",
    "\n",
    "#### 7. Integration with Other Guardrails\n",
    "- Combine with **Bedrock Guardrails** for content filtering\n",
    "- Add **contextual grounding** checks if using RAG\n",
    "- Layer with **Token Probability Level methods** (perplexity, entropy) for more robust detection\n",
    "\n",
    "<br/>\n",
    "\n",
    "> **Key Considerations**\n",
    ">\n",
    "> These methods have higher latency and computational cost due to multiple model generations and additional processing. However, Section 6 demonstrates how to mitigate this through parallel execution and production optimization strategies. These methods are valuable for high-stakes applications where accuracy is critical, and can be combined with faster methods for different use cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e11a07-f94a-454d-9d2c-c530c4acbe16",
   "metadata": {},
   "source": [
    "## 7. Conclusion and Best Practices\n",
    "(<a href=\"#top\">Go to top</a>)\n",
    "\n",
    "### What We've Learned\n",
    "\n",
    "In this notebook, we've explored three powerful Response Level techniques for detecting LLM hallucinations:\n",
    "\n",
    "1. **Semantic Similarity Detection** - Fast and effective for detecting response consistency\n",
    "2. **Non-Contradiction Probability** - Excellent for identifying logical inconsistencies  \n",
    "3. **Normalized Semantic Negentropy** - Most sophisticated, measuring semantic uncertainty\n",
    "<br>\n",
    "\n",
    "| Method | What It Detects | When to Use (High-Level Context) | Why It Works |\n",
    "|--------|----------------|----------------------------------|--------------|\n",
    "| **Semantic Similarity Detection** | Inconsistent knowledge across multiple generations | ‚Ä¢ **Content verification** where consistency matters<br>‚Ä¢ **Knowledge validation** with available compute budget<br>‚Ä¢ **Quick consistency checks** for important claims | True knowledge should be expressed the same way repeatedly - if answers vary wildly, the model isn't really \"sure\" |\n",
    "| **Non-Contradiction Probability** | Logical inconsistencies between responses | ‚Ä¢ **Factual content** where contradictions are unacceptable<br>‚Ä¢ **Research applications** requiring logical coherence<br>‚Ä¢ **Decision support** systems with clear right/wrong answers | Real facts don't contradict themselves - if multiple responses conflict, at least one must be wrong |\n",
    "| **Normalized Semantic Negentropy** | Semantic uncertainty through response clustering | ‚Ä¢ **High-precision applications** requiring confidence scores<br>‚Ä¢ **Research environments** with computational resources<br>‚Ä¢ **Advanced quality control** for critical outputs | Confident knowledge produces similar responses that cluster together - scattered responses indicate uncertainty |\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- **No single method is perfect** - Different techniques catch different types of hallucinations\n",
    "- **Model quality varies significantly** - Some models are much more consistent than others\n",
    "- **Question type matters** - Factual questions and creative prompts require different approaches\n",
    "- **Confidence thresholds are domain-specific** - Adjust based on your use case criticality\n",
    "\n",
    "### Best Practices for Production\n",
    "\n",
    "1. **Use Multiple Methods**: Combine techniques for more robust detection\n",
    "2. **Set Appropriate Thresholds**: Adjust confidence levels based on domain requirements\n",
    "3. **Consider Computational Cost**: Balance accuracy with response time needs\n",
    "4. **Monitor and Adapt**: Continuously evaluate and update detection parameters\n",
    "5. **Human-in-the-Loop**: Always have escalation paths for uncertain cases\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- **Integrate with existing systems**: Add hallucination detection to your AI pipelines\n",
    "- **Build domain-specific datasets**: Create evaluation sets for your specific use cases\n",
    "- **Experiment with ensemble methods**: Combine multiple models and detection techniques\n",
    "- **Explore fine-tuning**: Train models to be more consistent in your domain\n",
    "\n",
    "### Additional Resources\n",
    "\n",
    "- [AWS Bedrock Documentation](https://docs.aws.amazon.com/bedrock/)\n",
    "- [Sentence Transformers Documentation](https://www.sbert.net/)\n",
    "- [HuggingFace Transformers Guide](https://huggingface.co/docs/transformers)\n",
    "\n",
    "---\n",
    "\n",
    "**Remember**: The goal isn't to eliminate all uncertainty, but to **quantify and manage it** appropriately for your specific use cases.\n",
    "\n",
    "Happy detecting!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810acda7-1a6b-4d55-851c-0e29ebe73e40",
   "metadata": {},
   "source": [
    "## 8. (Optional) Challenge Exercises\n",
    "(<a href=\"#top\">Go to top</a>)\n",
    "\n",
    "<div style=\"border: 4px solid coral; text-align: left; margin: auto; padding: 20px;\">\n",
    "    <h3>üéØ Try These Exercises on your own time!</h3>\n",
    "    <p><b>Complete these challenges to deepen your understanding of hallucination detection:</b></p>\n",
    "    <h4>Beginner Challenges:</h4>\n",
    "    <ol>\n",
    "        <li><b>Different Question Types:</b> Test the methods with various question types:\n",
    "            <ul>\n",
    "                <li>Historical facts: \"When did World War II end?\"</li>\n",
    "                <li>Scientific concepts: \"What is photosynthesis?\"</li>\n",
    "                <li>Fictional scenarios: \"What is Sherlock Holmes' favorite tea?\"</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li><b>Model Comparison:</b> Compare all available models on the same question</li>\n",
    "        <li><b>Parameter Tuning:</b> Experiment with different temperature values and sample sizes</li>\n",
    "    </ol>\n",
    "    <h4>Intermediate Challenges:</h4>\n",
    "    <ol start=\"4\">\n",
    "        <li><b>Custom Thresholds:</b> Modify the confidence thresholds in each method</li>\n",
    "        <li><b>Ambiguous Questions:</b> Test with deliberately ambiguous prompts</li>\n",
    "        <li><b>Method Combination:</b> Create a combined score using all three methods</li>\n",
    "    </ol>\n",
    "    <h4>Advanced Challenges:</h4>\n",
    "    <ol start=\"7\">\n",
    "        <li><b>Domain-Specific Testing:</b> Test on medical, legal, or technical questions</li>\n",
    "        <li><b>Multi-Language Support:</b> Adapt methods for non-English responses</li>\n",
    "        <li><b>Real-Time Application:</b> Build a web interface for live hallucination detection</li>\n",
    "    </ol>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
