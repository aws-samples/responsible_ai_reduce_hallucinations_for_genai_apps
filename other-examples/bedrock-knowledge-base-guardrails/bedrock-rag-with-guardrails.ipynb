{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc84eb37-3b34-4506-8ebc-c70f28166077",
   "metadata": {},
   "source": [
    "# Amazon Bedrock Knowledge Bases with Guardrails to reduce hallucinations\n",
    "\n",
    "> **As featured in AWS re:Invent 2024 session AIM325**\n",
    "\n",
    "In this notebook, you will:\n",
    "1. Explore some LLM inference parameters that influence (and can help mitigate) model hallucinations.\n",
    "2. Understand how Retrieval Augmented Generation (RAG) helps reduce hallucinations, and can be implemented with [Amazon Bedrock Knowledge Bases](https://aws.amazon.com/bedrock/knowledge-bases/).\n",
    "3. Add real-time contextual grounding checks with [Amazon Bedrock Guardrails](https://aws.amazon.com/bedrock/guardrails/), to detect and intervene when a RAG pipeline could still be generating hallucinatory or irrelevant answers.\n",
    "4. Learn how the Open Source [Ragas](https://docs.ragas.io/en/stable/) framework can help evaluate RAG pipelines with different relevant metrics.\n",
    "\n",
    "**About the dataset:** To demonstrate knowledge search, this workshop will use a simple example corpus including only (an outdated version of) the [Amazon Bedrock User Guide](https://docs.aws.amazon.com/pdfs/bedrock/latest/userguide/bedrock-ug.pdf) in PDF format. A set of example questions and expected answers based on this document are provided in [data/bedrock-user-guide-test.csv](data/bedrock-user-guide-test.csv)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc006a3-48d5-40a5-9eb8-ea9bcd3d85e9",
   "metadata": {},
   "source": [
    "## Prerequisites and setup\n",
    "\n",
    "**Permissions:**\n",
    "- *This notebook* (i.e. your AWS CLI credentials, or your SageMaker Execution Role if running in SageMaker AI) needs access to invoke models, query Knowledge Bases, and create+invoke Guardrails in Amazon Bedrock.\n",
    "- *You* (i.e. your AWS Console user) will need access to deploy an OpenSearch Serverless-backed Bedrock Knowledge Base - including creating IAM Roles, which we'll set up via AWS CloudFormation\n",
    "\n",
    "**Kernel and libraries:**\n",
    "\n",
    "This notebook uses the same libraries as defined in the top-level [pyproject.toml](../../pyproject.toml) in this sample repository. Run the cells below to install those and then restart your notebook kernel, if you don't have them already:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba631f57-61f6-422f-8a42-472cb6046eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -e ../.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a617e1-d0ef-44d3-bb56-c1f11ee326dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# restart kernel\n",
    "from IPython.core.display import HTML\n",
    "\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a0a7fb",
   "metadata": {},
   "source": [
    "With the relevant libraries installed, we're ready to load them up and connect to the AWS services that'll be used in the rest of the notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4545e3ba-8b68-4251-9177-8a506d1dae04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python Built-Ins:\n",
    "import json\n",
    "import logging\n",
    "import uuid\n",
    "import warnings\n",
    "\n",
    "# External Dependencies:\n",
    "import boto3\n",
    "from botocore.config import Config as BotoConfig\n",
    "import pandas as pd\n",
    "import pprint\n",
    "\n",
    "# Connect to AWS Services:\n",
    "boto_session = boto3.Session()  # Can optionally override 'region_name' here if wanted\n",
    "bedrock_config = BotoConfig(\n",
    "    # Override default retry & timeout config for (maybe long-running/throttled) FM invocations\n",
    "    retries={\"max_attempts\": 5, \"mode\": \"adaptive\"},\n",
    "    read_timeout=1000,\n",
    "    connect_timeout=1000,\n",
    ")\n",
    "bedrock_client = boto_session.client(\"bedrock\")\n",
    "bedrock_runtime = boto_session.client(\"bedrock-runtime\", config=bedrock_config)\n",
    "bedrock_agent_client = boto_session.client(\"bedrock-agent\")\n",
    "bedrock_agent_runtime = boto_session.client(\n",
    "    \"bedrock-agent-runtime\", config=bedrock_config\n",
    ")\n",
    "\n",
    "# Setup for logging/printing outputs:\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s {%(filename)s:%(lineno)d} %(levelname)s: %(message)s\",\n",
    "    level=logging.INFO,\n",
    ")\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "pp = pprint.PrettyPrinter(indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6edb7c-dfa1-4460-a365-e3f287951ddb",
   "metadata": {},
   "source": [
    "This notebook has been tested with Anthropic Claude 3 Sonnet for text generation and Amazon Titan Text Embeddings v2 for embeddings, as configured below.\n",
    "\n",
    "It should be possible to run with other models instead if needed (for e.g. due to regional availability or updates over time), but this may impact the observed behaviour and metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34d99ca-e263-44e3-9e35-685a7a8f9859",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embedding_model_id = \"amazon.titan-embed-text-v2:0\"\n",
    "llm_model_id = \"anthropic.claude-3-sonnet-20240229-v1:0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a44d408-1086-48fc-a329-f757fed9b02a",
   "metadata": {},
   "source": [
    "## 1. Basic LLM invocation on Bedrock and hallucination risks\n",
    "\n",
    "To get started, let's test out the basics of asking a question to a text generation Foundation Model on Amazon Bedrock. We'll set up a utility function for this to simplify the code later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340537e1-ef81-4aef-a767-f0df5e1fef45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_message_claude(\n",
    "    query,\n",
    "    system_prompt=\"\",\n",
    "    max_tokens=1000,\n",
    "    model_id=llm_model_id,\n",
    "    temperature=1.0,\n",
    "    top_p=0.999,\n",
    "    top_k=250,\n",
    "):\n",
    "    \"\"\"Utility to simplify invoking Claude with a text prompt and getting a text response\"\"\"\n",
    "    user_message = {\"role\": \"user\", \"content\": query}\n",
    "    messages = [user_message]\n",
    "    body = json.dumps(\n",
    "        {\n",
    "            \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "            \"max_tokens\": max_tokens,\n",
    "            \"system\": system_prompt,\n",
    "            \"messages\": messages,\n",
    "            \"temperature\": temperature,\n",
    "            \"top_p\": top_p,\n",
    "            \"top_k\": top_k,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    response = bedrock_runtime.invoke_model(body=body, modelId=model_id)\n",
    "    response_body = json.loads(response[\"body\"].read())\n",
    "    return response_body[\"content\"][0][\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca8a8b4-4bd2-453e-9796-17b2a9f25234",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "In this example we're relying **only** on whatever information the model remembers from its initial training, and whatever information is available in the prompt: there's no web search or other knowledge lookup enabled.\n",
    "\n",
    "...So if we ask a specific, factual question on a specialized topic, there's a high chance the model may \"hallucinate\" a confident-sounding but factually incorrect answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd47d4b-8455-4fcf-ae91-874ab2f7c0e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "query = \"How does Amazon Bedrock Guardrails work?\"\n",
    "\n",
    "response = generate_message_claude(query)\n",
    "pp.pprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5637a9c5-0739-425d-a7bf-21b2083ee271",
   "metadata": {},
   "source": [
    "<div style=\"border: 4px solid coral; text-align: left; padding: 20px;\">\n",
    "    <strong>Note: If the LLM call to Bedrock did not work, enable model access on Amazon Bedrock console</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621f0fa6-9637-4c65-bad6-0e1eacc692e9",
   "metadata": {},
   "source": [
    "### 1.1 Apply a system prompt\n",
    "\n",
    "Perhaps the simplest way to steer the model away from this behaviour is to just give guidance in the system prompt - encouraging the model to consider whether it's confident and decline to answer if not.\n",
    "\n",
    "This approach isn't perfect by itself, but can help to avoid hallucinations in some cases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba2bec7-62f9-4e97-8cc2-9eafadccc8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = (\n",
    "    \"You are a helpful AI assistant. You try to answer the user queries to the best of your knowledge.\\\n",
    "If you are unsure of the answer, do not make up any information.\"\n",
    ")\n",
    "\n",
    "query = \"Is it possible to purchase provisioned throughput for Anthropic Claude models on Amazon Bedrock?\"\n",
    "\n",
    "response = generate_message_claude(query, system_prompt)\n",
    "pp.pprint(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5057db14-729d-4d64-8355-fd6fe2ee2398",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How do Amazon Bedrock Guardrails work?\"\n",
    "\n",
    "response = generate_message_claude(query, system_prompt)\n",
    "pp.pprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77f9196-7e32-4ef0-b516-b8801a688aa3",
   "metadata": {},
   "source": [
    "### 1.2 Understanding LLM generation parameters\n",
    "\n",
    "There are also a few standard parameters controlling the response generation process, which can be adjusted to influence hallucination likelihood:\n",
    "\n",
    "**Temperature** affects the shape of the probability distribution for the predicted output and influences the likelihood of the model selecting lower-probability outputs.\n",
    "\n",
    "- Choose a lower value to influence the model to select higher-probability outputs.\n",
    "- Choose a higher value to influence the model to select lower-probability outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f610c2e0-e810-47f4-89f9-9980319e32dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Create a haiku about a unicorn\"\n",
    "\n",
    "response = generate_message_claude(query, temperature=0.9)\n",
    "pp.pprint(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ff7dba-2f3f-4bd2-87ce-0d84c3c03a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Create a haiku about a unicorn\"\n",
    "\n",
    "response = generate_message_claude(query, temperature=0.1)\n",
    "pp.pprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7946bd-976d-4a8d-81ef-f7ff7e164f94",
   "metadata": {},
   "source": [
    "**top_k** limits the number of most-likely options that the model considers for choosing each token, in a process called nucleus sampling.\n",
    "\n",
    "- Choose a lower value to decrease the size of the pool and limit the options to more likely outputs.\n",
    "- Choose a higher value to increase the size of the pool and allow the model to consider less likely outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf753a4-8929-46ef-8aea-7ef040abb096",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is the universe\"\n",
    "\n",
    "response = generate_message_claude(query, top_k=3)\n",
    "pp.pprint(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea29f890-b8d8-4b8e-9b40-7e9b3b8941d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is the universe\"\n",
    "\n",
    "response = generate_message_claude(query, top_k=100)\n",
    "pp.pprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47723758-0f8c-4190-94ef-3bdd4d386908",
   "metadata": {},
   "source": [
    "**top_p**, like top_k, limits candidates that the model considers for choosing each token - but by their total probability score rather than the number of options. As a result, the pool will automatically be larger when the model is less certain, or smaller when just one or two tokens dominate the scores.\n",
    "\n",
    "- Choose a lower value to decrease the size of the pool and limit the options to more likely outputs.\n",
    "- Choose a higher value to increase the size of the pool and allow the model to consider less likely outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e300c40f-1b7b-4db1-bc5f-5f483507262e",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Who is mans best friend?\"\n",
    "\n",
    "response = generate_message_claude(query, top_p=0.1)\n",
    "pp.pprint(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfa57b3-bf14-4ba0-b86f-0c6bb79b1fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Who is mans best friend?\"\n",
    "\n",
    "response = generate_message_claude(query, top_p=0.9)\n",
    "pp.pprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9caf35-dccd-4c16-82eb-ea3e1327bc80",
   "metadata": {},
   "source": [
    "## Retrieval-Augmented Generation\n",
    "\n",
    "We are using the Retrieval Augmented Generation (RAG) technique with Amazon Bedrock. A RAG implementation consists of two parts:\n",
    "\n",
    "    1. A data pipeline that ingests that from documents (typically stored in Amazon S3) into a Knowledge Base i.e. a vector database such as Amazon OpenSearch Service Serverless (AOSS) so that it is available for lookup when a question is received.\n",
    "\n",
    "The data pipeline represents an undifferentiated heavy lifting and can be implemented using Amazon Bedrock Knowledge Bases. We can now connect an S3 bucket to a vector database such as AOSS and have a Bedrock Knowledge Bases read the objects (html, pdf, text etc.), chunk them, and then convert these chunks into embeddings using Amazon Titan Embeddings model and then store these embeddings in AOSS. All of this without having to build, deploy, and manage the data pipeline.\n",
    "\n",
    "![](images/fully_managed_ingestion.png \"This image shows how Aazon Bedrock Knowledge Bases ingests objects in a S3 bucket into the Knowledge Base for use in a RAG set up. The objects are chunks, embedded and then stored in a vector index.\")\n",
    "\n",
    "    2. An application that receives a question from the user, looks up the knowledge base for relevant pieces of information (context) and then creates a prompt that includes the question and the context and provides it to an LLM for generating a response.\n",
    "\n",
    "\n",
    "Once the data is available in the Bedrock knowledge base, then user questions can be answered using the following system design:\n",
    "\n",
    "![](images/retrieveAndGenerate.png \"This image shows the retrieval augmented generation (RAG) system design setup with knowledge bases, S3, and AOSS. Knowledge corpus is ingested into a vector database using Amazon Bedrock Knowledge Base Agent and then RAG approach is used to work question answering. The question is converted into embeddings followed by semantic similarity search to get similar documents. With the user prompt being augmented with the RAG search response, the LLM is invoked to get the final raw response for the user.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4588c8c3",
   "metadata": {},
   "source": [
    "### Set up the Knowledge Base\n",
    "\n",
    "In this example we'll use (an [outdated version](https://ws-assets-prod-iad-r-iad-ed304a55c2ca1aee.s3.us-east-1.amazonaws.com/1fa309f2-c771-42d5-87bc-e8f919e7bcc9/bedrock-ug.pdf) of) the publicly available [Amazon Bedrock User Guide](https://docs.aws.amazon.com/pdfs/bedrock/latest/userguide/bedrock-ug.pdf) as an example document to inform the model.\n",
    "\n",
    "The [Bedrock Knowledge Bases Console](https://console.aws.amazon.com/bedrock/home?#/knowledge-bases) provides a UI workflow to guide you through the multiple steps of creating and configuring a knowledge base, connecting a data source, and triggering a \"sync\" to index the data for search.\n",
    "\n",
    "For this sample though, we've provided an [AWS CloudFormation](https://aws.amazon.com/cloudformation/resources/templates/) template to make this multi-step setup as simple as possible - in [/infra/Bedrock-Knowledge-Base.yaml](../../infra/Bedrock-Knowledge-Base.yaml). You could upload this template yourself through the [CloudFormation Console](https://console.aws.amazon.com/cloudformation/home?#/stacks/create) - or click the button below to get started with a version we've already published to Amazon S3:\n",
    "\n",
    "[![Launch Stack](https://s3.amazonaws.com/cloudformation-examples/cloudformation-launch-stack.png)](https://console.aws.amazon.com/cloudformation/home?#/stacks/create/review?templateURL=https://s3.amazonaws.com/ws-assets-prod-iad-r-iad-ed304a55c2ca1aee/1fa309f2-c771-42d5-87bc-e8f919e7bcc9/Bedrock-Knowledge-Base.yaml&stackName=HallucinationKBDemo \"Launch Stack\")\n",
    "\n",
    "<div style=\"border: 4px solid coral; text-align: left; margin: auto; padding: 15px;\">\n",
    "    <strong>⏰ This deployment can take ~10-15 minutes to complete</strong>\n",
    "</div>\n",
    "<br/>\n",
    "\n",
    "Once the stack is created successfully, you can check in the [Amazon Bedrock Console](https://console.aws.amazon.com/bedrock/home?#/knowledge-bases) that your knowledge base is deployed successfully, and run the cell below to look up its unique ID automatically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe868f1-d5b1-490f-8e54-e5a856ca1b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "kb_name = \"bedrock-userguide-demo\"\n",
    "\n",
    "kb_id = None\n",
    "kb_list = bedrock_agent_client.list_knowledge_bases()[\"knowledgeBaseSummaries\"]\n",
    "for kb in kb_list:\n",
    "    if kb[\"name\"] == kb_name:\n",
    "        kb_id = kb[\"knowledgeBaseId\"]\n",
    "\n",
    "if kb_id is None:\n",
    "    raise ValueError(\n",
    "        \"Couldn't find pre-created Bedrock Knowledge Base. Please follow the instructions \"\n",
    "        \"above to deploy the sample knowledge base, double-check its name matches '%s' configured \"\n",
    "        \"above, then re-run this cell.\" % kb_name\n",
    "    )\n",
    "print(f\"Using existing Bedrock Knowledge Base with ID: {kb_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c35041-9487-46e6-9a36-fd8d408712c4",
   "metadata": {},
   "source": [
    "### Query the Knowledge Base\n",
    "\n",
    "Behind the scenes, RetrieveAndGenerate API converts queries into embeddings, searches the knowledge base, and then augments the foundation model prompt with the search results as context information and returns the FM-generated response to the question. For multi-turn conversations, Knowledge Bases manage short-term memory of the conversation to provide more contextual results.The output of the RetrieveAndGenerate API includes the generated response, source attribution as well as the retrieved text chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06645e80-eebb-4883-a086-70ccfdf604c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_bedrock_llm_with_knowledge_base(\n",
    "    query,\n",
    "    kb_id=kb_id,\n",
    "    model_arn=llm_model_id,\n",
    "    temperature=0,\n",
    "    top_p=1,\n",
    "):\n",
    "    response = bedrock_agent_runtime.retrieve_and_generate(\n",
    "        input={\"text\": query},\n",
    "        retrieveAndGenerateConfiguration={\n",
    "            \"type\": \"KNOWLEDGE_BASE\",\n",
    "            \"knowledgeBaseConfiguration\": {\n",
    "                \"knowledgeBaseId\": kb_id,\n",
    "                \"modelArn\": model_arn,\n",
    "                \"generationConfiguration\": {\n",
    "                    \"inferenceConfig\": {\n",
    "                        \"textInferenceConfig\": {\n",
    "                            \"maxTokens\": 2048,\n",
    "                            \"temperature\": temperature,\n",
    "                            \"topP\": top_p,\n",
    "                        }\n",
    "                    },\n",
    "                    \"promptTemplate\": {\n",
    "                        \"textPromptTemplate\": \"You are a helpful AI assistant. You try to answer the user queries based on the provided context.\\\n",
    "                        If you are unsure of the answer, do not make up any information. Context to the user query is $search_results$ \\\n",
    "                        $output_format_instructions$\"\n",
    "                    },\n",
    "                },\n",
    "            },\n",
    "        },\n",
    "    )\n",
    "\n",
    "    return response\n",
    "\n",
    "\n",
    "def pretty_display_rag_citations(response):\n",
    "    citations = response[\"citations\"]\n",
    "    contexts = []\n",
    "    for citation in citations:\n",
    "        retrievedReferences = citation[\"retrievedReferences\"]\n",
    "        for reference in retrievedReferences:\n",
    "            contexts.append(reference[\"content\"][\"text\"])\n",
    "    print(f\"---------- The citations for the response:\")\n",
    "    pp.pprint(contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a151db3d-b340-46b2-bd57-491714d2068c",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is Amazon Bedrock?\"\n",
    "\n",
    "response = ask_bedrock_llm_with_knowledge_base(query, kb_id, temperature=0)\n",
    "pp.pprint(response[\"output\"][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de961d7-3ca2-4fde-aea9-0f479b87e2ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pretty_display_rag_citations(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e05dee0-9210-4a73-a0dd-b0f061843fe6",
   "metadata": {},
   "source": [
    "### Change the temperature to choose a different amount of randomness in the model response\n",
    "- Choose a lower value to influence the model to select higher-probability outputs.\n",
    "\n",
    "- Choose a higher value to influence the model to select lower-probability outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c81ebda-0321-493e-a2db-e77d170b3dd4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "query = \"What is Amazon Bedrock?\"\n",
    "\n",
    "response = ask_bedrock_llm_with_knowledge_base(query, kb_id, temperature=0.8)\n",
    "pp.pprint(response[\"output\"][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31673ff-7e17-45d8-a1f7-330799665725",
   "metadata": {},
   "source": [
    "### Test another query!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0892703-6fa4-4bac-a3b1-7cd30523b888",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "query = \"Is it possible to purchase provisioned throughput for Anthropic Claude Sonnet on Amazon Bedrock?\"\n",
    "\n",
    "response = ask_bedrock_llm_with_knowledge_base(query, kb_id)\n",
    "pp.pprint(response[\"output\"][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424c0f09-2ecf-40c5-9dd2-1c42a7af6231",
   "metadata": {},
   "source": [
    "## Extra protection with Amazon Bedrock Guardrails\n",
    "Contextual grounding check evaluates for hallucinations across two paradigms:\n",
    "\n",
    "- Grounding – This checks if the model response is factually accurate based on the source and is grounded in the source. Any new information introduced in the response will be considered un-grounded.\n",
    "\n",
    "- Relevance – This checks if the model response is relevant to the user query.\n",
    "\n",
    "\n",
    "### Create the Guardrail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810e1acf-ddbe-4e93-bf96-32e7282a7db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create guardrail\n",
    "# (get first 6 characters of uuid string to generate guardrail name suffix)\n",
    "random_id_suffix = str(uuid.uuid1())[:6]\n",
    "guardrail_name = f\"bedrock-rag-grounding-guardrail-{random_id_suffix}\"\n",
    "print(guardrail_name)\n",
    "\n",
    "guardrail_response = bedrock_client.create_guardrail(\n",
    "    name=guardrail_name,\n",
    "    description=\"Guardrail for ensuring relevance and grounding of model responses in RAG powered chatbot\",\n",
    "    contextualGroundingPolicyConfig={\n",
    "        \"filtersConfig\": [\n",
    "            {\"type\": \"GROUNDING\", \"threshold\": 0.6},\n",
    "            {\"type\": \"RELEVANCE\", \"threshold\": 0.6},\n",
    "        ]\n",
    "    },\n",
    "    blockedInputMessaging=\"Can you please rephrase your question?\",\n",
    "    blockedOutputsMessaging=\"Sorry, I am not able to find the correct answer to your query - Can you try reframing your query to be more specific\",\n",
    ")\n",
    "guardrailId = guardrail_response[\"guardrailId\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a503292a-b84c-46d5-8fe2-f9d199198f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "guardrail_version = bedrock_client.create_guardrail_version(\n",
    "    guardrailIdentifier=guardrail_response[\"guardrailId\"],\n",
    "    description=\"Working version of RAG app guardrail with higher thresholds for contextual grounding\",\n",
    ")\n",
    "\n",
    "guardrailVersion = guardrail_response[\"version\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36ba46b",
   "metadata": {},
   "source": [
    "### Query the Knowledge Base with Guardrail enabled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfbd6ac-6411-47dd-b645-33df81614878",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_and_generate_with_guardrail(\n",
    "    query,\n",
    "    kb_id,\n",
    "    model_arn=llm_model_id,\n",
    "):\n",
    "    prompt_template = (\n",
    "        \"You are a helpful AI assistant to help users understand documented risks in various projects. \\\n",
    "    Answer the user query based on the context retrieved. If you dont know the answer, dont make up anything. \\\n",
    "    Only answer based on what you know from the provided context. You can ask the user for clarifying questions if anything is unclear\\\n",
    "    But generate an answer only when you are confident about it and based on the provided context.\\\n",
    "    User Query: $query$\\\n",
    "    Context: $search_results$\\\n",
    "    $output_format_instructions$\"\n",
    "    )\n",
    "\n",
    "    response = bedrock_agent_runtime.retrieve_and_generate(\n",
    "        input={\"text\": query},\n",
    "        retrieveAndGenerateConfiguration={\n",
    "            \"type\": \"KNOWLEDGE_BASE\",\n",
    "            \"knowledgeBaseConfiguration\": {\n",
    "                \"generationConfiguration\": {\n",
    "                    \"guardrailConfiguration\": {\n",
    "                        \"guardrailId\": guardrailId,\n",
    "                        \"guardrailVersion\": guardrailVersion,\n",
    "                    },\n",
    "                    \"inferenceConfig\": {\n",
    "                        \"textInferenceConfig\": {\"temperature\": 0.1, \"topP\": 0.25}\n",
    "                    },\n",
    "                    \"promptTemplate\": {\"textPromptTemplate\": prompt_template},\n",
    "                },\n",
    "                \"knowledgeBaseId\": kb_id,\n",
    "                \"modelArn\": model_arn,\n",
    "                \"retrievalConfiguration\": {\n",
    "                    \"vectorSearchConfiguration\": {\"overrideSearchType\": \"SEMANTIC\"}\n",
    "                },\n",
    "            },\n",
    "        },\n",
    "    )\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d421f9-a926-47a9-847b-ac38ed0552ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is Generative AI?\"\n",
    "\n",
    "model_response = retrieve_and_generate_with_guardrail(query, kb_id)\n",
    "\n",
    "pp.pprint(model_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4da3827-b217-422e-97a2-6c231c6322d9",
   "metadata": {},
   "source": [
    "<div style=\"border: 2px solid coral; text-align: left; margin: auto; padding-left: 20px; padding-right: 20px\">\n",
    "    <h4>The Guardrail intervenes when the generated model response is not grounded in a context</h4>\n",
    "</div>\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56677a75-464c-453b-b2bd-7813f2a65d1a",
   "metadata": {},
   "source": [
    "## Evaluating RAG with Ragas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e33826a-0836-4ad1-9553-dcb7df3f87c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# External Dependencies:\n",
    "from datasets import Dataset\n",
    "from langchain_aws import BedrockEmbeddings, ChatBedrockConverse\n",
    "from langchain_core.globals import set_verbose, set_debug\n",
    "import pandas as pd\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import answer_relevancy, answer_correctness\n",
    "\n",
    "# Disable verbose logging\n",
    "set_verbose(False)\n",
    "\n",
    "# Disable debug logging\n",
    "set_debug(False)\n",
    "\n",
    "\n",
    "llm_for_evaluation = ChatBedrockConverse(model=llm_model_id, client=bedrock_runtime)\n",
    "\n",
    "bedrock_embeddings = BedrockEmbeddings(\n",
    "    model_id=embedding_model_id, client=bedrock_runtime\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3511743-c8c2-4e3c-949a-79bf9bec79cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"data/bedrock-user-guide-test.csv\").dropna()\n",
    "test.style.set_properties(**{\"text-align\": \"left\", \"border\": \"1px solid black\"})\n",
    "test.to_string(justify=\"left\", index=False)\n",
    "with pd.option_context(\"display.max_colwidth\", None):\n",
    "    display(pd.DataFrame(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a5f15a-8053-4edc-97e5-c6b96641c219",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = test[\"Question/prompt\"].tolist()\n",
    "ground_truth = [gt for gt in test[\"Correct answer\"].tolist()]\n",
    "\n",
    "answers = []\n",
    "contexts = []\n",
    "\n",
    "for query in questions:\n",
    "    response = ask_bedrock_llm_with_knowledge_base(query, kb_id)\n",
    "    generatedResult = response[\"output\"][\"text\"]\n",
    "    answers.append(generatedResult)\n",
    "\n",
    "    context = []\n",
    "    citations = response[\"citations\"]\n",
    "    for citation in citations:\n",
    "        retrievedReferences = citation[\"retrievedReferences\"]\n",
    "        for reference in retrievedReferences:\n",
    "            context.append(reference[\"content\"][\"text\"])\n",
    "    contexts.append(context)\n",
    "\n",
    "# To dict\n",
    "data = {\n",
    "    \"question\": questions,\n",
    "    \"answer\": answers,\n",
    "    \"contexts\": contexts,\n",
    "    \"ground_truth\": ground_truth,\n",
    "}\n",
    "\n",
    "# Convert dict to dataset\n",
    "dataset = Dataset.from_dict(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bce932e-94b8-4068-acc4-c0d85287daaa",
   "metadata": {},
   "source": [
    "Let's explore two particular Ragas metrics that we'll use in the next lab\n",
    "\n",
    "### answer_relevancy metric\n",
    "\n",
    "Answer Relevancy metric focuses on assessing how pertinent the generated answer is to the given prompt. A lower score is assigned to answers that are incomplete or contain redundant information and higher scores indicate better relevancy. This metric is computed using the user_input, the retrived_contexts and the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16eb532-3c65-4adf-b6f2-cab77d11cc81",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "metrics_ar = [answer_relevancy]\n",
    "\n",
    "result_ar = evaluate(\n",
    "    dataset=dataset,\n",
    "    metrics=metrics_ar,\n",
    "    llm=llm_for_evaluation,\n",
    "    embeddings=bedrock_embeddings,\n",
    "    raise_exceptions=False,\n",
    ")\n",
    "\n",
    "ragas_df_ar = result_ar.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76503a5-36de-4365-a390-a37dba57c439",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ragas_df_ar.style.set_properties(**{\"text-align\": \"left\", \"border\": \"1px solid black\"})\n",
    "ragas_df_ar.to_string(justify=\"left\", index=False)\n",
    "with pd.option_context(\"display.max_colwidth\", None):\n",
    "    display(pd.DataFrame(ragas_df_ar))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2380a8fa-076b-4b3c-bd75-746d0e2d1555",
   "metadata": {},
   "source": [
    "### answer_correctness metric\n",
    "\n",
    "The assessment of Answer Correctness involves gauging the accuracy of the generated answer when compared to the ground truth. This evaluation relies on the ground truth and the answer, with scores ranging from 0 to 1. A higher score indicates a closer alignment between the generated answer and the ground truth, signifying better correctness. Answer correctness encompasses two critical aspects: semantic similarity between the generated answer and the ground truth, as well as factual similarity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7cbce17-792b-47da-b9a9-1053e3ae11be",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_ac = [answer_correctness]\n",
    "\n",
    "result_ac = evaluate(\n",
    "    dataset=dataset,\n",
    "    metrics=metrics_ac,\n",
    "    llm=llm_for_evaluation,\n",
    "    embeddings=bedrock_embeddings,\n",
    "    raise_exceptions=False,\n",
    ")\n",
    "\n",
    "ragas_df_ac = result_ac.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78fd21d-f3e4-496d-8f34-bbfda4c91687",
   "metadata": {},
   "outputs": [],
   "source": [
    "ragas_df_ac.style.set_properties(**{\"text-align\": \"left\", \"border\": \"1px solid black\"})\n",
    "ragas_df_ac.to_string(justify=\"left\", index=False)\n",
    "with pd.option_context(\"display.max_colwidth\", None):\n",
    "    display(pd.DataFrame(ragas_df_ac))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95cb27ab",
   "metadata": {},
   "source": [
    "### <a >Challenge Exercise :: Try it Yourself! </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c62acdc",
   "metadata": {},
   "source": [
    "\n",
    "<div style=\"border: 4px solid coral; text-align: left; margin: auto;\">\n",
    "    <br>\n",
    "    <p style=\"text-align: center; margin: auto;\"><b>Try the following exercises in this lab and note the observations.</b></p>\n",
    "<p style=\" text-align: left; margin: auto;\">\n",
    "<ol>\n",
    "    <li>Test the RAG based LLM with more questions about Amazon Bedrock. </li>\n",
    "<li>Look the the citations or retrieved references and see if the answer generated by the RAG chatbot aligns with these retrieved contexts. What response do you get when the retrieved context comes up empty? </li>\n",
    "<li>Apply system prompts to RAG as well as amazon Bedrock Guardrails and test which is more consistent in blocking responses when the model response is hallucinated </li>\n",
    "<li>Run the tutorial for RAG Checker and compare the difference with RAGAS evaluation framework: https://github.com/amazon-science/RAGChecker/blob/main/tutorial/ragchecker_tutorial_en.md </li>\n",
    "</ol>\n",
    "<br>\n",
    "</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dfc89ac-b0e8-438a-bba2-12bc3a4a3f94",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "We now have an understanding of parameters which influence hallucinations in Large Language Models. We learnt how to set up Retrieval Augmented Generation to provide a context to the model while answering.\n",
    "We used Contextual grounding in Amazon Bedrock Guardrials to intervene when hallucinations are detected.\n",
    "Finally we looked into the metrics of RAGAS and how to use them to measure hallucinations in your RAG powered chatbot.\n",
    "\n",
    "To explore further, check out the [../bedrock-agent-self-reflection](../bedrock-agent-self-reflection/) sample in which we'll:\n",
    "1. Build a custom hallucination detector\n",
    "2. Use Amazon Bedrock Agents to intervene when hallucinations are detected\n",
    "3. Call a human for support when the LLM hallucinates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5039a789",
   "metadata": {},
   "source": [
    "## Clean-up\n",
    "\n",
    "Once you're done experimenting, remember to clean up created AWS resources in order to avoid ongoing costs.\n",
    "\n",
    "⚠️ **Note:** The following will be re-used in the `bedrock-agent-self-reflection` sample, so don't clean them up just yet if you're about to explore that!\n",
    "\n",
    "1. The Knowledge Base we deployed via AWS CloudFormation can be deleted by deleting the stack you provisioned from the [CloudFormation Console](https://console.aws.amazon.com/cloudformation/home?#/stacks)\n",
    "2. The Amazon Bedrock Guardrail we created can be deleted either from the [Bedrock Guardrails Console](https://console.aws.amazon.com/bedrock/home?#/guardrails) or by un-commenting and running the code cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec48e136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bedrock_client.delete_guardrail(guardrailIdentifier=guardrailId)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
