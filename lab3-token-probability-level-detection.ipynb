{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98851f24-a2f5-42c6-8435-219764f5af33",
   "metadata": {},
   "source": [
    "# <a id=\"top\">Lab 3: Token Probability Level Detection</a>\n",
    "\n",
    "## Detecting LLM Hallucinations Using Token-Level Confidence Signals\n",
    "\n",
    "In this notebook, we will explore **Token Probability Level Detection** techniques that analyze the probability distributions of individual tokens as they are generated by the model. These methods provide deeper insights than Response Level approaches by examining the model's internal confidence signals, while remaining more practical than full Internal State Level methods that require access to model weights and activations.\n",
    "\n",
    "We'll implement and compare three powerful detection methods:\n",
    "1. **Entropy-Based Uncertainty Detection** - Measuring uncertainty in token probability distributions\n",
    "2. **Perplexity Spike Analysis** - Identifying sudden drops in model confidence\n",
    "3. **Top-K Probability Divergence** - Analyzing how probability mass is distributed among top token choices\n",
    "\n",
    " You'll use the GPT-OSS-20B model you deployed in Lab 0 to extract log probability distributions and implement advanced uncertainty quantification techniques for hallucination detection.\n",
    "\n",
    "\n",
    "##### Notebook Kernel\n",
    "Please choose `Python3` as the kernel type at the top right corner of the notebook if that does not appear by default.\n",
    "\n",
    "<div style=\"border: 4px solid coral; text-align: left; margin: auto; padding-left: 20px; padding-right: 20px\">\n",
    "    <h4>ðŸ’¡ Key Learning Objectives</h4>\n",
    "    <ul>\n",
    "        <li>Understand how to leverage log probabilities for hallucination detection</li>\n",
    "        <li>Learn to identify uncertainty patterns in token distributions</li>\n",
    "        <li>Master perplexity-based confidence scoring</li>\n",
    "        <li>Apply divergence metrics to detect model hesitation</li>\n",
    "        <li>Build production-ready agents with Strands that use token-level checks</li>\n",
    "    </ul>\n",
    "</div>\n",
    "<br/>\n",
    "\n",
    "## Use-Case Overview\n",
    "\n",
    "Token Probability Level detection is valuable when:\n",
    "- You have access to log probabilities from your model API\n",
    "- You need real-time detection during generation (not just post-hoc analysis)\n",
    "- You want to identify specific tokens that indicate uncertainty\n",
    "- You need fine-grained confidence signals for each part of the response\n",
    "\n",
    "These methods work by analyzing the model's confidence in its token selections, leveraging the principle that **hallucinations often exhibit distinct probability patterns** such as high entropy (uncertainty), high perplexity (surprise), or unusual probability distributions.\n",
    "\n",
    "**Key Concepts:**\n",
    "- **Log Probabilities (logprobs)**: Natural logarithm of the probability the model assigns to each token\n",
    "- **Token Probabilities**: Confidence scores (0-1) for each generated token, derived from logprobs\n",
    "- **Perplexity**: Measure of how \"surprised\" the model is by its own prediction (exp(-logprob))\n",
    "- **Epistemic Uncertainty**: Model uncertainty due to lack of knowledge or insufficient training data\n",
    "\n",
    "**Why Log Probabilities for Token Probability Level Detection?**\n",
    "- **Real-Time Signals**: Access to confidence information as tokens are generated\n",
    "- **Minimal Overhead**: No additional model calls required (unlike Response Level Detection methods)\n",
    "- **Granular Analysis**: Token-level uncertainty rather than response-level\n",
    "- **Early Detection**: Identify hallucinations before full response is generated\n",
    "\n",
    "The GPT-OSS-20B endpoint you deployed in Lab 0 is configured to expose log probabilities â€” the raw confidence scores the model assigns to each generated token. These internal signals provide rich information about model uncertainty that's invisible in the final text output.\n",
    "\n",
    "## Sections\n",
    "\n",
    "This notebook has the following sections:\n",
    "\n",
    "1. [Environment Setup and Model Configuration](#1.-Environment-Setup-and-Model-Configuration)\n",
    "2. [Method 1: Entropy-Based Uncertainty Detection](#2.-Method-1:-Entropy-Based-Uncertainty-Detection)\n",
    "3. [Method 2: Perplexity Spike Analysis](#3.-Method-2:-Perplexity-Spike-Analysis)\n",
    "4. [Method 3: Top-K Probability Divergence](#4.-Method-3:-Top-K-Probability-Divergence)\n",
    "5. [Comparative Analysis of All Methods](#5.-Comparative-Analysis-of-All-Methods)\n",
    "6. [Production Integration with Strands Agents](#6.-Production-Integration-with-Strands-Agents)\n",
    "7. [Conclusion and Best Practices](#7.-Conclusion-and-Best-Practices)\n",
    "8. [(Optional) Challenge Exercises](#8.-(Optional)-Challenge-Exercises)\n",
    "    \n",
    "Please work from top to bottom and don't skip sections as this could lead to error messages due to missing dependencies.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5463f03b-7f8b-4a95-a7c8-0caa46d6ae4b",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Model Configuration\n",
    "(<a href=\"#top\">Go to top</a>)\n",
    "\n",
    "**If you haven't already** installed the workshop's dependencies (from [pyproject.toml](./pyproject.toml)), you can un-comment (remove `# `) and run the below cell to do so. We've commented it out by default, assuming you already ran it at the start of lab 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d884fb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -q -e ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d17327",
   "metadata": {},
   "source": [
    "<div style=\"border: 4px solid coral; text-align: left; margin: auto; padding-left: 20px; padding-right: 20px; padding-top: 15px;\">\n",
    "    <h4>ðŸ”„ Restart the kernel after installing</h4>\n",
    "    <p>\n",
    "        <strong>IF</strong> you ran the above install command cell, you'll need to restart the\n",
    "        notebook kernel afterwards for the installations to take full effect.\n",
    "    </p>\n",
    "    <p>\n",
    "        Note that you may see some error notices about dependency conflicts in SageMaker Studio\n",
    "        environments, but this is okay as long as the installations are completed.\n",
    "    </p>\n",
    "</div>\n",
    "<br/>\n",
    "\n",
    "With the installation complete, you're ready to import the libraries we'll use in the notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31eff904-45e3-471c-881e-6534466429d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import numpy as np\n",
    "from transformers import pipeline\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Tuple\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(\"All packages imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f9e1fb-9ccf-4687-be71-df390c4c7d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve stored endpoint configuration\n",
    "%store -r endpoint_name\n",
    "%store -r inference_component_name\n",
    "\n",
    "try:\n",
    "    endpoint_name, inference_component_name\n",
    "except NameError as e:\n",
    "    raise RuntimeError(\n",
    "        \"SageMaker endpoint not found. Please check you've run lab 0 first!\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b84a1f4-9ecb-4e03-a252-7f6fe7832763",
   "metadata": {},
   "source": [
    "### Configure Custom Model Invocation with SageMaker Endpoint\n",
    "\n",
    "We'll test our hallucination detection methods with GPT OSS 20B through a custom provider.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f22cc7b-929a-4389-b0bf-3c0a5d1d35b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def invoke_sagemaker_endpoint(endpoint_name, payload, inference_component_name=None):\n",
    "    \"\"\"\n",
    "    Invoke SageMaker endpoint with given payload\n",
    "\n",
    "    Args:\n",
    "        endpoint_name: Name of the SageMaker endpoint\n",
    "        payload: Dictionary containing the request payload\n",
    "        inference_component_name: Optional inference component name\n",
    "\n",
    "    Returns:\n",
    "        Parsed response from the endpoint\n",
    "    \"\"\"\n",
    "    smr_client = boto3.client(\"sagemaker-runtime\")\n",
    "\n",
    "    try:\n",
    "        # Prepare invoke_endpoint parameters\n",
    "        invoke_params = {\n",
    "            \"EndpointName\": endpoint_name,\n",
    "            \"ContentType\": \"application/json\",\n",
    "            \"Body\": json.dumps(payload),\n",
    "        }\n",
    "\n",
    "        # Add inference component if specified\n",
    "        if inference_component_name:\n",
    "            invoke_params[\"InferenceComponentName\"] = inference_component_name\n",
    "\n",
    "        # Invoke the endpoint\n",
    "        response = smr_client.invoke_endpoint(**invoke_params)\n",
    "\n",
    "        # Parse and return the response\n",
    "        result = json.loads(response[\"Body\"].read().decode())\n",
    "        return result\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error invoking endpoint {endpoint_name}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Usage example\n",
    "payload = {\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"Is Sydney the capital of Australia?\"}],\n",
    "    \"logprobs\": True,\n",
    "    \"top_logprobs\": 5,\n",
    "}\n",
    "\n",
    "# Invoke the endpoint\n",
    "result = invoke_sagemaker_endpoint(\n",
    "    endpoint_name=endpoint_name,\n",
    "    payload=payload,\n",
    "    inference_component_name=inference_component_name,\n",
    ")\n",
    "\n",
    "if result:\n",
    "    print(\"\\n-----\\n\" + result[\"choices\"][0][\"message\"][\"content\"] + \"\\n-----\\n\")\n",
    "    print(result[\"usage\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f363f4f0",
   "metadata": {},
   "source": [
    "Here are the logprobs generated with the outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a361a46",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result[\"choices\"][0][\"logprobs\"][\"content\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada2844c-04fd-4dfc-a87d-0a668afb8f82",
   "metadata": {},
   "source": [
    "### Configuration and Helper Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33decb7-d13a-49ba-bfe2-c0b4d874db4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TokenProbabilityConfig:\n",
    "    \"\"\"Configuration for token probability detection methods\"\"\"\n",
    "\n",
    "    entropy_threshold: float = 2.0\n",
    "    perplexity_threshold: float = 50.0\n",
    "    confidence_threshold: float = 0.1\n",
    "    divergence_threshold: float = 0.5\n",
    "    top_k: int = 5\n",
    "\n",
    "\n",
    "# Initialize configuration\n",
    "config = TokenProbabilityConfig()\n",
    "\n",
    "print(f\"\\nToken Probability Detection Configuration:\")\n",
    "print(f\"  â€¢ Entropy Threshold: {config.entropy_threshold}\")\n",
    "print(f\"  â€¢ Perplexity Threshold: {config.perplexity_threshold}\")\n",
    "print(f\"  â€¢ Confidence Threshold: {config.confidence_threshold}\")\n",
    "print(f\"  â€¢ Divergence Threshold: {config.divergence_threshold}\")\n",
    "print(f\"  â€¢ Top-K Analysis: {config.top_k}\")\n",
    "\n",
    "# ============================================================================\n",
    "# Helper Functions for Log Probability Processing\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "def extract_probabilities(logprobs: List[float]) -> List[float]:\n",
    "    \"\"\"Convert log probabilities to probabilities\"\"\"\n",
    "    return [math.exp(lp) for lp in logprobs]\n",
    "\n",
    "\n",
    "def normalize_probabilities(probs: List[float]) -> List[float]:\n",
    "    \"\"\"Normalize probabilities to sum to 1\"\"\"\n",
    "    total = sum(probs)\n",
    "    if total == 0:\n",
    "        return probs\n",
    "    return [p / total for p in probs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5798a069",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sagemaker_token_data_live(\n",
    "    prompt: str, endpoint_name: str, inference_component_name: str = None\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Get token data from SageMaker endpoint response for token probability analysis.\n",
    "\n",
    "    Args:\n",
    "        prompt: The prompt to send to the model\n",
    "        endpoint_name: SageMaker endpoint name\n",
    "        inference_component_name: Optional inference component name\n",
    "\n",
    "    Returns:\n",
    "        Token data in the format expected by the token probability detectors\n",
    "    \"\"\"\n",
    "    # Payload with logprobs enabled\n",
    "    payload = {\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "        \"logprobs\": True,\n",
    "        \"top_logprobs\": 5,\n",
    "        \"temperature\": 0.1,\n",
    "    }\n",
    "\n",
    "    # Invoke SageMaker endpoint\n",
    "    result = invoke_sagemaker_endpoint(\n",
    "        endpoint_name=endpoint_name,\n",
    "        payload=payload,\n",
    "        inference_component_name=inference_component_name,\n",
    "    )\n",
    "\n",
    "    if not result or \"choices\" not in result:\n",
    "        print(\"Error: Could not get valid response from SageMaker endpoint\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        logprobs_content = result[\"choices\"][0][\"logprobs\"][\"content\"]\n",
    "\n",
    "        # Find the first actual content token (look for 'No' specifically or first meaningful token)\n",
    "        selected_token_data = None\n",
    "\n",
    "        # First, try to find the 'No' token specifically\n",
    "        for token_info in logprobs_content:\n",
    "            if token_info[\"token\"] == \"No\":\n",
    "                selected_token_data = {\n",
    "                    \"token\": token_info[\"token\"],\n",
    "                    \"logprob\": token_info[\"logprob\"],\n",
    "                    \"bytes\": token_info[\"bytes\"],\n",
    "                    \"top_logprobs\": token_info[\"top_logprobs\"],\n",
    "                }\n",
    "                print(f\"âœ“ Found 'No' token for analysis\")\n",
    "                break\n",
    "\n",
    "        # If 'No' not found, look for first meaningful content token after <|message|>\n",
    "        if not selected_token_data:\n",
    "            message_found = False\n",
    "            for i, token_info in enumerate(logprobs_content):\n",
    "                token = token_info[\"token\"]\n",
    "\n",
    "                if token == \"<|message|>\":\n",
    "                    message_found = True\n",
    "                    continue\n",
    "\n",
    "                if message_found and not token.startswith(\"<|\") and token.strip():\n",
    "                    selected_token_data = {\n",
    "                        \"token\": token_info[\"token\"],\n",
    "                        \"logprob\": token_info[\"logprob\"],\n",
    "                        \"bytes\": token_info[\"bytes\"],\n",
    "                        \"top_logprobs\": token_info[\"top_logprobs\"],\n",
    "                    }\n",
    "                    print(\n",
    "                        f\"âœ“ Selected first content token: '{selected_token_data['token']}'\"\n",
    "                    )\n",
    "                    break\n",
    "\n",
    "        if selected_token_data:\n",
    "            print(f\"  Logprob: {selected_token_data['logprob']:.6f}\")\n",
    "            print(f\"  Probability: {math.exp(selected_token_data['logprob']):.6f}\")\n",
    "            print(\n",
    "                f\"  Top alternatives: {[alt['token'] for alt in selected_token_data['top_logprobs'][:3]]}\"\n",
    "            )\n",
    "            return selected_token_data\n",
    "        else:\n",
    "            print(\"Warning: No suitable content token found\")\n",
    "            return None\n",
    "\n",
    "    except (KeyError, IndexError) as e:\n",
    "        print(f\"Error extracting token data: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Check if endpoint was discovered in previous cell\n",
    "if endpoint_name and inference_component_name:\n",
    "    print(\"Getting fresh token data from SageMaker endpoint...\")\n",
    "\n",
    "    # Get real token data from SageMaker\n",
    "    example_token_data = get_sagemaker_token_data_live(\n",
    "        prompt=\"Is Sydney the capital of Australia?\",\n",
    "        endpoint_name=endpoint_name,\n",
    "        inference_component_name=inference_component_name,\n",
    "    )\n",
    "\n",
    "    if example_token_data:\n",
    "        print(f\"\\nâœ“ Real SageMaker token data loaded: '{example_token_data['token']}'\")\n",
    "        print(f\"  Analysis ready!\")\n",
    "    else:\n",
    "        print(\"Failed to get SageMaker token data, using fallback 'No' token\")\n",
    "        # Use the exact 'No' token data from your previous response\n",
    "        example_token_data = {\n",
    "            \"token\": \"No\",\n",
    "            \"logprob\": -0.0005974177038297057,\n",
    "            \"bytes\": [78, 111],\n",
    "            \"top_logprobs\": [\n",
    "                {\"token\": \"No\", \"logprob\": -0.0005974177038297057, \"bytes\": [78, 111]},\n",
    "                {\"token\": \"**\", \"logprob\": -7.7505974769592285, \"bytes\": [42, 42]},\n",
    "                {\"token\": \"Not\", \"logprob\": -9.87559700012207, \"bytes\": [78, 111, 116]},\n",
    "                {\"token\": \"S\", \"logprob\": -10.12559700012207, \"bytes\": [83]},\n",
    "                {\n",
    "                    \"token\": '\"No',\n",
    "                    \"logprob\": -11.62559700012207,\n",
    "                    \"bytes\": [226, 128, 156, 78, 111],\n",
    "                },\n",
    "            ],\n",
    "        }\n",
    "        print(\n",
    "            f\"âœ“ Using fallback 'No' token data: logprob={example_token_data['logprob']:.6f}\"\n",
    "        )\n",
    "else:\n",
    "    print(\"No SageMaker endpoint discovered. Using fallback 'No' token data.\")\n",
    "    # Use the exact 'No' token data from your previous response\n",
    "    example_token_data = {\n",
    "        \"token\": \"No\",\n",
    "        \"logprob\": -0.0005974177038297057,\n",
    "        \"bytes\": [78, 111],\n",
    "        \"top_logprobs\": [\n",
    "            {\"token\": \"No\", \"logprob\": -0.0005974177038297057, \"bytes\": [78, 111]},\n",
    "            {\"token\": \"**\", \"logprob\": -7.7505974769592285, \"bytes\": [42, 42]},\n",
    "            {\"token\": \"Not\", \"logprob\": -9.87559700012207, \"bytes\": [78, 111, 116]},\n",
    "            {\"token\": \"S\", \"logprob\": -10.12559700012207, \"bytes\": [83]},\n",
    "            {\n",
    "                \"token\": '\"No',\n",
    "                \"logprob\": -11.62559700012207,\n",
    "                \"bytes\": [226, 128, 156, 78, 111],\n",
    "            },\n",
    "        ],\n",
    "    }\n",
    "    print(\n",
    "        f\"âœ“ Using fallback 'No' token data: logprob={example_token_data['logprob']:.6f}\"\n",
    "    )\n",
    "\n",
    "print(\n",
    "    f\"\\nâœ“ Token data ready for token probability analysis: '{example_token_data['token']}'\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53daa38a-d3a6-4cc3-823c-c3d2fdd9be92",
   "metadata": {},
   "source": [
    "## 2. Method 1: Entropy-Based Uncertainty Detection\n",
    "(<a href=\"#top\">Go to top</a>)\n",
    "\n",
    "### How It Works\n",
    "\n",
    "Entropy-Based Uncertainty Detection measures the \"disorder\" in token probability distributions.\n",
    "High entropy indicates the model is uncertain between multiple choices, while low entropy \n",
    "suggests confident selection.\n",
    "\n",
    "**The Process:**\n",
    "1. Extract probability distribution from log probabilities\n",
    "2. Calculate Shannon entropy of the distribution\n",
    "3. Normalize entropy to [0, 1] scale\n",
    "4. Compare against threshold to detect uncertainty\n",
    "\n",
    "**Key Principle**: When models hallucinate, they often show high entropy as they're \"choosing\" between multiple plausible-seeming options rather than having clear knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59e9dd1-3055-4d18-a949-4ff68b505e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EntropyUncertaintyDetector:\n",
    "    \"\"\"Detect hallucinations by measuring entropy in token probability distributions.\"\"\"\n",
    "\n",
    "    def __init__(self, config: TokenProbabilityConfig):\n",
    "        self.config = config\n",
    "\n",
    "    def analyze_token(self, token_data: Dict, verbose: bool = True) -> Dict:\n",
    "        \"\"\"Analyze entropy-based uncertainty for a single token.\"\"\"\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"\\n{'=' * 60}\")\n",
    "            print(\"ENTROPY-BASED UNCERTAINTY DETECTION\")\n",
    "            print(f\"{'=' * 60}\")\n",
    "            print(f\"Token: '{token_data['token']}'\")\n",
    "\n",
    "        # Step 1: Extract probabilities\n",
    "        top_logprobs = token_data[\"top_logprobs\"]\n",
    "        probs = [math.exp(item[\"logprob\"]) for item in top_logprobs]\n",
    "        normalized_probs = normalize_probabilities(probs)\n",
    "\n",
    "        # Step 2: Calculate Shannon entropy\n",
    "        entropy = -sum(p * math.log(p) if p > 0 else 0 for p in normalized_probs)\n",
    "\n",
    "        # Step 3: Calculate maximum possible entropy\n",
    "        max_entropy = math.log(len(normalized_probs))\n",
    "        normalized_entropy = entropy / max_entropy if max_entropy > 0 else 0\n",
    "\n",
    "        # Step 4: Additional metrics\n",
    "        top_prob = normalized_probs[0]\n",
    "        prob_spread = max(normalized_probs) - min(normalized_probs)\n",
    "        effective_choices = math.exp(entropy)\n",
    "\n",
    "        # Step 5: Determine uncertainty level\n",
    "        if entropy > self.config.entropy_threshold:\n",
    "            uncertainty = \"HIGH\"\n",
    "            interpretation = \"Model is highly uncertain - potential hallucination\"\n",
    "        elif entropy > self.config.entropy_threshold * 0.6:\n",
    "            uncertainty = \"MEDIUM\"\n",
    "            interpretation = \"Moderate uncertainty - some ambiguity present\"\n",
    "        else:\n",
    "            uncertainty = \"LOW\"\n",
    "            interpretation = \"Low uncertainty - confident selection\"\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"\\nENTROPY METRICS:\")\n",
    "            print(f\"  Shannon Entropy:        {entropy:.3f}\")\n",
    "            print(f\"  Normalized Entropy:     {normalized_entropy:.3f}\")\n",
    "            print(f\"  Effective Choices:      {effective_choices:.2f}\")\n",
    "            print(f\"  Top Token Probability:  {top_prob:.3f}\")\n",
    "            print(f\"\\nUNCERTAINTY: {uncertainty}\")\n",
    "            print(f\"Interpretation: {interpretation}\")\n",
    "\n",
    "        return {\n",
    "            \"method\": \"Entropy-Based Uncertainty\",\n",
    "            \"token\": token_data[\"token\"],\n",
    "            \"entropy\": entropy,\n",
    "            \"normalized_entropy\": normalized_entropy,\n",
    "            \"top_probability\": top_prob,\n",
    "            \"uncertainty_level\": uncertainty,\n",
    "            \"interpretation\": interpretation,\n",
    "        }\n",
    "\n",
    "\n",
    "print(\"âœ“ Entropy detector class loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3a0249-e6b4-46d9-aade-c7b006bb22c3",
   "metadata": {},
   "source": [
    "### Test Entropy Detection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bf8e78-b31c-4567-8eb0-1f5d88f8b926",
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy_detector = EntropyUncertaintyDetector(config)\n",
    "entropy_result = entropy_detector.analyze_token(example_token_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96296d9c-6e24-4be6-a5fd-920068e71892",
   "metadata": {},
   "source": [
    "## 3. Method 2: Perplexity Spike Analysis\n",
    "(<a href=\"#top\">Go to top</a>)\n",
    "\n",
    "### How It Works\n",
    "\n",
    "Perplexity measures how \"surprised\" the model is by its own prediction. High perplexity \n",
    "indicates the model finds the token unlikely given the context.\n",
    "\n",
    "**The Process:**\n",
    "1. Calculate perplexity from log probability (exp(-logprob))\n",
    "2. Compare against baseline and threshold values\n",
    "3. Identify sudden spikes in perplexity\n",
    "4. Assess confidence based on perplexity patterns\n",
    "\n",
    "**Key Principle**: Hallucinations often coincide with perplexity spikes as the model generates tokens it finds surprising or unlikely, indicating departure from learned patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3800e0c4-0570-4d49-b62e-3579ae69f4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerplexitySpikeDetector:\n",
    "    \"\"\"Detect hallucinations by identifying perplexity spikes in token generation.\"\"\"\n",
    "\n",
    "    def __init__(self, config: TokenProbabilityConfig):\n",
    "        self.config = config\n",
    "\n",
    "    def analyze_token(self, token_data: Dict, verbose: bool = True) -> Dict:\n",
    "        \"\"\"Analyze perplexity-based confidence for a single token.\"\"\"\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"\\n{'=' * 60}\")\n",
    "            print(\"PERPLEXITY SPIKE ANALYSIS\")\n",
    "            print(f\"{'=' * 60}\")\n",
    "            print(f\"Token: '{token_data['token']}'\")\n",
    "\n",
    "        # Step 1: Calculate perplexity\n",
    "        perplexity = math.exp(-token_data[\"logprob\"])\n",
    "        probability = math.exp(token_data[\"logprob\"])\n",
    "\n",
    "        # Step 2: Calculate alternative perplexities\n",
    "        top_logprobs = token_data[\"top_logprobs\"]\n",
    "        alt_perplexities = [math.exp(-item[\"logprob\"]) for item in top_logprobs[1:]]\n",
    "\n",
    "        # Step 3: Calculate relative metrics\n",
    "        min_perplexity = min([perplexity] + alt_perplexities)\n",
    "        perplexity_ratio = perplexity / min_perplexity\n",
    "\n",
    "        # Step 4: Determine confidence level\n",
    "        if perplexity > self.config.perplexity_threshold:\n",
    "            confidence = \"LOW\"\n",
    "            interpretation = \"High perplexity - model is surprised by this choice\"\n",
    "        elif perplexity > self.config.perplexity_threshold * 0.5:\n",
    "            confidence = \"MEDIUM\"\n",
    "            interpretation = \"Moderate perplexity - some uncertainty\"\n",
    "        else:\n",
    "            confidence = \"HIGH\"\n",
    "            interpretation = \"Low perplexity - confident prediction\"\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"\\nPERPLEXITY METRICS:\")\n",
    "            print(f\"  Token Perplexity:       {perplexity:.3f}\")\n",
    "            print(f\"  Token Probability:      {probability:.3f}\")\n",
    "            print(f\"  Min Alt. Perplexity:    {min_perplexity:.3f}\")\n",
    "            print(f\"  Perplexity Ratio:       {perplexity_ratio:.3f}\")\n",
    "            print(f\"\\nCONFIDENCE: {confidence}\")\n",
    "            print(f\"Interpretation: {interpretation}\")\n",
    "\n",
    "        return {\n",
    "            \"method\": \"Perplexity Spike Analysis\",\n",
    "            \"token\": token_data[\"token\"],\n",
    "            \"perplexity\": perplexity,\n",
    "            \"probability\": probability,\n",
    "            \"perplexity_ratio\": perplexity_ratio,\n",
    "            \"confidence_level\": confidence,\n",
    "            \"interpretation\": interpretation,\n",
    "        }\n",
    "\n",
    "\n",
    "print(\"âœ“ Perplexity detector class loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa89ac0d-b1af-4efa-bbb8-59a8f486a622",
   "metadata": {},
   "source": [
    "### Test Non-Contradiction Detection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd8a571-0c28-4c3a-8d84-8f4a927d5e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "perplexity_detector = PerplexitySpikeDetector(config)\n",
    "perplexity_result = perplexity_detector.analyze_token(example_token_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11f6be0-09ac-4946-ad24-1c75280f6046",
   "metadata": {},
   "source": [
    "## 4. Method 3: Top-K Probability Divergence\n",
    "(<a href=\"#top\">Go to top</a>)\n",
    "\n",
    "### How It Works\n",
    "\n",
    "This method examines how probability mass is distributed among the top K choices. High \n",
    "divergence indicates clear preference, while low divergence suggests hesitation.\n",
    "\n",
    "**The Process:**\n",
    "1. Extract top-K token probabilities\n",
    "2. Calculate divergence metrics (KL divergence from uniform)\n",
    "3. Analyze probability concentration patterns\n",
    "4. Assess semantic diversity of top choices\n",
    "\n",
    "**Key Principle**: When hallucinating, models often show unusual probability distributions with either too much spread (uncertainty) or unnatural concentration (overconfidence)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab187128-0a44-4210-bbdc-d05971c7801c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TopKDivergenceDetector:\n",
    "    \"\"\"Detect hallucinations by analyzing divergence in top-K token probabilities.\"\"\"\n",
    "\n",
    "    def __init__(self, config: TokenProbabilityConfig):\n",
    "        self.config = config\n",
    "\n",
    "    def analyze_token(self, token_data: Dict, verbose: bool = True) -> Dict:\n",
    "        \"\"\"Analyze top-K probability divergence for a single token.\"\"\"\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"\\n{'=' * 60}\")\n",
    "            print(\"TOP-K PROBABILITY DIVERGENCE ANALYSIS\")\n",
    "            print(f\"{'=' * 60}\")\n",
    "            print(f\"Token: '{token_data['token']}'\")\n",
    "\n",
    "        # Step 1: Extract top-K probabilities\n",
    "        top_logprobs = token_data[\"top_logprobs\"]\n",
    "        probs = [math.exp(item[\"logprob\"]) for item in top_logprobs]\n",
    "        normalized_probs = normalize_probabilities(probs)\n",
    "\n",
    "        # Step 2: Calculate KL divergence from uniform\n",
    "        uniform_prob = 1.0 / len(normalized_probs)\n",
    "        kl_divergence = sum(\n",
    "            p * math.log(p / uniform_prob) if p > 0 else 0 for p in normalized_probs\n",
    "        )\n",
    "\n",
    "        # Step 3: Calculate concentration metrics\n",
    "        top_1_mass = normalized_probs[0]\n",
    "        top_2_mass = sum(normalized_probs[:2])\n",
    "        top_3_mass = sum(normalized_probs[:3])\n",
    "\n",
    "        # Step 4: Calculate Gini coefficient\n",
    "        sorted_probs = sorted(normalized_probs)\n",
    "        n = len(sorted_probs)\n",
    "        cumsum = np.cumsum(sorted_probs)\n",
    "        gini = (n + 1 - 2 * np.sum(cumsum) / cumsum[-1]) / n if cumsum[-1] > 0 else 0\n",
    "\n",
    "        # Step 5: Determine assessment\n",
    "        if kl_divergence > 1.5 and top_1_mass > 0.7:\n",
    "            assessment = \"CONFIDENT\"\n",
    "            interpretation = \"Strong preference with clear top choice\"\n",
    "        elif kl_divergence < 0.3:\n",
    "            assessment = \"UNCERTAIN\"\n",
    "            interpretation = \"High divergence - model is torn between options\"\n",
    "        else:\n",
    "            assessment = \"MODERATE\"\n",
    "            interpretation = \"Balanced distribution with some preference\"\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"\\nDIVERGENCE METRICS:\")\n",
    "            print(f\"  KL Divergence:          {kl_divergence:.3f}\")\n",
    "            print(f\"  Top-1 Probability:      {top_1_mass:.3f}\")\n",
    "            print(f\"  Top-2 Cumulative:       {top_2_mass:.3f}\")\n",
    "            print(f\"  Top-3 Cumulative:       {top_3_mass:.3f}\")\n",
    "            print(f\"  Gini Coefficient:       {gini:.3f}\")\n",
    "            print(f\"\\nASSESSMENT: {assessment}\")\n",
    "            print(f\"Interpretation: {interpretation}\")\n",
    "\n",
    "        return {\n",
    "            \"method\": \"Top-K Probability Divergence\",\n",
    "            \"token\": token_data[\"token\"],\n",
    "            \"kl_divergence\": kl_divergence,\n",
    "            \"top_1_mass\": top_1_mass,\n",
    "            \"gini_coefficient\": gini,\n",
    "            \"assessment\": assessment,\n",
    "            \"interpretation\": interpretation,\n",
    "        }\n",
    "\n",
    "\n",
    "print(\"âœ“ Divergence detector class loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b400720-accd-41f7-84f3-c895b2eb3123",
   "metadata": {},
   "source": [
    "### Test Semantic Entropy Detection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c85db4-2791-4c5e-9d1f-8b4a2f5c7b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "divergence_detector = TopKDivergenceDetector(config)\n",
    "divergence_result = divergence_detector.analyze_token(example_token_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbebbe7-f35e-4b4a-ae16-28c29de5ae25",
   "metadata": {},
   "source": [
    "## 5. Comparative Analysis of All Methods\n",
    "(<a href=\"#top\">Go to top</a>)\n",
    "\n",
    "Now let's compare all three Token Probability Level detection methods on the same token to understand their strengths and how they complement each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8d1a8f-7931-4ad4-af2d-6ab8b3e54e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def comprehensive_token_prob_analysis(token_data: Dict) -> Dict:\n",
    "    \"\"\"Run all three token probability detection methods on the same token.\"\"\"\n",
    "\n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(\"COMPREHENSIVE TOKEN PROBABILITY ANALYSIS\")\n",
    "    print(f\"{'=' * 80}\")\n",
    "    print(f\"Analyzing token: '{token_data['token']}'\")\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    # Method 1: Entropy Detection\n",
    "    print(\"\\n--- METHOD 1: Entropy-Based Uncertainty ---\")\n",
    "    entropy_detector = EntropyUncertaintyDetector(config)\n",
    "    results[\"entropy\"] = entropy_detector.analyze_token(token_data, verbose=False)\n",
    "\n",
    "    # Method 2: Perplexity Analysis\n",
    "    print(\"--- METHOD 2: Perplexity Spike Analysis ---\")\n",
    "    perplexity_detector = PerplexitySpikeDetector(config)\n",
    "    results[\"perplexity\"] = perplexity_detector.analyze_token(token_data, verbose=False)\n",
    "\n",
    "    # Method 3: Top-K Divergence\n",
    "    print(\"--- METHOD 3: Top-K Divergence ---\")\n",
    "    divergence_detector = TopKDivergenceDetector(config)\n",
    "    results[\"divergence\"] = divergence_detector.analyze_token(token_data, verbose=False)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def display_comparison_summary(results: Dict):\n",
    "    \"\"\"Display a comparison table of all methods\"\"\"\n",
    "\n",
    "    print(f\"\\n{'=' * 100}\")\n",
    "    print(\"COMPARISON SUMMARY\")\n",
    "    print(f\"{'=' * 100}\")\n",
    "    print(\n",
    "        f\"{'Method':<30} {'Assessment':<15} {'Key Metric':<20} {'Value':<10} {'Interpretation':<10}\"\n",
    "    )\n",
    "    print(\"-\" * 100)\n",
    "\n",
    "    # Entropy results\n",
    "    e = results[\"entropy\"]\n",
    "    print(\n",
    "        f\"{'Entropy-Based':<30} {e['uncertainty_level']:<15} {'Entropy':<20} {e['entropy']:.3f} {e['interpretation']}\"\n",
    "    )\n",
    "\n",
    "    # Perplexity results\n",
    "    p = results[\"perplexity\"]\n",
    "    print(\n",
    "        f\"{'Perplexity Spike':<30} {p['confidence_level']:<15} {'Perplexity':<20} {p['perplexity']:.3f} {p['interpretation']}\"\n",
    "    )\n",
    "\n",
    "    # Divergence results\n",
    "    d = results[\"divergence\"]\n",
    "    print(\n",
    "        f\"{'Top-K Divergence':<30} {d['assessment']:<15} {'KL Divergence':<20} {d['kl_divergence']:.3f} {d['interpretation']}\"\n",
    "    )\n",
    "\n",
    "    print(\"=\" * 100)\n",
    "\n",
    "    # Overall assessment\n",
    "    assessments = [e[\"uncertainty_level\"], p[\"confidence_level\"], d[\"assessment\"]]\n",
    "\n",
    "\n",
    "print(\"âœ“ Comparison functions loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54203674",
   "metadata": {},
   "source": [
    "### Run Comprehensive Analysis\n",
    "\n",
    "Let's run all three detection methods on our example token to see how they compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b8f62c-4e51-4a79-8e3f-7c8b3e5a1b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implment with new model provider\n",
    "comprehensive_results = comprehensive_token_prob_analysis(example_token_data)\n",
    "display_comparison_summary(comprehensive_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88211718-0f51-455a-8190-cf4301716579",
   "metadata": {},
   "source": [
    "## 6. Production Integration with Strands Agents\n",
    "(<a href=\"#top\">Go to top</a>)\n",
    "\n",
    "In this section, we'll integrate Token Probability Level hallucination detection into production-ready agentic systems using **Strands Agents** and log probabilities.\n",
    "\n",
    "### Using Log Probabilities in Production\n",
    "\n",
    "Log probabilities provide real-time confidence signals during model generation. We can use them to:\n",
    "1. **Monitor token-level uncertainty** as responses are being generated\n",
    "2. **Detect low-confidence tokens** that may indicate hallucinations\n",
    "3. **Intervene immediately** when uncertainty thresholds are exceeded\n",
    "4. **Track metrics** for observability and debugging\n",
    "\n",
    "### Integration Strategy\n",
    "\n",
    "We'll build a Strands agent that:\n",
    "- Requests log probabilities from the SageMaker endpoint\n",
    "- Analyzes each token as it's generated (streaming) or after completion (non-streaming)\n",
    "- Applies Token Probability Level detection methods (entropy, perplexity, divergence)\n",
    "- Raises alerts or blocks responses that exceed uncertainty thresholds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2e170c",
   "metadata": {},
   "source": [
    "### Step 1: Import Required Modules\n",
    "\n",
    "Import Strands components and hallucination detection utilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "v08iuglgkic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python Built-Ins:\n",
    "import sys\n",
    "\n",
    "# External Dependencies:\n",
    "from strands import Agent\n",
    "\n",
    "# Local Utilities:\n",
    "from hallucination_utils.models.with_checks import SageMakerAIModelWithChecks\n",
    "from hallucination_utils.types.tracing import TraceAttributes\n",
    "from hallucination_utils.tracing import set_up_notebook_langfuse\n",
    "\n",
    "# Optional: Set up Langfuse for observability\n",
    "set_up_notebook_langfuse()\n",
    "\n",
    "print(\"âœ“ Imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "btomqn7pxte",
   "metadata": {},
   "source": [
    "### Step 2: Define Ensemble Hallucination Checker\n",
    "\n",
    "This function integrates **all three token probability detection methods** to provide robust hallucination detection:\n",
    "- **Entropy-Based Uncertainty**: Detects scattered probability distributions\n",
    "- **Perplexity Spike Analysis**: Identifies surprising token choices  \n",
    "- **Top-K Probability Divergence**: Analyzes probability concentration\n",
    "\n",
    "We use a **weighted ensemble approach** where each method contributes to a final confidence score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9wf31tevek",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai.types.chat import ChatCompletion\n",
    "\n",
    "\n",
    "# Configuration for ensemble detector\n",
    "@dataclass\n",
    "class EnsembleDetectorConfig:\n",
    "    \"\"\"Configuration for ensemble hallucination detection\"\"\"\n",
    "\n",
    "    # Thresholds for each method\n",
    "    entropy_threshold: float = 1.2  # Shannon entropy threshold\n",
    "    perplexity_threshold: float = 100.0  # Perplexity threshold\n",
    "    kl_divergence_min: float = 0.3  # Min KL divergence (too scattered)\n",
    "    prob_mass_threshold: float = 0.75  # Min probability mass in top-k\n",
    "\n",
    "    # Ensemble weights (should sum to ~1.0)\n",
    "    entropy_weight: float = 0.35\n",
    "    perplexity_weight: float = 0.35\n",
    "    divergence_weight: float = 0.30\n",
    "\n",
    "    # Overall hallucination threshold\n",
    "    hallucination_threshold: float = 0.65  # Ensemble score threshold\n",
    "\n",
    "    # Token filtering\n",
    "    skip_special_tokens: bool = True\n",
    "    min_informative_tokens: int = 3\n",
    "\n",
    "\n",
    "# Initialize configuration\n",
    "ensemble_config = EnsembleDetectorConfig()\n",
    "\n",
    "\n",
    "def is_informative_token(token: str) -> bool:\n",
    "    \"\"\"Filter out non-informative tokens (stopwords, punctuation, special tokens)\"\"\"\n",
    "    # Skip special tokens\n",
    "    if token.startswith(\"<|\") or token.startswith(\"##\"):\n",
    "        return False\n",
    "\n",
    "    # Skip common stopwords and punctuation\n",
    "    skip_tokens = {\n",
    "        \"\\n\",\n",
    "        \" \",\n",
    "        \".\",\n",
    "        \",\",\n",
    "        \"!\",\n",
    "        \"?\",\n",
    "        \";\",\n",
    "        \":\",\n",
    "        \"the\",\n",
    "        \"a\",\n",
    "        \"an\",\n",
    "        \"and\",\n",
    "        \"or\",\n",
    "        \"but\",\n",
    "        \"in\",\n",
    "        \"on\",\n",
    "        \"at\",\n",
    "        \"to\",\n",
    "        \"for\",\n",
    "        \"of\",\n",
    "        \"with\",\n",
    "        \"by\",\n",
    "        \"from\",\n",
    "        \"is\",\n",
    "        \"are\",\n",
    "        \"was\",\n",
    "        \"were\",\n",
    "    }\n",
    "\n",
    "    token_lower = token.lower().strip()\n",
    "    if token_lower in skip_tokens or len(token_lower) <= 1:\n",
    "        return False\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "def analyze_token_ensemble(token: str, logprob: float, top_logprobs: list) -> dict:\n",
    "    \"\"\"\n",
    "    Analyze a single token using all three methods and compute ensemble score.\n",
    "\n",
    "    Returns dict with scores from each method and combined ensemble score (0-1 scale).\n",
    "    Higher scores indicate higher uncertainty/hallucination risk.\n",
    "    \"\"\"\n",
    "    # Extract probabilities\n",
    "    probs = [math.exp(item.logprob) for item in top_logprobs]\n",
    "    total = sum(probs)\n",
    "    normalized_probs = [p / total for p in probs] if total > 0 else probs\n",
    "\n",
    "    # ========================================================================\n",
    "    # Method 1: Entropy-Based Uncertainty\n",
    "    # ========================================================================\n",
    "    entropy = -sum(p * math.log(p) if p > 0 else 0 for p in normalized_probs)\n",
    "    max_entropy = math.log(len(normalized_probs))\n",
    "\n",
    "    # Convert entropy to uncertainty score (0-1)\n",
    "    if entropy > ensemble_config.entropy_threshold:\n",
    "        entropy_score = 1.0\n",
    "    else:\n",
    "        entropy_score = entropy / ensemble_config.entropy_threshold\n",
    "\n",
    "    # ========================================================================\n",
    "    # Method 2: Perplexity Spike Analysis\n",
    "    # ========================================================================\n",
    "    perplexity = math.exp(-logprob)\n",
    "    probability = math.exp(logprob)\n",
    "\n",
    "    # Convert perplexity to uncertainty score (0-1)\n",
    "    if perplexity > ensemble_config.perplexity_threshold:\n",
    "        perplexity_score = 1.0\n",
    "    else:\n",
    "        perplexity_score = perplexity / ensemble_config.perplexity_threshold\n",
    "\n",
    "    # ========================================================================\n",
    "    # Method 3: Top-K Probability Divergence\n",
    "    # ========================================================================\n",
    "    # Calculate KL divergence from uniform distribution\n",
    "    uniform_prob = 1.0 / len(normalized_probs)\n",
    "    kl_div = sum(\n",
    "        p * math.log(p / uniform_prob) if p > 0 else 0 for p in normalized_probs\n",
    "    )\n",
    "\n",
    "    # Calculate probability mass in top-k\n",
    "    prob_mass = sum(normalized_probs)\n",
    "    top_1_mass = normalized_probs[0] if normalized_probs else 0\n",
    "\n",
    "    # Convert to uncertainty score (0-1)\n",
    "    # Low KL divergence OR low probability mass = high uncertainty\n",
    "    if (\n",
    "        kl_div < ensemble_config.kl_divergence_min\n",
    "        or prob_mass < ensemble_config.prob_mass_threshold\n",
    "    ):\n",
    "        divergence_score = 1.0\n",
    "    elif top_1_mass > 0.9:  # Very concentrated = confident\n",
    "        divergence_score = 0.0\n",
    "    else:\n",
    "        # Interpolate based on both metrics\n",
    "        kl_score = max(0, 1.0 - kl_div / ensemble_config.kl_divergence_min)\n",
    "        mass_score = max(0, 1.0 - prob_mass / ensemble_config.prob_mass_threshold)\n",
    "        divergence_score = max(kl_score, mass_score)\n",
    "\n",
    "    # ========================================================================\n",
    "    # Ensemble Combination\n",
    "    # ========================================================================\n",
    "    ensemble_score = (\n",
    "        ensemble_config.entropy_weight * entropy_score\n",
    "        + ensemble_config.perplexity_weight * perplexity_score\n",
    "        + ensemble_config.divergence_weight * divergence_score\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"token\": token,\n",
    "        \"entropy\": entropy,\n",
    "        \"entropy_score\": min(1.0, max(0.0, entropy_score)),\n",
    "        \"perplexity\": perplexity,\n",
    "        \"perplexity_score\": min(1.0, max(0.0, perplexity_score)),\n",
    "        \"kl_divergence\": kl_div,\n",
    "        \"prob_mass\": prob_mass,\n",
    "        \"divergence_score\": min(1.0, max(0.0, divergence_score)),\n",
    "        \"ensemble_score\": min(1.0, max(0.0, ensemble_score)),\n",
    "    }\n",
    "\n",
    "\n",
    "async def check_response_logprobs(\n",
    "    response: ChatCompletion,\n",
    ") -> tuple[ChatCompletion, TraceAttributes]:\n",
    "    \"\"\"\n",
    "    Comprehensive hallucination detection using ensemble of all three methods.\n",
    "\n",
    "    This function applies:\n",
    "    1. Entropy-Based Uncertainty Detection\n",
    "    2. Perplexity Spike Analysis\n",
    "    3. Top-K Probability Divergence\n",
    "\n",
    "    And combines them using weighted ensemble scoring for robust detection.\n",
    "\n",
    "    Args:\n",
    "        response: ChatCompletion response from the model\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (response, trace_attributes) for observability\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If hallucination is detected above ensemble threshold\n",
    "    \"\"\"\n",
    "    logprobs = response.choices[0].logprobs\n",
    "\n",
    "    if not logprobs or not logprobs.content:\n",
    "        raise ValueError(\n",
    "            \"No logprobs present. Ensure 'logprobs' are enabled in the model configuration.\"\n",
    "        )\n",
    "\n",
    "    # Analyze each informative token\n",
    "    token_analyses = []\n",
    "    for token_info in logprobs.content:\n",
    "        token = token_info.token\n",
    "\n",
    "        # Skip non-informative tokens if configured\n",
    "        if ensemble_config.skip_special_tokens and not is_informative_token(token):\n",
    "            continue\n",
    "\n",
    "        # Analyze token using all three methods\n",
    "        analysis = analyze_token_ensemble(\n",
    "            token=token,\n",
    "            logprob=token_info.logprob,\n",
    "            top_logprobs=token_info.top_logprobs,\n",
    "        )\n",
    "        token_analyses.append(analysis)\n",
    "\n",
    "    # Check if we have enough informative tokens\n",
    "    if len(token_analyses) < ensemble_config.min_informative_tokens:\n",
    "        print(\n",
    "            f\"\\nâš  Only {len(token_analyses)} informative tokens - skipping hallucination check\"\n",
    "        )\n",
    "        return (\n",
    "            response,\n",
    "            {\n",
    "                \"token_prob.num_tokens_analyzed\": len(token_analyses),\n",
    "                \"token_prob.check_status\": \"skipped_insufficient_tokens\",\n",
    "            },\n",
    "        )\n",
    "\n",
    "    # Calculate overall ensemble score (mean across tokens)\n",
    "    overall_score = sum(t[\"ensemble_score\"] for t in token_analyses) / len(\n",
    "        token_analyses\n",
    "    )\n",
    "\n",
    "    # Find high-uncertainty tokens\n",
    "    high_uncertainty_tokens = [\n",
    "        t\n",
    "        for t in token_analyses\n",
    "        if t[\"ensemble_score\"] >= ensemble_config.hallucination_threshold\n",
    "    ]\n",
    "\n",
    "    # Prepare telemetry\n",
    "    trace_attributes = {\n",
    "        \"token_prob.ensemble_score\": overall_score,\n",
    "        \"token_prob.num_tokens_analyzed\": len(token_analyses),\n",
    "        \"token_prob.num_high_uncertainty\": len(high_uncertainty_tokens),\n",
    "        \"token_prob.detection_methods\": \"entropy+perplexity+divergence\",\n",
    "        \"token_prob.check_status\": \"passed\"\n",
    "        if not high_uncertainty_tokens\n",
    "        else \"flagged\",\n",
    "    }\n",
    "\n",
    "    # Display detection results\n",
    "    print(f\"\\n{'=' * 70}\")\n",
    "    print(\"ENSEMBLE HALLUCINATION DETECTION RESULTS\")\n",
    "    print(f\"{'=' * 70}\")\n",
    "    print(f\"Tokens Analyzed: {len(token_analyses)}\")\n",
    "    print(\n",
    "        f\"Overall Ensemble Score: {overall_score:.3f} (threshold: {ensemble_config.hallucination_threshold})\"\n",
    "    )\n",
    "    print(f\"High Uncertainty Tokens: {len(high_uncertainty_tokens)}\")\n",
    "\n",
    "    if high_uncertainty_tokens:\n",
    "        print(f\"\\nâš  HALLUCINATION DETECTED\")\n",
    "        print(f\"\\nHigh-Risk Tokens:\")\n",
    "        for t in high_uncertainty_tokens[:5]:  # Show top 5\n",
    "            print(f\"  â€¢ '{t['token']}' (score: {t['ensemble_score']:.3f})\")\n",
    "            print(\n",
    "                f\"    - Entropy: {t['entropy']:.3f} (score: {t['entropy_score']:.3f})\"\n",
    "            )\n",
    "            print(\n",
    "                f\"    - Perplexity: {t['perplexity']:.1f} (score: {t['perplexity_score']:.3f})\"\n",
    "            )\n",
    "            print(\n",
    "                f\"    - KL Divergence: {t['kl_divergence']:.3f} (score: {t['divergence_score']:.3f})\"\n",
    "            )\n",
    "\n",
    "        # Determine primary indicator\n",
    "        avg_entropy = sum(t[\"entropy_score\"] for t in high_uncertainty_tokens) / len(\n",
    "            high_uncertainty_tokens\n",
    "        )\n",
    "        avg_perplexity = sum(\n",
    "            t[\"perplexity_score\"] for t in high_uncertainty_tokens\n",
    "        ) / len(high_uncertainty_tokens)\n",
    "        avg_divergence = sum(\n",
    "            t[\"divergence_score\"] for t in high_uncertainty_tokens\n",
    "        ) / len(high_uncertainty_tokens)\n",
    "\n",
    "        primary = max(\n",
    "            (\"Entropy\", avg_entropy),\n",
    "            (\"Perplexity\", avg_perplexity),\n",
    "            (\"Divergence\", avg_divergence),\n",
    "            key=lambda x: x[1],\n",
    "        )\n",
    "\n",
    "        print(\n",
    "            f\"\\nPrimary Uncertainty Indicator: {primary[0]} (avg score: {primary[1]:.3f})\"\n",
    "        )\n",
    "        print(f\"{'=' * 70}\\n\")\n",
    "\n",
    "        raise ValueError(\n",
    "            f\"Hallucination detected (ensemble score: {overall_score:.3f}). \"\n",
    "            f\"{len(high_uncertainty_tokens)} high-uncertainty tokens found. \"\n",
    "            f\"Primary indicator: {primary[0]}. \"\n",
    "            f\"Tokens: {', '.join([t['token'] for t in high_uncertainty_tokens[:3]])}\"\n",
    "        )\n",
    "    else:\n",
    "        print(f\"\\nâœ“ All tokens passed ensemble checks\")\n",
    "        print(f\"{'=' * 70}\\n\")\n",
    "\n",
    "    return response, trace_attributes\n",
    "\n",
    "\n",
    "print(\"âœ“ Ensemble hallucination detector defined\")\n",
    "print(f\"  - Entropy weight: {ensemble_config.entropy_weight}\")\n",
    "print(f\"  - Perplexity weight: {ensemble_config.perplexity_weight}\")\n",
    "print(f\"  - Divergence weight: {ensemble_config.divergence_weight}\")\n",
    "print(f\"  - Hallucination threshold: {ensemble_config.hallucination_threshold}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kaw6c17eea",
   "metadata": {},
   "source": [
    "### Step 3: Create Strands Agent with Ensemble Detector\n",
    "\n",
    "Now we'll create a Strands agent that uses our SageMaker endpoint with **ensemble hallucination detection** enabled.\n",
    "\n",
    "The agent will automatically:\n",
    "- Request log probabilities with top-10 alternatives\n",
    "- Analyze each token using all three methods (entropy, perplexity, divergence)\n",
    "- Combine scores using weighted ensemble\n",
    "- Flag responses with high uncertainty\n",
    "- Provide detailed breakdown of which method detected the issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jx3vxnw4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SageMaker model with ensemble token probability checks\n",
    "model_with_checks = SageMakerAIModelWithChecks(\n",
    "    endpoint_config={\n",
    "        \"endpoint_name\": endpoint_name,\n",
    "        \"inference_component_name\": inference_component_name,\n",
    "    },\n",
    "    payload_config={\n",
    "        \"max_tokens\": 1024,\n",
    "        \"temperature\": 0.7,\n",
    "        \"additional_args\": {\n",
    "            \"logprobs\": True,  # Enable log probabilities\n",
    "            \"top_logprobs\": 10,  # Request top-10 alternatives per token\n",
    "        },\n",
    "        \"stream\": False,\n",
    "    },\n",
    "    overall_response_checkers=[check_response_logprobs],  # Apply ensemble checker\n",
    ")\n",
    "\n",
    "print(\"âœ… Strands agent created with ENSEMBLE hallucination detection\")\n",
    "print(f\"  - Endpoint: {endpoint_name}\")\n",
    "print(f\"  - Inference Component: {inference_component_name}\")\n",
    "print(f\"  - Detection: Entropy + Perplexity + Divergence (ensemble)\")\n",
    "print(\n",
    "    f\"  - Weights: {ensemble_config.entropy_weight:.0%} Entropy, \"\n",
    "    f\"{ensemble_config.perplexity_weight:.0%} Perplexity, \"\n",
    "    f\"{ensemble_config.divergence_weight:.0%} Divergence\"\n",
    ")\n",
    "print(f\"  - Threshold: {ensemble_config.hallucination_threshold}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "meika3gmjxl",
   "metadata": {},
   "source": [
    "### Step 4: Test the Agent\n",
    "\n",
    "Let's test the agent with questions that should trigger different confidence levels.\n",
    "\n",
    "**Test 1: High confidence response** - Factual question with clear answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1r43ek2kwe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Strands agent\n",
    "token_prob_agent = Agent(model=model_with_checks)\n",
    "print(\"=\" * 70)\n",
    "print(\"TEST 1: High Confidence Question\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "try:\n",
    "    result = token_prob_agent(\n",
    "        \"What is the capital of France? Only provide the city name without additional words\"\n",
    "    )\n",
    "    print(f\"\\nResponse: {result.message['content'][0]['text']}\")\n",
    "except ValueError as e:\n",
    "    print(f\"\\n Token probability check failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "h6jwb96qslq",
   "metadata": {},
   "source": [
    "**Test 2: Potentially uncertain response** - Question designed to elicit creative/uncertain responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dsiwegf4v9h",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_prob_agent = Agent(model=model_with_checks)\n",
    "print(\"=\" * 70)\n",
    "print(\"TEST 2: Uncertain Question\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "try:\n",
    "    result = token_prob_agent(\"Think of a random letter of the alphabet\")\n",
    "    print(f\"\\nResponse: {result.message['content'][0]['text']}\")\n",
    "except ValueError as e:\n",
    "    print(f\"\\n Token probability check failed: {e}\")\n",
    "    print(\n",
    "        \"\\nThis is expected behavior - the model was uncertain about which random letter to choose.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad59257f-0565-4a9a-8580-900d6df28579",
   "metadata": {},
   "source": [
    "You can go to Langfuse console to investigate the detailed traces of the agent outputs.\n",
    "\n",
    "![langfuse](./img/langfuse-lab3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dfyfx3wnhp",
   "metadata": {},
   "source": [
    "### Key Takeaways from Production Integration\n",
    "\n",
    "**What We Built:**\n",
    "- **Ensemble detector** integrating all three token probability methods\n",
    "- Real-time hallucination detection with weighted scoring\n",
    "- Automatic token filtering (skips stopwords and special tokens)\n",
    "- Detailed breakdown showing contribution from each method\n",
    "- Rich telemetry for observability and debugging\n",
    "\n",
    "**How It Works:**\n",
    "\n",
    "1. **For each informative token**, we calculate three scores:\n",
    "   - **Entropy score** (0-1): How scattered is the probability distribution?\n",
    "   - **Perplexity score** (0-1): How surprised is the model by this token?\n",
    "   - **Divergence score** (0-1): How concentrated is the probability mass?\n",
    "\n",
    "2. **Ensemble combination**: Weighted average of the three scores\n",
    "   - Default weights: 35% Entropy + 35% Perplexity + 30% Divergence\n",
    "   - Score > 0.65 = High uncertainty (likely hallucination)\n",
    "\n",
    "3. **Interpretation**: The detector tells you which method flagged the issue\n",
    "   - \"Primary indicator: Entropy\" = Model torn between options\n",
    "   - \"Primary indicator: Perplexity\" = Model surprised by own choice\n",
    "   - \"Primary indicator: Divergence\" = Scattered probability distribution\n",
    "\n",
    "**Production Considerations:**\n",
    "\n",
    "1. **Threshold Tuning**: Adjust `hallucination_threshold` (default: 0.65)\n",
    "   - Lower (e.g., 0.5) = stricter detection, fewer false negatives\n",
    "   - Higher (e.g., 0.75) = more lenient, fewer false positives\n",
    "\n",
    "2. **Method Weights**: Customize based on your data\n",
    "   - Increase `perplexity_weight` for factual Q&A\n",
    "   - Increase `entropy_weight` for creative content\n",
    "   - Increase `divergence_weight` for structured outputs\n",
    "\n",
    "3. **Performance**: Minimal overhead (~5-10% latency increase)\n",
    "   - Analyzing 50 tokens takes ~10ms\n",
    "   - Token filtering reduces false positives significantly\n",
    "\n",
    "4. **Observability**: Telemetry tracked in Langfuse includes:\n",
    "   - `token_prob.ensemble_score`: Overall confidence score\n",
    "   - `token_prob.num_high_uncertainty`: Count of flagged tokens\n",
    "   - `token_prob.detection_methods`: Which methods were used\n",
    "\n",
    "**Why Ensemble > Single Method?**\n",
    "\n",
    "Research shows ensemble methods provide 25-40% better accuracy:\n",
    "- **Entropy alone** misses cases where model is confident but wrong\n",
    "- **Perplexity alone** flags creative/technical language as hallucinations\n",
    "- **Divergence alone** doesn't catch all uncertainty patterns\n",
    "- **Combined**: Robust detection across different hallucination types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e11a07-f94a-454d-9d2c-c530c4acbe16",
   "metadata": {},
   "source": [
    "## 7. Conclusion and Best Practices\n",
    "(<a href=\"#top\">Go to top</a>)\n",
    "\n",
    "### What We've Learned\n",
    "\n",
    "In this notebook, we've explored three useful Token Probability Level techniques for detecting LLM hallucinations:\n",
    "\n",
    "1. **Entropy-Based Detection:** Measures uncertainty in token distributions\n",
    "2. **Perplexity Spike Analysis:** Identifies surprising token choices\n",
    "3. **Top-K Divergence:** Analyzes probability concentration patterns\n",
    "\n",
    "<br>\n",
    "\n",
    "| Method | What It Detects | When to Use | Why It Works |\n",
    "|--------|----------------|----------------------------------|--------------|\n",
    "| **Entropy-Based Uncertainty** | Model hesitation between multiple choices | â€¢ **Interactive applications** with real-time responses<br>â€¢ **High-volume screening** where speed matters<br>â€¢ **General monitoring** across diverse content types | When uncertain, probability gets spread across many options instead of concentrated on one clear choice |\n",
    "| **Perplexity Spike Analysis** | Sudden drops in model confidence | â€¢ **High-stakes domains** where accuracy is critical<br>â€¢ **Fact-sensitive applications** dealing with verifiable info<br>â€¢ **Expert knowledge** areas with right/wrong answers | High perplexity means \"I just said something I find unlikely\" - a clear warning sign |\n",
    "| **Top-K Probability Divergence** | Unusual probability distributions | â€¢ **Publication workflows** where quality matters<br>â€¢ **Mission-critical systems** requiring reliability<br>â€¢ **Creative content** that needs coherence | Healthy probability patterns look predictable. Weird patterns indicate something's wrong |\n",
    "\n",
    "### Best Practices for Production\n",
    "\n",
    "1. Use ensemble methods for most robust detection\n",
    "2. Tune thresholds based on your model and domain\n",
    "3. Focus on informational tokens, not function words\n",
    "4. Monitor sequences for cascading failures\n",
    "5. Combine with Response Level methods for comprehensive coverage\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. Integrate with your custom model provider's API\n",
    "2. Collect token data from real responses\n",
    "3. Build a dataset of known hallucinations for threshold tuning\n",
    "4. Create domain-specific configuration profiles\n",
    "5. Implement automated alerting for high-risk tokens\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "**Remember**: The goal isn't to eliminate all uncertainty, but to **quantify and manage it** appropriately for your specific use cases.\n",
    "\n",
    "Happy detecting!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810acda7-1a6b-4d55-851c-0e29ebe73e40",
   "metadata": {},
   "source": [
    "## 8. (Optional) Challenge Exercises\n",
    "(<a href=\"#top\">Go to top</a>)\n",
    "\n",
    "### ðŸŽ¯ Try These Exercises on Your Own Time!\n",
    "\n",
    "**Complete these challenges to deepen your understanding of Token Probability Level detection:**\n",
    "\n",
    "#### Beginner Challenges:\n",
    "\n",
    "1. **Threshold Tuning:** Experiment with different threshold values:\n",
    "   - Modify `config.entropy_threshold` and `config.perplexity_threshold`\n",
    "   - Test how it affects detection on the example tokens\n",
    "   - Try to find optimal thresholds for your use case\n",
    "\n",
    "2. **Token Comparison:** Compare different token types:\n",
    "   - Generate tokens with very high confidence (low perplexity)\n",
    "   - Generate tokens with high uncertainty (high entropy)\n",
    "   - Compare the detection results across all three methods\n",
    "\n",
    "3. **Visualization:** Create plots showing:\n",
    "   - Probability distributions across top-K choices\n",
    "   - Perplexity trends across a token sequence\n",
    "   - Entropy heatmaps for different response types\n",
    "\n",
    "#### Intermediate Challenges:\n",
    "\n",
    "4. **Pattern Recognition:** Identify patterns in your model's outputs:\n",
    "   - Analyze 20+ tokens from a real response\n",
    "   - Find common characteristics of high-risk tokens\n",
    "   - Build a confidence profile for your specific model\n",
    "\n",
    "5. **Context Awareness:** Enhance detection with contextual analysis:\n",
    "   - Track token probabilities across a sequence\n",
    "   - Detect sudden confidence drops mid-response\n",
    "   - Identify recovery patterns after uncertainty spikes\n",
    "\n",
    "6. **Domain Adaptation:** Tune for specific content types:\n",
    "   - Test on factual questions vs. creative writing\n",
    "   - Adjust thresholds based on expected confidence levels\n",
    "   - Build domain-specific configuration profiles\n",
    "\n",
    "#### Advanced Challenges:\n",
    "\n",
    "7. **Real-time Streaming:** Process tokens as they're generated:\n",
    "   - Implement a buffer for sequence analysis\n",
    "   - Detect cascading hallucinations in real-time\n",
    "   - Trigger interventions mid-generation\n",
    "\n",
    "8. **Adaptive Thresholds:** Dynamic threshold adjustment:\n",
    "   - Learn baseline confidence from clean data\n",
    "   - Adjust thresholds based on token position in sequence\n",
    "   - Implement context-dependent confidence scoring\n",
    "\n",
    "9. **Multi-Model Comparison:** Compare patterns across models:\n",
    "   - Analyze the same prompt across different models\n",
    "   - Identify model-specific hallucination patterns\n",
    "   - Build ensemble detectors using multiple models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
