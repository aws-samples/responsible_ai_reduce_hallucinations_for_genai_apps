{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "993dcf59-dd85-4a31-a8cc-02c5d4be5dfe",
   "metadata": {},
   "source": [
    "# Lab 0: Deploy GPT-OSS Model to SageMaker AI Endpoint\n",
    "\n",
    "This notebook guides you through deploying a [GPT-OSS 20B](https://github.com/openai/gpt-oss) model to [Amazon SageMaker AI](https://docs.aws.amazon.com/sagemaker/latest/dg/whatis.html) using the [Large Model Inference (LMI)](https://docs.aws.amazon.com/sagemaker/latest/dg/large-model-inference-container-docs.html) v16 container and the vLLM backend. We'll use this deployed endpoint in later notebooks, to explore detection methods that rely on the extra internal visibility offered by open-weight models.\n",
    "\n",
    "‚è∞ We start the deployment early because it can take some time to complete, **but**...\n",
    "\n",
    "üèéÔ∏è Since the first lab doesn't need it yet, you can start working on that **in parallel** when you hit a long wait in this notebook!\n",
    "\n",
    "For further examples on hosting different Foundation Models on SageMaker, check out [aws-samples/sagemaker-genai-hosting-examples](https://github.com/aws-samples/sagemaker-genai-hosting-examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d7977b-1a65-4161-96bf-4f0a55bbed69",
   "metadata": {},
   "source": [
    "## Getting started and connecting to AWS\n",
    "\n",
    "If you haven't already, install the dependencies for the workshop as listed in [pyproject.toml](./pyproject.toml) by running the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131bf879-5914-4d66-ac4e-b5178af32d7e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%pip install -e ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9ec592",
   "metadata": {},
   "source": [
    "<div style=\"border: 4px solid coral; text-align: left; margin: auto; padding-left: 20px; padding-right: 20px\">\n",
    "    <h4>üîÑ Restart the kernel after installing</h4>\n",
    "    <p>\n",
    "        You'll need to restart the notebook kernel after running this cell, for the installations\n",
    "        to take full effect.\n",
    "    </p>\n",
    "    <p>\n",
    "        Note that you may see some error notices about dependency conflicts in SageMaker Studio\n",
    "        environments, but this is okay as long as the installations are completed.\n",
    "    </p>\n",
    "</div>\n",
    "<br/>\n",
    "\n",
    "Once the necessary libraries are installed, run the cell below to initially connect to the AWS services we'll use.\n",
    "\n",
    "For more information, you can refer to:\n",
    "- [`sagemaker` Python SDK Documentation](https://sagemaker.readthedocs.io/en/stable/)\n",
    "- Boto3 client docs for [sagemaker](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker.html) and [sagemaker-runtime](https://boto3.amazonaws.com/v1/documentation/api/1.21.44/reference/services/sagemaker-runtime.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9183bf15-3f3f-4d70-91da-fe269ff421b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python Built-Ins:\n",
    "import json\n",
    "\n",
    "# External Dependencies:\n",
    "import boto3  # AWS SDKs for Python\n",
    "import sagemaker  # High-level SDK for SageMaker AI\n",
    "from sagemaker.compute_resource_requirements.resource_requirements import (\n",
    "    ResourceRequirements,\n",
    ")\n",
    "from sagemaker.enums import EndpointType\n",
    "from sagemaker.session import Session as SageMakerSession\n",
    "\n",
    "# You could instead set region_name explicitly in Session() if wanted:\n",
    "boto_sess = boto3.Session()\n",
    "region = boto_sess.region_name\n",
    "\n",
    "# Low-level clients for SageMaker control plane and endpoint invocation:\n",
    "sm_client = boto_sess.client(\"sagemaker\")\n",
    "smr_client = boto_sess.client(\"sagemaker-runtime\")\n",
    "\n",
    "sm_sess = SageMakerSession(boto_session=boto_sess)  # High-level client for SageMaker\n",
    "\n",
    "print(f\"Working in AWS Region: {region}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sap37yu33x",
   "metadata": {},
   "source": [
    "## Specify Container and vLLM Serving Properties\n",
    "\n",
    "For deploying GPT-OSS-20B model, we'll use:\n",
    "\n",
    "- The AWS [Large Model Inference (LMI)](https://docs.aws.amazon.com/sagemaker/latest/dg/large-model-inference-container-docs.html) container image, at version 16+ to include vLLM version 0.10.2+\n",
    "- A GPU-accelerated G5 compute instance type: [ml.g5.4xlarge](https://aws.amazon.com/ec2/instance-types/g5/)\n",
    "\n",
    "Note that there are more samples around some of the newer features/support with v16, that you can reference down below:\n",
    "\n",
    "- [Custom Input/Output Formatters](https://github.com/deepjavalibrary/djl-serving/blob/0.34.0-dlc/serving/docs/lmi/user_guides/input_formatter_schema.md)\n",
    "- [Multi-Adapter Inference](https://github.com/deepjavalibrary/djl-serving/blob/0.34.0-dlc/serving/docs/adapters.md)\n",
    "- [Sticky Session Routing](https://github.com/deepjavalibrary/djl-serving/blob/0.34.0-dlc/serving/docs/stateful_sessions.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e98731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify container LMIv16\n",
    "CONTAINER_VERSION = \"0.34.0-lmi16.0.0-cu128-v1.2\"\n",
    "inference_image = (\n",
    "    f\"763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:{CONTAINER_VERSION}\"\n",
    ")\n",
    "num_gpu = 1\n",
    "instance_type = \"ml.g5.4xlarge\"\n",
    "print(f\"Using image URI: {inference_image}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddd8338-f5d8-44b2-af6b-e08517b6ea6d",
   "metadata": {},
   "source": [
    "**Regarding your endpoint `role`:**\n",
    "\n",
    "Your endpoint will run with an [execution role](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html) that defines its permissions, configured below. The provided role must:\n",
    "\n",
    "1. Be assumable by the SageMaker service\n",
    "2. Have sufficient permissions to pull your container image from Amazon ECR (and should probably also have permissions to send logs, events, etc to Amazon CloudWatch) - see [here in the SageMaker AI Developer Guide](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html#sagemaker-roles-createmodel-perms) for more details\n",
    "\n",
    "If you're running this notebook outside of SageMaker, the `sagemaker.get_execution_role()` we use below might fail or you might be using a role that's not assumable by SageMaker. You can instead set `role` to an existing role ARN of your choice.\n",
    "\n",
    "**Configuring vLLM:**\n",
    "\n",
    "This container is set up to map `OPTION_...` environment variables into [vllm serve CLI arguments](https://docs.vllm.ai/en/latest/cli/serve.html). For example:\n",
    "- `OPTION_MODEL` -> [`--model`](https://docs.vllm.ai/en/latest/cli/serve.html#-model)\n",
    "- `OPTION_TOOL_CALL_PARSER` -> [`--tool-call-parser`](https://docs.vllm.ai/en/latest/cli/serve.html#-tool-call-parser).\n",
    "\n",
    "**Learn more:**\n",
    "- [SageMaker Inference Components](https://docs.aws.amazon.com/sagemaker/latest/dg/inference-component-based-endpoint.html)\n",
    "- [vLLM Engine Arguments](https://docs.vllm.ai/en/latest/models/engine_args.html)\n",
    "- [SageMaker Instance Types](https://aws.amazon.com/sagemaker/pricing/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59043967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment variables including vLLM config\n",
    "config = {\n",
    "    \"HF_MODEL_ID\": \"openai/gpt-oss-20b\",  # Load GPT-OSS from Hugging Face\n",
    "    \"OPTION_ASYNC_MODE\": \"true\",\n",
    "    \"OPTION_ENTRYPOINT\": \"djl_python.lmi_vllm.vllm_async_service\",\n",
    "    \"OPTION_REASONING_PARSER\": \"openai_gptoss\",\n",
    "    \"OPTION_SERVED_MODEL_NAME\": \"model\",\n",
    "    \"OPTION_TENSOR_PARALLEL_DEGREE\": \"max\",\n",
    "    \"OPTION_TOOL_CALL_PARSER\": \"openai\",\n",
    "    \"SERVING_FAIL_FAST\": \"true\",\n",
    "}\n",
    "\n",
    "# Naming your SageMaker resources:\n",
    "# (name_from_base attaches a timestamp-based suffix for transparent uniqueness)\n",
    "model_name = sagemaker.utils.name_from_base(\"gpt-oss-20b-vllm\")\n",
    "endpoint_name = model_name\n",
    "inference_component_name = f\"ic-{model_name}\"\n",
    "\n",
    "# IAM permissions (see note above):\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "print(f\"Using IAM Role ARN:\\n{role}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e265c224",
   "metadata": {},
   "source": [
    "## Create Inference Component and Endpoint\n",
    "Here we utilize the higher level SageMaker Python SDK to create both an endpoint and an Inference Component, adjust the resource requirements depending on your model and use-case.\n",
    "\n",
    "This next cell deploys your model to SageMaker AI. The deployment process:\n",
    "1. Creates a SageMaker Model object from your container image\n",
    "2. Deploys a component-based Endpoint (with associated Endpoint Configuration) to host your model\n",
    "3. Adds the model to the endpoint as an 'inference component'\n",
    "\n",
    "**Learn more:**\n",
    "- [SageMaker Model Deployment](https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints-deploy-models.html)\n",
    "- [SageMaker Predictor API](https://sagemaker.readthedocs.io/en/stable/api/inference/predictors.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60cc77f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lmi_model = sagemaker.Model(\n",
    "    env=config,\n",
    "    image_uri=inference_image,\n",
    "    name=model_name,\n",
    "    role=role,\n",
    "    sagemaker_session=sm_sess,\n",
    ")\n",
    "\n",
    "print(f\"Deploying endpoint name: {endpoint_name}\")\n",
    "print(f\"Inference component name: {inference_component_name}\")\n",
    "lmi_model.deploy(\n",
    "    container_startup_health_check_timeout=600,\n",
    "    endpoint_name=endpoint_name,\n",
    "    endpoint_type=EndpointType.INFERENCE_COMPONENT_BASED,\n",
    "    inference_component_name=inference_component_name,\n",
    "    initial_instance_count=1,\n",
    "    instance_type=instance_type,\n",
    "    resources=ResourceRequirements(\n",
    "        requests={\n",
    "            \"num_accelerators\": num_gpu,\n",
    "            \"memory\": 1024 * 3,\n",
    "            \"copies\": 1,\n",
    "        }\n",
    "    ),\n",
    ")\n",
    "print(\"\\nDeployed!\")\n",
    "\n",
    "# Store the endpoint configuration for use in other notebooks\n",
    "%store endpoint_name\n",
    "%store inference_component_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1642fd",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<div style=\"border: 4px solid coral; text-align: left; margin: auto; padding-left: 20px; padding-right: 20px\">\n",
    "    <h4>‚è∞ This process can take ~6-10 minutes</h4>\n",
    "    <p>\n",
    "        While it's running, you can move on to start working through lab 1 and check back later. You need to make sure the SageMaker endpoint is successfully deployed before starting lab 2, so come back after you finished Lab 1.\n",
    "    </p>\n",
    "    <p>\n",
    "        Check the logs below to make sure the build and push completed successfully before moving\n",
    "        on in this notebook.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212fde0a-1a56-4f2b-95b8-bc98d7854d5c",
   "metadata": {},
   "source": [
    "## Testing the Deployed Endpoint\n",
    "\n",
    "Now that your model is deployed, let's test it with various inference examples using the OpenAI-compatible API format.\n",
    "\n",
    "Here, we'll set up a \"Predictor\" client using the SageMaker high-level Python SDK to make inference requests. It's also possible to use the low-level SageMaker `InvokeEndpoint` API via generic AWS SDKs like `boto3` for Python or equivalents for other languages.\n",
    "\n",
    "**Learn more:**\n",
    "- [OpenAI Chat Completions API](https://platform.openai.com/docs/api-reference/chat/create)\n",
    "- [SageMaker Runtime API](https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_runtime_InvokeEndpoint.html)\n",
    "- [SageMaker Python SDK 'Predictor' client](https://sagemaker.readthedocs.io/en/stable/api/inference/predictors.html)\n",
    "- [Boto3 SageMakerRuntime with low-level Python `invoke_endpoint` method](https://boto3.amazonaws.com/v1/documentation/api/1.21.44/reference/services/sagemaker-runtime.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11ae72d-4e66-442d-af11-0d0e4020f22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r endpoint_name\n",
    "%store -r inference_component_name\n",
    "\n",
    "llm = sagemaker.Predictor(\n",
    "    endpoint_name=endpoint_name,\n",
    "    sagemaker_session=sm_sess,\n",
    "    serializer=sagemaker.serializers.JSONSerializer(),\n",
    "    deserializer=sagemaker.deserializers.JSONDeserializer(),\n",
    "    component_name=inference_component_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xpopl78mza",
   "metadata": {},
   "source": [
    "### Basic Inference Example\n",
    "\n",
    "This example demonstrates a simple, non-streaming chat completion request to the model - extracting just the text content of the response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db82df26",
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {\n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": \"Name popular places to visit in London?\"}\n",
    "    ],\n",
    "}\n",
    "res = llm.predict(payload)\n",
    "print(\"\\n\".join((\"-----\", res[\"choices\"][0][\"message\"][\"content\"], \"-----\", \"\")))\n",
    "print(res[\"usage\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28f66b1",
   "metadata": {},
   "source": [
    "Alternatively, you can also view the full response object including metadata:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7a780f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json.dumps(res, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d865b1",
   "metadata": {},
   "source": [
    "### Tool calling\n",
    "\n",
    "The following shows how tool/function calling would typically work through the provided model API: Passing in a (list of) tool definition(s) which the model can decide to invoke as part of its response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533da82f-f84b-4d2a-8a23-ecc344929f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"text\": \"What's the service pricing code for Amazon SageMaker AI?\",\n",
    "                    \"type\": \"text\",\n",
    "                }\n",
    "            ],\n",
    "        }\n",
    "    ],\n",
    "    \"tools\": [\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"get_pricing_service_codes\",\n",
    "                \"description\": \"Get AWS service codes available in the Price List API.\",\n",
    "                \"parameters\": {\n",
    "                    \"properties\": {},\n",
    "                    \"title\": \"get_pricing_service_codesArguments\",\n",
    "                    \"type\": \"object\",\n",
    "                    \"required\": [],\n",
    "                },\n",
    "            },\n",
    "        }\n",
    "    ],\n",
    "    \"tool_choice\": \"auto\",\n",
    "}\n",
    "\n",
    "res = llm.predict(payload)\n",
    "print(\"\\n\".join((\"-----\", json.dumps(res[\"choices\"][0], indent=2), \"-----\", \"\")))\n",
    "print(res[\"usage\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03kpvo2oi49l",
   "metadata": {},
   "source": [
    "### Advanced options: Log-Probabilities\n",
    "\n",
    "We can also enable **log probabilities** (logprobs), which return the model's confidence scores for generated tokens. This is useful for:\n",
    "- **Uncertainty quantification:** Understanding model confidence in its predictions\n",
    "- **Hallucination detection:** Lower confidence scores may indicate uncertain or hallucinated content\n",
    "- **Alternative token analysis:** Seeing what other tokens the model considered\n",
    "\n",
    "**Parameters:**\n",
    "- `logprobs: True` - Enable log probability output\n",
    "- `top_logprobs: 5` - Return top 5 alternative tokens with their probabilities for each position\n",
    "- `n: 2` - (Optional) Generate multiple response choices for comparison\n",
    "\n",
    "**Learn more:**\n",
    "- [Understanding Log Probabilities in LLMs](https://platform.openai.com/docs/api-reference/chat/create#chat-create-logprobs)\n",
    "- [Using Logprobs for Hallucination Detection](https://github.com/aws-samples/sagemaker-genai-hosting-examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237c7422-2077-4154-8056-23308b03da3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {\n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": \"Name popular places to visit in Singapore?\"}\n",
    "    ],\n",
    "    \"logprobs\": True,  # <- Enable logprobs\n",
    "    \"top_logprobs\": 5,  # <- Set number of logprobs to return per token\n",
    "    # \"n\": 2,  # <- Generate multiple 'choices' (good for semantic similarity!)\n",
    "}\n",
    "res = llm.predict(payload)\n",
    "print(\"-----\\n\" + res[\"choices\"][0][\"message\"][\"content\"] + \"\\n-----\\n\")\n",
    "print(res[\"usage\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aeca8d8-313c-460a-a542-00c4cc759b7e",
   "metadata": {},
   "source": [
    "You can view the detailed log probability data for each token in the response. Each entry contains:\n",
    "- The generated token\n",
    "- Its log probability (confidence score)\n",
    "- Top alternative tokens and their probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93fc2ee-fa06-483a-87f2-c6d2ba3039a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res[\"choices\"][0][\"logprobs\"][\"content\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dp882bgdjg",
   "metadata": {},
   "source": [
    "## Clean Up Resources (Optional)\n",
    "\n",
    "You'll need this endpoint for the labs in the workshop, but endpoints are billable for as long as they're deployed. Remember to delete your endpoint when you're done experimenting with the workshop, to avoid ongoing charges.\n",
    "\n",
    "**Learn more:**\n",
    "- [SageMaker Pricing](https://aws.amazon.com/sagemaker/pricing/)\n",
    "- [Deleting SageMaker Resources](https://docs.aws.amazon.com/sagemaker/latest/dg/ex1-cleanup.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1429843",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Uncomment to delete the endpoint\n",
    "# sm_client.delete_inference_component(InferenceComponentName=inference_component_name)\n",
    "# sm_client.delete_endpoint(EndpointName=endpoint_name)\n",
    "# sm_client.delete_endpoint_config(EndpointConfigName=endpoint_name)\n",
    "# sm_client.delete_model(ModelName=model_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
